<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一线攻城狮</title>
  
  <subtitle>十年磨一剑，一步一步脚踏实地的耕种</subtitle>
  <link href="http://researchlab.github.io/atom.xml" rel="self"/>
  
  <link href="http://researchlab.github.io/"/>
  <updated>2021-02-20T18:13:04.979Z</updated>
  <id>http://researchlab.github.io/</id>
  
  <author>
    <name>Lee Hong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>聊一聊接口和抽象类</title>
    <link href="http://researchlab.github.io/2021/02/21/interface-vs-abstract/"/>
    <id>http://researchlab.github.io/2021/02/21/interface-vs-abstract/</id>
    <published>2021-02-20T16:33:40.000Z</published>
    <updated>2021-02-20T18:13:04.979Z</updated>
    
    <content type="html"><![CDATA[<p>接口和抽象类的区别是什么？什么时候用接口？什么时候用抽象类？抽象类和接口存在的意义是什么？能解决哪些编程问题？<br><a id="more"></a></p><h2 id="语法特性"><a href="#语法特性" class="headerlink" title="语法特性"></a>语法特性</h2><p>抽象类: 不允许被实例化，只能被继承。它可以包含属性和方法。方法既可以包含代码实现，也可以不包含代码实现。不包含代码实现的方法叫作抽象方法。子类继承抽象类，必须实现抽象类中的所有抽象方法。</p><p>接口: 不能包含属性，只能声明方法，方法不能包含代码实现。包含抽象方法、静态方法、default 方法；类实现接口时，必须实现抽象方法。</p><h2 id="解决哪些编程问题"><a href="#解决哪些编程问题" class="headerlink" title="解决哪些编程问题"></a>解决哪些编程问题</h2><p>抽象类: 是对成员变量和方法的抽象，是一种 is-a 关系，是为了解决代码复用问题。</p><p>接口: 仅仅是对方法的抽象，是一种 has-a 关系，表示具有某一组行为特性，是为了解决解耦问题，隔离接口和具体的实现，提高代码的扩展性。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>什么时候该用抽象类？什么时候该用接口？</p><p>如果要表示一种 is-a 的关系，并且是为了解决代码复用问题，就用抽象类；</p><p>如果要表示一种 has-a 关系，并且是为了解决抽象而非代码复用问题，那就用接口。</p><p>解决复用问题：java中的子类FileInputStream和PipeInputStream等继承抽象类InputStream。重写了read(source)方法，InputStream 中还包含其他方法，FileInputStream继承抽象类复用了父类的其他方法。</p><p>解决抽象问题：抽象类InputStream实现了Closeable接口，该接口中包含close()抽象方法。Closeable这个接口还在很多其他类中实现了，例如Channel，Socket中都有close() 关闭这个功能，但具体实现每个类又各有不同的实现，这个就是抽象。</p><h2 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h2><ul><li>抽象类可以实现接口，而且可以只实现部分接口。</li></ul><blockquote><p>接口A有三个方法: aa(),bb(),cc()，抽象类B只实现了接口A的aa(),bb()两个方法，而剩下的cc()方法怎么办呢？<br>这时，当一个类C继承B时，就需要实现cc()方法了。这样的结果是:C类可以使用A接口的所有方法，而且可以自定义cc()方法的逻辑。<br>而如果C直接实现接口A，那么它必须实现A的所有方法。当C有aa(),bb()方法逻辑相同的兄弟类时，直接实现接口A的思路就会造成代码的重复率比较高了。</p></blockquote><ul><li>接口可以继承接口，但是接口不能继承抽象类。</li></ul><p><img src="/2021/02/21/interface-vs-abstract/interface-vs-abstract.png" alt="interface-vs-abstract"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.it610.com/article/2291299.htm" target="_blank" rel="noopener">选择接口还是抽象类</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;接口和抽象类的区别是什么？什么时候用接口？什么时候用抽象类？抽象类和接口存在的意义是什么？能解决哪些编程问题？&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="better-coding" scheme="http://researchlab.github.io/categories/better-coding/"/>
    
    
    <category term="dp" scheme="http://researchlab.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>聊一聊面向对象编程</title>
    <link href="http://researchlab.github.io/2021/02/20/oop/"/>
    <id>http://researchlab.github.io/2021/02/20/oop/</id>
    <published>2021-02-19T16:19:23.000Z</published>
    <updated>2021-02-20T17:55:59.519Z</updated>
    
    <content type="html"><![CDATA[<p>面向对象编程是一种编程范式。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石。<br><a id="more"></a></p><h2 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h2><p><strong>What:</strong> 隐藏信息（隐藏属性字段），保护数据访问（仅提供有限的公共方法操作属性字段）。<br><strong>How:</strong> 暴露有限的接口和属性，需要编程语言提供访问控制的语法， 如Java 中的 private、protected、public 关键字。<br><strong>Why:</strong> 提高代码可维护性；降低接口复杂度，提高类的易用性。</p><h2 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h2><p><strong>What:</strong> 隐藏方法的具体实现，使用者只需关心方法提供了哪些功能，无需关心内部实现。<br><strong>How:</strong> 通过接口类或者抽象类实现，特殊语法机制非必须，如Java中的interface, abstract关键字。<br><strong>Why:</strong> 提高代码的扩展性、维护性；降低复杂度，减少细节负担。</p><h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p><strong>What:</strong> 表示 is-a 关系，分为单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类。<br><strong>How:</strong> 需要编程语言提供特殊语法机制。例如 Java 的 “extends”，C++ 的 “:” 。<br><strong>Why:</strong> 解决代码复用问题。</p><font color="blue"> 过度使用继承，继承层次过深过复杂，就会导致代码可读性、可维护性变差。</font><h2 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h2><p><strong>What:</strong> 子类替换父类，在运行时调用子类的实现。<br><strong>How:</strong> 需要编程语言提供特殊的语法机制。比如继承、接口类、duck-typing。<br><strong>Why:</strong> 提高代码扩展性和复用性。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;面向对象编程是一种编程范式。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="better-coding" scheme="http://researchlab.github.io/categories/better-coding/"/>
    
    
    <category term="oop" scheme="http://researchlab.github.io/tags/oop/"/>
    
  </entry>
  
  <entry>
    <title>如何编写高质量代码</title>
    <link href="http://researchlab.github.io/2021/02/19/better-coding/"/>
    <id>http://researchlab.github.io/2021/02/19/better-coding/</id>
    <published>2021-02-19T01:04:31.000Z</published>
    <updated>2021-02-20T17:55:59.452Z</updated>
    
    <content type="html"><![CDATA[<p>编写高质量代码，提高复杂代码的设计和开发能力；让读源码、学框架事半功倍；<br><a id="more"></a></p><h2 id="常用的评判代码质量的标准"><a href="#常用的评判代码质量的标准" class="headerlink" title="常用的评判代码质量的标准"></a>常用的评判代码质量的标准</h2><ul><li><p>可维护性（maintainability）<br>  代码分层清晰、模块化好、高内聚低耦合、遵从基于接口而非实现编程的设计原则</p></li><li><p>可读性（readability）<br>符合编码规范、命名是否达意、注释是否详尽、函数是否长短合适、模块划分是否清晰、是否符合高内聚低耦合</p></li><li><p>可扩展性（extensibility）<br>  在不修改或少量修改原有代码的情况下，通过扩展的方式添加新的功能代码。</p></li><li><p>灵活性（flexibility）<br>  代码易扩展、易复用或者易用</p></li><li><p>简洁性（simplicity）<br>Keep It Simple，Stupid, 思从深而行从简，真正的高手能云淡风轻地用最简单的方法解决最复杂的问题</p></li><li><p>可复用性（reusability）</p></li><li><p>可测试性（testability）</p></li></ul><h2 id="如何才能写出高质量的代码"><a href="#如何才能写出高质量的代码" class="headerlink" title="如何才能写出高质量的代码"></a>如何才能写出高质量的代码</h2><p>要写出高质量代码，需要掌握一些更加细化、更加能落地的编程方法论，这就包含面向对象设计思想、设计原则、设计模式、编码规范、重构技巧等。</p><ul><li><p>面向对象编程因为其具有丰富的特性（封装、抽象、继承、多态），可以实现很多复杂的设计思路，写出可复用的代码，是很多设计原则、设计模式等编码实现的基础。</p></li><li><p>设计原则是指导代码设计的一些经验总结，比如，设计原则中的单一职责、DRY、基于接口而非实现、里式替换原则等，可以让我们写出可复用、灵活、可读性好、易扩展、易维护的代码</p></li><li><p>设计模式是针对软件开发中经常遇到的一些设计问题，总结出来的一套解决方案或者设计思路。应用设计模式的主要目的是提高代码的可扩展性，写出易扩展的代码。</p></li><li><p>编程规范主要解决的是代码的可读性问题。编码规范相对于设计原则、设计模式，更加具体、更加偏重代码细节、更加能落地。持续的小重构依赖的理论基础主要就是编程规范。</p></li><li><p>重构作为保持代码质量不下降的有效手段，利用的就是面向对象、设计原则、设计模式、编码规范这些理论。</p></li></ul><p><img src="/2021/02/19/better-coding/code.png" alt="better-coding"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;编写高质量代码，提高复杂代码的设计和开发能力；让读源码、学框架事半功倍；&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="better-coding" scheme="http://researchlab.github.io/categories/better-coding/"/>
    
    
    <category term="dp" scheme="http://researchlab.github.io/tags/dp/"/>
    
  </entry>
  
  <entry>
    <title>Cloud vs On-Premises Pros and Cons</title>
    <link href="http://researchlab.github.io/2020/06/15/Cloud-vs-On-Premises-Pros-and-Cons/"/>
    <id>http://researchlab.github.io/2020/06/15/Cloud-vs-On-Premises-Pros-and-Cons/</id>
    <published>2020-06-15T11:25:23.000Z</published>
    <updated>2021-02-20T17:55:59.452Z</updated>
    
    <content type="html"><![CDATA[<p>The matrix below endeavors to provide guidance as to the factors one should consider when looking towards on-premises or cloud deployments. Like everything else in IT, there are trade-offs, what is right for one organization may be totally inappropriate for another.<br><a id="more"></a><br>By prioritizing and then determining the type of technical and economic resources your organization is willing devote to each of the areas below, you’ll be able to make a better and more informed decision.</p><table><thead><tr><th></th><th></th><th>Cost</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Can be cheaper in the long run</td></tr><tr><td></td><td>Disadvantage</td><td>Significant upfront hardware and software costs (cap-ex) means a mistake can be hugely expensive</td></tr><tr><td>Cloud</td><td>Advantage</td><td>Predictable subscription pricing <br> Costs allocated to op-ex <br>No upfront hardware/software cost</td></tr><tr><td></td><td>Disadvantage</td><td>Really need to project costs out over the long-term<br> Software licensing can mitigate hardware savings</td></tr></tbody></table><table><thead><tr><th></th><th></th><th>Security</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Organization in full control of the data locally<br>No third-party access to the data </td></tr><tr><td></td><td>Disadvantage</td><td>Requires dedicated and knowledgeable resources <br>If organization lacks appropriate expertise, it risks significant exposure </td></tr><tr><td>Cloud</td><td>Advantage</td><td>Deliver superior data security</td></tr><tr><td></td><td>Disadvantage</td><td>Aggressively targeted by hackers <br>Data could be accessed by 3rd party.</td></tr></tbody></table><table><thead><tr><th></th><th></th><th>Agility and Scalability</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Physical control over the hardware means upgrades can be tightly controlled</td></tr><tr><td></td><td>Disadvantage</td><td>Need to plan well in advance to changes in demand because of the time necessary to research, justify, order, and deploy hardware. <br> Could be stuck with excess IT infrastructure which may or may not be able to be re-purposed</td></tr><tr><td>Cloud</td><td>Advantage</td><td>Cloud resources can be rapidly adjusted to accommodate specific demand</td></tr><tr><td></td><td>Disadvantage</td><td>Costs escalate when the cloud infrastructure is improperly managed and not properly tracked </td></tr></tbody></table><table><thead><tr><th></th><th></th><th>Resiliency</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Build out only the level of redundancy needed.<br>  Especially cost effective when 99.999% uptime isn’t a concern</td></tr><tr><td></td><td>Disadvantage</td><td>Costly to properly build out when 99.999% is required</td></tr><tr><td>Cloud</td><td>Advantage</td><td>Much easier and in most cases more cost effective when building out multi-site/ geo-redundant server and storage</td></tr><tr><td></td><td>Disadvantage</td><td>Outside your control - you’re at the cloud providers mercy</td></tr></tbody></table><table><thead><tr><th></th><th></th><th>Software Customization</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Readily able to customize the platform to meet unique requirements.</td></tr><tr><td></td><td>Disadvantage</td><td>Customizations can lead to reliability and software support issues</td></tr><tr><td>Cloud</td><td>Advantage</td><td>SaaS platforms are quite stable. Updates are iterative</td></tr><tr><td></td><td>Disadvantage</td><td>SaaS minimize the ability to modify the platform</td></tr></tbody></table><table><thead><tr><th></th><th></th><th>Software Deployment</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Technology can be deployed based on unique IT infrastructure and application requirements</td></tr><tr><td></td><td>Disadvantage</td><td>Complexity can mean lengthy and difficult deployments</td></tr><tr><td>Cloud</td><td>Advantage</td><td>SaaS is quick to implement</td></tr><tr><td></td><td>Disadvantage</td><td>SaaS is implemented to a lowest common denominator, limiting functionality</td></tr></tbody></table><table><thead><tr><th></th><th></th><th>IT Support</th></tr></thead><tbody><tr><td><nobr>On-Premises</nobr></td><td>Advantage</td><td>Infrastructure and application expertise are readily available</td></tr><tr><td></td><td>Disadvantage</td><td>Dedicated IT necessary, especially when applications are tailored to meet an organization’s unique requirements.</td></tr><tr><td>Cloud</td><td>Advantage</td><td>The economies of scale inherent to cloud deployments mean that IT support can manage more with less time and effort</td></tr><tr><td></td><td>Disadvantage</td><td>The learning curve for cloud is significant.<br> Properly trained personnel are expensive</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;The matrix below endeavors to provide guidance as to the factors one should consider when looking towards on-premises or cloud deployments. Like everything else in IT, there are trade-offs, what is right for one organization may be totally inappropriate for another.&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="DevOps" scheme="http://researchlab.github.io/categories/DevOps/"/>
    
    
    <category term="cloud computing" scheme="http://researchlab.github.io/tags/cloud-computing/"/>
    
  </entry>
  
  <entry>
    <title>scala 入门之HelloWorld</title>
    <link href="http://researchlab.github.io/2019/11/07/scala-hello-world/"/>
    <id>http://researchlab.github.io/2019/11/07/scala-hello-world/</id>
    <published>2019-11-07T09:29:56.000Z</published>
    <updated>2021-02-20T17:55:59.528Z</updated>
    
    <content type="html"><![CDATA[<p>Scala是一门混合了函数式和面向对象的语言。用Scala创建多线程应用时，你会倾向于函数 式编程风格，用不变状态(immutable state)1编写无锁(lock-free)代码。<br><a id="more"></a><br>Scala提供一个基于actor的消息传递(message-passing)模型，消除了涉及并发的痛苦问题。运用这个模型，可以写出简洁的多线程代码，而无需顾虑线程间的数据竞争，以及处理加锁和释放带来的梦魇;</p><h4 id="HelloWorld-from-scala"><a href="#HelloWorld-from-scala" class="headerlink" title="HelloWorld from scala"></a>HelloWorld from scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  cat HelloWorld.scala</span><br><span class="line">object HelloWorld &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    println(&quot;Hello, Scala的HelloWorld的程序!&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">➜  scalac HelloWorld.scala</span><br><span class="line">➜  scala HelloWorld</span><br><span class="line">Hello, Scala的HelloWorld的程序!</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Scala是一门混合了函数式和面向对象的语言。用Scala创建多线程应用时，你会倾向于函数 式编程风格，用不变状态(immutable state)1编写无锁(lock-free)代码。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="java" scheme="http://researchlab.github.io/categories/java/"/>
    
    
    <category term="scala" scheme="http://researchlab.github.io/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Git 提交日志格式规约</title>
    <link href="http://researchlab.github.io/2019/09/14/git-comment-specs/"/>
    <id>http://researchlab.github.io/2019/09/14/git-comment-specs/</id>
    <published>2019-09-14T07:50:34.000Z</published>
    <updated>2021-02-20T17:55:59.479Z</updated>
    
    <content type="html"><![CDATA[<p>规范Git 提交日志内容及格式对说明代码内容和后记追溯问题有很多的帮助, 本文从工作中总结相关的经验成文,<br><a id="more"></a></p><h2 id="git规范说明"><a href="#git规范说明" class="headerlink" title="git规范说明"></a>git规范说明</h2><blockquote><p>为什么要对 Git 提交日志的格式进行规约约束？</p></blockquote><ol><li>最重要的原因：便于人类程序员对提交历史进行追溯，了解发生了什么情况。因此像「update」甚至于「u」这样的 log，都是不合格的写法，想必诸如此类的 log 已经被大家咒骂过一遍遍。</li><li>另外，一旦约束了提交日志，意味着我们将慎重地进行每一次提交，不能再一股脑儿地把各种各样的改动都放到一个提交里面，这样一来，整个代码改动的历史也将更加清晰。</li><li>想得更远一点，格式化的 log 才可能用于自动化输出 changelog。</li></ol><blockquote><p>对Git提交日志进行格式约束是否带来较高的执行成本？</p></blockquote><ol><li>所谓仓廪实而知礼节，随着大型共建项目 / 开源项目的增多，必然要用更专业化的态度去面对。规约化的 Git log 正是其中一环。</li><li>最后，如果实在无法完美遵循日志规约，最最重要的原则是：至少要保证在整个项目中 log 格式的一致性！不要做一个朝秦暮楚的人</li></ol><h2 id="Commit-格式"><a href="#Commit-格式" class="headerlink" title="Commit 格式"></a>Commit 格式</h2><p> Commit Message = <code>{type}:{Jira号},{subject}</code></p><blockquote><p>type, 必须</p></blockquote><p>提交类型 type 用来描述一次提交行为的改动方向。</p><p>type 的可选值如下。注意：Git log 的 type 和 changelog 的 type 存在紧密联系；然而它们两者之间并非一一对应，比如在 changelog 中一般不会指出文档 docs 或测试用例 test 等方面发生的变化。</p><ul><li>feat: 新增功能。（task功能提交）</li><li>fix: 修复 bug。(bug修复)</li><li>docs: 文档相关的改动。(在线文档，说明文档，注释内容)</li><li>style: 对代码的格式化改动，代码逻辑并未产生任何变化。(样式文件修改)</li><li>test: 新增或修改测试用例。</li><li>refactor: 重构代码或其他优化举措。</li><li>chore: 项目工程方面的改动，代码逻辑并未产生任何变化。</li></ul><blockquote><p>Jira号</p></blockquote><ul><li>task/已有bug提交，必须携带Jira号，提交纪录，jira上都会有log统计</li><li>自我修复，优化内容，如果没有对应的Jira可以省略，但subject必须写清楚修改内容</li></ul><blockquote><p>subject</p></blockquote><ul><li>主题 subject 用来概括一次提交行为囊括的内容（注意不是jira的标题）</li><li>简要明了的描述下修改的内容，方便代码review，和后期自己查找纪录</li><li>原则上，通过描述就能清楚的知道，修改的内容</li><li>默认使用中文</li></ul><h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><ul><li>单一原则，一个commit就只做一个内容，不要提交和主题无关的内容，无论这次提交的内容有多简单</li><li>没有task, 自己创建并关联到对应的Story上，代码通过review，关闭相关task/bug</li><li>不要task,bug混提交</li><li>不要多个task,bug合并提交</li><li>如果在提交feat/fix, 对这部分提交相关的代码做了重构/优化，可以合并到feat/fix提交中</li><li>提交后立即找相关Review人review代码，如有有打回，继续追加提交修复内容了，建议不要多个commit一起提交</li></ul><h2 id="示例说明"><a href="#示例说明" class="headerlink" title="示例说明"></a>示例说明</h2><ul><li>feat：JIRA-1224，告警信息列表页面以及告警信息详情页面，日期时间的显示调整</li><li>fix：JIRA-1215，修复系统镜像、自定义镜像，修改镜像名称，操作系统和系统盘大小不会相应改变</li><li>style：修复列表头部按钮，左右间距</li><li>docs: 更新在线文档</li><li>chore: 调整开发环境地址</li><li>refactor：优化云主机创建页面代码逻辑</li></ul>]]></content>
    
    
    <summary type="html">规范化git提交日志,便于检索和追溯代码修订含义</summary>
    
    
    
    <category term="DevOps" scheme="http://researchlab.github.io/categories/DevOps/"/>
    
    
    <category term="git" scheme="http://researchlab.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Switching between JDK 8 and 11 using SDKMAN!</title>
    <link href="http://researchlab.github.io/2019/09/07/mac-upgrade-java-version/"/>
    <id>http://researchlab.github.io/2019/09/07/mac-upgrade-java-version/</id>
    <published>2019-09-07T02:19:46.000Z</published>
    <updated>2021-02-20T17:55:59.505Z</updated>
    
    <content type="html"><![CDATA[<p>Mac os upgrade and use Multi-java version solution : Switching between JDK 8 and 11 using SDKMAN!<br><a id="more"></a></p><blockquote><p>This article is transferred from <a href="https://wimdeblauwe.wordpress.com/2018/09/26/switching-between-jdk-8-and-11-using-sdkman/" target="_blank" rel="noopener">https://wimdeblauwe.wordpress.com/2018/09/26/switching-between-jdk-8-and-11-using-sdkman/</a></p></blockquote><blockquote><p>I have written about switching JDK versions on your mac before. With JDK 11 now being out, it is time to give an updated version.</p></blockquote><h2 id="sdkman-install"><a href="#sdkman-install" class="headerlink" title="sdkman install"></a>sdkman install</h2><p>You can make switching between the Oracle JDK 8 and the OpenJDK 11 very easy if you use SDKMAN!. Just follow the installation instructions at <a href="https://sdkman.io/install" target="_blank" rel="noopener">https://sdkman.io/install</a> to get started.</p><h2 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h2><p>After that, run <code>sdk list java</code>. This will show something like this:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line">curl -s "https://get.sdkman.io" | bash</span><br><span class="line"></span><br><span class="line">                                -+syyyyyyys:</span><br><span class="line">                            `/yho:`       -yd.</span><br><span class="line">                         `/yh/`             +m.</span><br><span class="line">                       .oho.                 hy                          .`</span><br><span class="line">                     .sh/`                   :N`                `-/o`  `+dyyo:.</span><br><span class="line">                   .yh:`                     `M-          `-/osysoym  :hs` `-+sys:      hhyssssssssy+</span><br><span class="line">                 .sh:`                       `N:          ms/-``  yy.yh-      -hy.    `.N-````````+N.</span><br><span class="line">               `od/`                         `N-       -/oM-      ddd+`     `sd:     hNNm        -N:</span><br><span class="line">              :do`                           .M.       dMMM-     `ms.      /d+`     `NMMs       `do</span><br><span class="line">            .yy-                             :N`    ```mMMM.      -      -hy.       /MMM:       yh</span><br><span class="line">          `+d+`           `:/oo/`       `-/osyh/ossssssdNMM`           .sh:         yMMN`      /m.</span><br><span class="line">         -dh-           :ymNMMMMy  `-/shmNm-`:N/-.``   `.sN            /N-         `NMMy      .m/</span><br><span class="line">       `oNs`          -hysosmMMMMydmNmds+-.:ohm           :             sd`        :MMM/      yy</span><br><span class="line">      .hN+           /d:    -MMMmhs/-.`   .MMMh   .ss+-                 `yy`       sMMN`     :N.</span><br><span class="line">     :mN/           `N/     `o/-`         :MMMo   +MMMN-         .`      `ds       mMMh      do</span><br><span class="line">    /NN/            `N+....--:/+oooosooo+:sMMM:   hMMMM:        `my       .m+     -MMM+     :N.</span><br><span class="line">   /NMo              -+ooooo+/:-....`...:+hNMN.  `NMMMd`        .MM/       -m:    oMMN.     hs</span><br><span class="line">  -NMd`                                    :mm   -MMMm- .s/     -MMm.       /m-   mMMd     -N.</span><br><span class="line"> `mMM/                                      .-   /MMh. -dMo     -MMMy        od. .MMMs..---yh</span><br><span class="line"> +MMM.                                           sNo`.sNMM+     :MMMM/        sh`+MMMNmNm+++-</span><br><span class="line"> mMMM-                                           /--ohmMMM+     :MMMMm.       `hyymmmdddo</span><br><span class="line"> MMMMh.                  ````                  `-+yy/`yMMM/     :MMMMMy       -sm:.``..-:-.`</span><br><span class="line"> dMMMMmo-.``````..-:/osyhddddho.           `+shdh+.   hMMM:     :MmMMMM/   ./yy/` `:sys+/+sh/</span><br><span class="line"> .dMMMMMMmdddddmmNMMMNNNNNMMMMMs           sNdo-      dMMM-  `-/yd/MMMMm-:sy+.   :hs-      /N`</span><br><span class="line">  `/ymNNNNNNNmmdys+/::----/dMMm:          +m-         mMMM+ohmo/.` sMMMMdo-    .om:       `sh</span><br><span class="line">     `.-----+/.`       `.-+hh/`         `od.          NMMNmds/     `mmy:`     +mMy      `:yy.</span><br><span class="line">           /moyso+//+ossso:.           .yy`          `dy+:`         ..       :MMMN+---/oys:</span><br><span class="line">         /+m:  `.-:::-`               /d+                                    +MMMMMMMNh:`</span><br><span class="line">        +MN/                        -yh.                                     `+hddhy+.</span><br><span class="line">       /MM+                       .sh:</span><br><span class="line">      :NMo                      -sh/</span><br><span class="line">     -NMs                    `/yy:</span><br><span class="line">    .NMy                  `:sh+.</span><br><span class="line">   `mMm`               ./yds-</span><br><span class="line">  `dMMMmyo:-.````.-:oymNy:`</span><br><span class="line">  +NMMMMMMMMMMMMMMMMms:`</span><br><span class="line">    -+shmNMMMNmdy+:`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                                                                 Now attempting installation...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Looking for a previous installation of SDKMAN...</span><br><span class="line">Looking for unzip...</span><br><span class="line">Looking for zip...</span><br><span class="line">Looking for curl...</span><br><span class="line">Looking for sed...</span><br><span class="line">Installing SDKMAN scripts...</span><br><span class="line">Create distribution directories...</span><br><span class="line">Getting available candidates...</span><br><span class="line">Prime the config file...</span><br><span class="line">Download script archive...</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">####################################################################### 100.0%</span></span></span><br><span class="line">Extract script archive...</span><br><span class="line">Install scripts...</span><br><span class="line">Set version to 5.7.3+337 ...</span><br><span class="line">Attempt update of login bash profile on OSX...</span><br><span class="line">Added sdkman init snippet to /Users/lihong/.bash_profile</span><br><span class="line">Attempt update of zsh profile...</span><br><span class="line">Updated existing /Users/lihong/.zshrc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All done!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please open a new terminal, or run the following in the existing one:</span><br><span class="line"></span><br><span class="line">    source "/Users/lihong/.sdkman/bin/sdkman-init.sh"</span><br><span class="line"></span><br><span class="line">Then issue the following command:</span><br><span class="line"></span><br><span class="line">    sdk help</span><br><span class="line"></span><br><span class="line">Enjoy!!!</span><br><span class="line"></span><br><span class="line">➜  source ~/.sdkman/bin/sdkman-init.sh</span><br><span class="line">➜  sdk version</span><br><span class="line">==== BROADCAST =================================================================</span><br><span class="line">* 2019-09-06: Springboot 2.1.8.RELEASE released on SDKMAN! #springboot</span><br><span class="line">* 2019-09-05: Micronaut 1.2.1 released on SDKMAN! #micronautfw</span><br><span class="line">* 2019-09-05: Gradle 5.6.2 released on SDKMAN! #gradle</span><br><span class="line">================================================================================</span><br><span class="line"></span><br><span class="line">SDKMAN 5.7.3+337</span><br><span class="line"></span><br><span class="line">➜  sdk list java</span><br><span class="line">================================================================================</span><br><span class="line">Available Java Versions</span><br><span class="line">================================================================================</span><br><span class="line"> Vendor        | Use | Version      | Dist    | Status     | Identifier</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line"> AdoptOpenJDK  |     | 12.0.1.j9    | adpt    |            | 12.0.1.j9-adpt</span><br><span class="line">               |     | 12.0.1.hs    | adpt    |            | 12.0.1.hs-adpt</span><br><span class="line">               |     | 11.0.4.j9    | adpt    |            | 11.0.4.j9-adpt</span><br><span class="line">               |     | 11.0.4.hs    | adpt    |            | 11.0.4.hs-adpt</span><br><span class="line">               |     | 8.0.222.j9   | adpt    |            | 8.0.222.j9-adpt</span><br><span class="line">               |     | 8.0.222.hs   | adpt    |            | 8.0.222.hs-adpt</span><br><span class="line"> Amazon        |     | 11.0.4       | amzn    |            | 11.0.4-amzn</span><br><span class="line">               |     | 8.0.222      | amzn    |            | 8.0.222-amzn</span><br><span class="line">               |     | 8.0.202      | amzn    |            | 8.0.202-amzn</span><br><span class="line"> Azul Zulu     |     | 12.0.2       | zulu    |            | 12.0.2-zulu</span><br><span class="line">               |     | 11.0.4       | zulu    |            | 11.0.4-zulu</span><br><span class="line">               |     | 10.0.2       | zulu    |            | 10.0.2-zulu</span><br><span class="line">               |     | 9.0.7        | zulu    |            | 9.0.7-zulu</span><br><span class="line">               |     | 8.0.222      | zulu    |            | 8.0.222-zulu</span><br><span class="line">               |     | 8.0.202      | zulu    |            | 8.0.202-zulu</span><br><span class="line">               |     | 7.0.232      | zulu    |            | 7.0.232-zulu</span><br><span class="line">               |     | 7.0.181      | zulu    |            | 7.0.181-zulu</span><br><span class="line"> Azul ZuluFX   |     | 11.0.2       | zulufx  |            | 11.0.2-zulufx</span><br><span class="line">               |     | 8.0.202      | zulufx  |            | 8.0.202-zulufx</span><br><span class="line"> BellSoft      |     | 12.0.2       | librca  |            | 12.0.2-librca</span><br><span class="line">               |     | 11.0.4       | librca  |            | 11.0.4-librca</span><br><span class="line">               |     | 8.0.222      | librca  |            | 8.0.222-librca</span><br><span class="line"> GraalVM       |     | 19.2.0       | grl     |            | 19.2.0-grl</span><br><span class="line">               |     | 19.1.1       | grl     |            | 19.1.1-grl</span><br><span class="line">               |     | 19.0.2       | grl     |            | 19.0.2-grl</span><br><span class="line">               |     | 1.0.0        | grl     |            | 1.0.0-rc-16-grl</span><br><span class="line"> Java.net      |     | 14.ea.11     | open    |            | 14.ea.11-open</span><br><span class="line">               |     | 13.ea.33     | open    |            | 13.ea.33-open</span><br><span class="line">               |     | 12.0.2       | open    |            | 12.0.2-open</span><br><span class="line">               |     | 11.0.2       | open    |            | 11.0.2-open</span><br><span class="line">               |     | 10.0.2       | open    |            | 10.0.2-open</span><br><span class="line">               |     | 9.0.4        | open    |            | 9.0.4-open</span><br><span class="line"> SAP           |     | 12.0.2       | sapmchn |            | 12.0.2-sapmchn</span><br><span class="line">               |     | 11.0.4       | sapmchn |            | 11.0.4-sapmchn</span><br><span class="line">================================================================================</span><br><span class="line">Use the Identifier for installation:</span><br><span class="line"></span><br><span class="line">    $ sdk install java 11.0.3.hs-adpt</span><br><span class="line">================================================================================</span><br><span class="line">➜  sdk install java 11.0.2-open</span><br><span class="line"></span><br><span class="line">Downloading: java 11.0.2-open</span><br><span class="line"></span><br><span class="line">In progress...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">####################################################################################################################################### 100.0%</span></span></span><br><span class="line"></span><br><span class="line">Repackaging Java 11.0.2-open...</span><br><span class="line"></span><br><span class="line">Done repackaging...</span><br><span class="line">Cleaning up residual files...</span><br><span class="line">basename: illegal option -- r</span><br><span class="line">usage: basename string [suffix]</span><br><span class="line">       basename [-a] [-s suffix] string [...]</span><br><span class="line">mv: illegal option -- r</span><br><span class="line">usage: mv [-f | -i | -n] [-v] source target</span><br><span class="line">       mv [-f | -i | -n] [-v] source ... directory</span><br><span class="line">basename: illegal option -- r</span><br><span class="line">usage: basename string [suffix]</span><br><span class="line">       basename [-a] [-s suffix] string [...]</span><br><span class="line">mv: illegal option -- r</span><br><span class="line">usage: mv [-f | -i | -n] [-v] source target</span><br><span class="line">       mv [-f | -i | -n] [-v] source ... directory</span><br><span class="line"></span><br><span class="line">Installing: java 11.0.2-open</span><br><span class="line">basename: illegal option -- r</span><br><span class="line">usage: basename string [suffix]</span><br><span class="line">       basename [-a] [-s suffix] string [...]</span><br><span class="line">mv: illegal option -- r</span><br><span class="line">usage: mv [-f | -i | -n] [-v] source target</span><br><span class="line">       mv [-f | -i | -n] [-v] source ... directory</span><br><span class="line">mv: rename /Users/lihong/.sdkman/tmp/out to /Users/lihong/.Trash/out.2019-09-07_10_21_57: No such file or directory</span><br><span class="line">Done installing!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Setting java 11.0.2-open as default.</span><br><span class="line">➜ java --version</span><br><span class="line">openjdk 11.0.2 2019-01-15</span><br><span class="line">OpenJDK Runtime Environment 18.9 (build 11.0.2+9)</span><br><span class="line">OpenJDK 64-Bit Server VM 18.9 (build 11.0.2+9, mixed mode)</span><br><span class="line">================================================================================</span><br></pre></td></tr></table></figure><p>We can now install Oracle JDK 8 with: <code>sdk install java 8.0.181-oracle</code></p><p>And OpenJDK 11 after that with: <code>sdk install java 11.0.0-open</code></p><p>During the installation, you can choose what version to make the default.</p><p>If you run sdk list java again, you will see what versions are installed and what version is the default one:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">➜  sdk list java</span><br><span class="line">================================================================================</span><br><span class="line">Available Java Versions</span><br><span class="line">================================================================================</span><br><span class="line"> Vendor        | Use | Version      | Dist    | Status     | Identifier</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line"> AdoptOpenJDK  |     | 12.0.1.j9    | adpt    |            | 12.0.1.j9-adpt</span><br><span class="line">               |     | 12.0.1.hs    | adpt    |            | 12.0.1.hs-adpt</span><br><span class="line">               |     | 11.0.4.j9    | adpt    |            | 11.0.4.j9-adpt</span><br><span class="line">               |     | 11.0.4.hs    | adpt    |            | 11.0.4.hs-adpt</span><br><span class="line">               |     | 8.0.222.j9   | adpt    |            | 8.0.222.j9-adpt</span><br><span class="line">               |     | 8.0.222.hs   | adpt    |            | 8.0.222.hs-adpt</span><br><span class="line"> Amazon        |     | 11.0.4       | amzn    |            | 11.0.4-amzn</span><br><span class="line">               |     | 8.0.222      | amzn    |            | 8.0.222-amzn</span><br><span class="line">               |     | 8.0.202      | amzn    |            | 8.0.202-amzn</span><br><span class="line"> Azul Zulu     |     | 12.0.2       | zulu    |            | 12.0.2-zulu</span><br><span class="line">               |     | 11.0.4       | zulu    |            | 11.0.4-zulu</span><br><span class="line">               |     | 10.0.2       | zulu    |            | 10.0.2-zulu</span><br><span class="line">               |     | 9.0.7        | zulu    |            | 9.0.7-zulu</span><br><span class="line">               |     | 8.0.222      | zulu    |            | 8.0.222-zulu</span><br><span class="line">               |     | 8.0.202      | zulu    |            | 8.0.202-zulu</span><br><span class="line">               |     | 7.0.232      | zulu    |            | 7.0.232-zulu</span><br><span class="line">               |     | 7.0.181      | zulu    |            | 7.0.181-zulu</span><br><span class="line"> Azul ZuluFX   |     | 11.0.2       | zulufx  |            | 11.0.2-zulufx</span><br><span class="line">               |     | 8.0.202      | zulufx  |            | 8.0.202-zulufx</span><br><span class="line"> BellSoft      |     | 12.0.2       | librca  |            | 12.0.2-librca</span><br><span class="line">               |     | 11.0.4       | librca  |            | 11.0.4-librca</span><br><span class="line">               |     | 8.0.222      | librca  |            | 8.0.222-librca</span><br><span class="line"> GraalVM       |     | 19.2.0       | grl     |            | 19.2.0-grl</span><br><span class="line">               |     | 19.1.1       | grl     |            | 19.1.1-grl</span><br><span class="line">               |     | 19.0.2       | grl     |            | 19.0.2-grl</span><br><span class="line">               |     | 1.0.0        | grl     |            | 1.0.0-rc-16-grl</span><br><span class="line"> Java.net      |     | 14.ea.11     | open    |            | 14.ea.11-open</span><br><span class="line">               |     | 13.ea.33     | open    |            | 13.ea.33-open</span><br><span class="line">               |     | 12.0.2       | open    |            | 12.0.2-open</span><br><span class="line">               | &gt;&gt;&gt; | 11.0.2       | open    | installed  | 11.0.2-open</span><br><span class="line">               |     | 10.0.2       | open    |            | 10.0.2-open</span><br><span class="line">               |     | 9.0.4        | open    |            | 9.0.4-open</span><br><span class="line"> SAP           |     | 12.0.2       | sapmchn |            | 12.0.2-sapmchn</span><br><span class="line">               |     | 11.0.4       | sapmchn |            | 11.0.4-sapmchn</span><br><span class="line">================================================================================</span><br><span class="line">Use the Identifier for installation:</span><br><span class="line"></span><br><span class="line">    $ sdk install java 11.0.3.hs-adpt</span><br><span class="line">================================================================================</span><br></pre></td></tr></table></figure><p>To temporarily switch to another version, use the sdk use command. For instance, if you made JDK 8 the default, then switch to JDK 11 in the current session by typing:</p><p>sdk use java 11.0.0-open<br>Result:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">21:08 $ java -version</span><br><span class="line">openjdk version "11" 2018-09-25</span><br><span class="line">OpenJDK Runtime Environment 18.9 (build 11+28)</span><br><span class="line">OpenJDK 64-Bit Server VM 18.9 (build 11+28, mixed mode)</span><br></pre></td></tr></table></figure></p><p>To set a permanent default, use the sdk default command. For instance, to make JDK 11 the default, type: <code>sdk default java 11.0.0-open</code></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Mac os upgrade and use Multi-java version solution : Switching between JDK 8 and 11 using SDKMAN!&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="DevOps" scheme="http://researchlab.github.io/categories/DevOps/"/>
    
    
    <category term="java" scheme="http://researchlab.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>minikube从本地docker registry 拉取镜像的两种方法</title>
    <link href="http://researchlab.github.io/2019/08/24/minikube-pull-image-from-docker-registry/"/>
    <id>http://researchlab.github.io/2019/08/24/minikube-pull-image-from-docker-registry/</id>
    <published>2019-08-24T11:47:33.000Z</published>
    <updated>2021-02-20T17:55:59.506Z</updated>
    
    <content type="html"><![CDATA[<p>在mac中用minikube搭建了一个k8s环境，但是每次拉取镜像都很慢甚至失败, 本文总结两种从本地私有镜像仓库拉取镜像的方法实践, 先将镜像在宿主机上翻墙拉取到本地, 再由minikube 虚机中的k8s 来拉取本地镜像, 这样即可解决之前的拉取镜像失败，也可提升拉取镜像的速度;<br><a id="more"></a></p><h2 id="两种方案"><a href="#两种方案" class="headerlink" title="两种方案"></a>两种方案</h2><ol><li>把私有镜像仓库docker registry 搭建在宿主机上, k8s从本地宿主机上拉取镜像;</li><li>把本地镜像推送到minikube的私有镜像仓库上, k8s从minikube的私有镜像仓库(k8s的本地私有镜像仓库)拉取镜像;</li></ol><h2 id="方案一-k8s从宿主机私有镜像仓库中拉取镜像"><a href="#方案一-k8s从宿主机私有镜像仓库中拉取镜像" class="headerlink" title="方案一: k8s从宿主机私有镜像仓库中拉取镜像"></a>方案一: k8s从宿主机私有镜像仓库中拉取镜像</h2><ol><li>搭建宿主机私有镜像仓库; </li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash </span></span><br><span class="line"></span><br><span class="line">docker pull registry</span><br><span class="line"></span><br><span class="line">docker run -d -p 5000:5000 -v $(pwd):/var/lib/registry --restart always --name registry registry:2</span><br></pre></td></tr></table></figure><ol start="2"><li>验证私有镜像仓库</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl http://127.0.0.1:5000/v2/_catalog</span><br><span class="line">&#123;"repositories":[]&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>推送镜像到私有镜像仓库</li></ol><p>对docker 作出如下配置，并重启使其生效,</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "insecure-registries" : [</span><br><span class="line">    "10.21.71.237:5000"</span><br><span class="line">  ],</span><br><span class="line">  "debug" : true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>10.21.71.237 是宿主机ip </p></blockquote><blockquote><p>mac 配置路径:  工具栏–&gt; docker –&gt; Preferences –&gt; Daemon –&gt; basic –&gt; Insecure registries 加上一行:  10.21.71.237:5000</p></blockquote><p>为镜像打tag, 并推送</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  docker tag luksa/kubia 10.21.71.237:5000/kubia-local</span><br><span class="line">➜  docker push 10.21.71.237:5000/kubia-local</span><br><span class="line">The push refers to repository [10.21.71.237:5000/kubia-local]</span><br><span class="line">5ef1e33458c5: Pushed</span><br><span class="line">dff19e36a9d2: Pushed</span><br><span class="line">28e6af096bca: Pushed</span><br><span class="line">d84d07ed960c: Pushed</span><br><span class="line">894dceed4a75: Pushed</span><br><span class="line">f078de0e3e2b: Pushed</span><br><span class="line">e6562eb04a92: Pushed</span><br><span class="line">596280599f68: Pushed</span><br><span class="line">5d6cbe0dbcf9: Pushed</span><br><span class="line">latest: digest: sha256:0d41ead4dbf022881deafdea4cc3c5a46e0315ebd5805b40f8bbcb7d8b745575 size: 2213</span><br></pre></td></tr></table></figure><p>4 配置minikube 并重启</p><p>minikube 配置如下, 并重启(重启时确保翻墙VPN开启)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">minikube delete &amp;&amp; minikube start --cpus=2 --memory=4096 --disk-size=10g \</span><br><span class="line">--docker-env http_proxy=10.21.71.237:1087 \</span><br><span class="line">--docker-env https_proxy=10.21.71.237:1087 \</span><br><span class="line">--docker-env no_proxy=192.168.99.0/24 \</span><br><span class="line">--registry-mirror=https://registry.docker-cn.com \</span><br><span class="line">  --insecure-registry=10.21.71.237:5000</span><br></pre></td></tr></table></figure><blockquote><p>为了使得minikube 虚机中的k8s 能拉取到宿主机私有镜像 需要上述两项配置  –registry-mirror 和 –insecure-registry</p></blockquote><p>5 k8s 拉取宿主机私有镜像仓库镜像</p><p>k8s pod 文件如下,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  k8s cat kubia-local.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-local-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - image: 10.21.71.237:5000/kubia-local</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      name: kubia-local-mike</span><br><span class="line">      ports:</span><br><span class="line">      - containerPort: 8080</span><br><span class="line">        protocol: TCP</span><br></pre></td></tr></table></figure></p><p>创建 kubia-local pod ,</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">➜  k8s kubectl create -f kubia-local.yaml</span><br><span class="line">➜  k8s kubectl describe pod kubia-local-pod</span><br><span class="line">Name:               kubia-local-pod</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               minikube/10.0.2.15</span><br><span class="line">Start Time:         Sat, 24 Aug 2019 19:38:29 +0800</span><br><span class="line">Labels:             &lt;none&gt;</span><br><span class="line">Annotations:        &lt;none&gt;</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 172.17.0.7</span><br><span class="line">Containers:</span><br><span class="line">  kubia-local-mike:</span><br><span class="line">    Container ID:   docker://fc4df4e9328fb4896672cd4c2504d1a950294ced574200d607e778a72ba8100a</span><br><span class="line">    Image:          10.21.71.237:5000/kubia-local</span><br><span class="line">    Image ID:       docker-pullable://10.21.71.237:5000/kubia-local@sha256:0d41ead4dbf022881deafdea4cc3c5a46e0315ebd5805b40f8bbcb7d8b745575</span><br><span class="line">    Port:           8080/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sat, 24 Aug 2019 19:41:14 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-qkm5t (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-qkm5t:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-qkm5t</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age    From               Message</span><br><span class="line">  ----    ------     ----   ----               -------</span><br><span class="line">  Normal  Scheduled  6m44s  default-scheduler  Successfully assigned default/kubia-local-pod to minikube</span><br><span class="line">  Normal  Pulling    6m43s  kubelet, minikube  Pulling image "10.21.71.237:5000/kubia-local"</span><br><span class="line">  Normal  Pulled     4m     kubelet, minikube  Successfully pulled image "10.21.71.237:5000/kubia-local"</span><br><span class="line">  Normal  Created    4m     kubelet, minikube  Created container kubia-local-mike</span><br><span class="line">  Normal  Started    3m59s  kubelet, minikube  Started container kubia-local-mike</span><br><span class="line">➜  k8s kubectl get po</span><br><span class="line">NAME              READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-local-pod   1/1     Running   0          7m22s</span><br></pre></td></tr></table></figure><p>通过 k8s port-forward 端口转发机制 可方便临时对pod中的服务进行访问</p><p>端口转发<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  k8s kubectl port-forward kubia-local-pod 8888:8080</span><br><span class="line">Forwarding from 127.0.0.1:8888 -&gt; 8080</span><br></pre></td></tr></table></figure></p><p>本地访问k8s pod中的服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ curl localhost:8888</span><br><span class="line">You've hit kubia-local-pod</span><br></pre></td></tr></table></figure><h2 id="方案二-k8s从minikube-虚机本地私有镜像仓库直接拉取镜像"><a href="#方案二-k8s从minikube-虚机本地私有镜像仓库直接拉取镜像" class="headerlink" title="方案二 k8s从minikube 虚机本地私有镜像仓库直接拉取镜像"></a>方案二 k8s从minikube 虚机本地私有镜像仓库直接拉取镜像</h2><p><a href="https://minikube.sigs.k8s.io/docs/tasks/docker_registry/" target="_blank" rel="noopener">https://minikube.sigs.k8s.io/docs/tasks/docker_registry/</a></p><p><a href="https://blog.hasura.io/sharing-a-local-registry-for-minikube-37c7240d0615/" target="_blank" rel="noopener">https://blog.hasura.io/sharing-a-local-registry-for-minikube-37c7240d0615/</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>本文总结了两种从本地拉取镜像的方案;</li><li>推荐使用方案一, 更方便;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在mac中用minikube搭建了一个k8s环境，但是每次拉取镜像都很慢甚至失败, 本文总结两种从本地私有镜像仓库拉取镜像的方法实践, 先将镜像在宿主机上翻墙拉取到本地, 再由minikube 虚机中的k8s 来拉取本地镜像, 这样即可解决之前的拉取镜像失败，也可提升拉取镜像的速度;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="k8s" scheme="http://researchlab.github.io/categories/k8s/"/>
    
    
    <category term="minikube" scheme="http://researchlab.github.io/tags/minikube/"/>
    
  </entry>
  
  <entry>
    <title>mac 利用minikube搭建k8s环境</title>
    <link href="http://researchlab.github.io/2019/08/13/mac-install-minikube/"/>
    <id>http://researchlab.github.io/2019/08/13/mac-install-minikube/</id>
    <published>2019-08-13T13:22:03.000Z</published>
    <updated>2021-02-20T17:55:59.499Z</updated>
    
    <content type="html"><![CDATA[<p>在mac上利用minikube 搭建k8s环境或多或少会因为pull镜像失败而搭建不成功, 本文介绍两种在mac上利用minikube 搭建k8s环境的方法; 一种是通过VPN翻墙利用官方minikube 搭建k8s 环境， 另外一种是借助阿里云的minikube版本来搭建k8s环境;<br><a id="more"></a></p><h2 id="利用官方minikube搭建k8s环境"><a href="#利用官方minikube搭建k8s环境" class="headerlink" title="利用官方minikube搭建k8s环境"></a>利用官方minikube搭建k8s环境</h2><p>1 安装minikube </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew cask install minikube virtualbox</span><br></pre></td></tr></table></figure><p>2 设置VPN</p><p><img src="/2019/08/13/mac-install-minikube/vpn.png" alt=""></p><p>3 编写执行脚本 run.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">minikube delete &amp;&amp; minikube start --cpus=2 --memory=4096 --disk-size=10g \</span><br><span class="line">--docker-env http_proxy=本机IP:1087 \</span><br><span class="line">--docker-env https_proxy=本机IP:1087 \</span><br><span class="line">--docker-env no_proxy=192.168.99.0/24</span><br></pre></td></tr></table></figure><p>其中<code>本机IP</code> 可通过如下命令获得</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> ifconfig |grep inet |grep -v 127.0.0.1</span></span><br><span class="line">inet6 fe80::cf3:df1b:2dc0:6e96%en0 prefixlen 64 secured scopeid 0x5</span><br><span class="line">inet 192.168.11.234 netmask 0xffffff00 broadcast 192.168.11.255</span><br><span class="line">inet6 fe80::71:c740:8d33:ce8f%utun1 prefixlen 64 scopeid 0xc</span><br><span class="line">inet 192.168.99.1 netmask 0xffffff00 broadcast 192.168.99.255</span><br></pre></td></tr></table></figure><p>4 执行脚本启动minikube 搭建k8s</p><blockquote><p>执行脚本时 记得已开启VPN </p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">➜  sh run.sh</span><br><span class="line">There is a newer version of minikube available (v1.3.0).  Download it here:</span><br><span class="line">https://github.com/kubernetes/minikube/releases/tag/v1.3.0</span><br><span class="line"></span><br><span class="line">To disable this notification, run the following:</span><br><span class="line">minikube config set WantUpdateNotification false</span><br><span class="line">🔥  Deleting "minikube" from virtualbox ...</span><br><span class="line">💔  The "minikube" cluster has been deleted.</span><br><span class="line">😄  minikube v1.0.0 on darwin (amd64)</span><br><span class="line">🤹  Downloading Kubernetes v1.14.0 images in the background ...</span><br><span class="line">🔥  Creating virtualbox VM (CPUs=2, Memory=4096MB, Disk=10000MB) ...</span><br><span class="line">📶  "minikube" IP address is 192.168.99.129</span><br><span class="line">🐳  Configuring Docker as the container runtime ...</span><br><span class="line">    ▪ env http_proxy=192.168.11.234:1087</span><br><span class="line">    ▪ env https_proxy=192.168.11.234:1087</span><br><span class="line">    ▪ env no_proxy=192.168.99.0/24</span><br><span class="line">🐳  Version of container runtime is 18.06.2-ce</span><br><span class="line">⌛  Waiting for image downloads to complete ...</span><br><span class="line">✨  Preparing Kubernetes environment ...</span><br><span class="line">🚜  Pulling images required by Kubernetes v1.14.0 ...</span><br><span class="line">🚀  Launching Kubernetes v1.14.0 using kubeadm ...</span><br><span class="line">⌛  Waiting for pods: apiserver proxy etcd scheduler controller dns</span><br><span class="line">🔑  Configuring cluster permissions ...</span><br><span class="line">🤔  Verifying component health .....</span><br><span class="line">💗  kubectl is now configured to use "minikube"</span><br><span class="line">🏄  Done! Thank you for using minikube!</span><br><span class="line">➜  minikube status</span><br><span class="line">host: Running</span><br><span class="line">kubelet: Running</span><br><span class="line">apiserver: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.129</span><br><span class="line">➜  kubectl get nodes -o wide</span><br><span class="line">NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE            KERNEL-VERSION   CONTAINER-RUNTIME</span><br><span class="line">minikube   Ready    master   5m7s   v1.14.0   10.0.2.15     &lt;none&gt;        Buildroot 2018.05   4.15.0           docker://18.6.2</span><br><span class="line">➜ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://192.168.99.129:8443</span><br><span class="line">KubeDNS is running at https://192.168.99.129:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span><br><span class="line">➜  kubectl get svc -o wide</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   5m24s   &lt;none&gt;</span><br><span class="line">➜  minikube addons list</span><br><span class="line">- addon-manager: enabled</span><br><span class="line">- dashboard: enabled</span><br><span class="line">- default-storageclass: enabled</span><br><span class="line">- efk: disabled</span><br><span class="line">- freshpod: disabled</span><br><span class="line">- gvisor: disabled</span><br><span class="line">- heapster: disabled</span><br><span class="line">- ingress: enabled</span><br><span class="line">- logviewer: disabled</span><br><span class="line">- metrics-server: disabled</span><br><span class="line">- nvidia-driver-installer: disabled</span><br><span class="line">- nvidia-gpu-device-plugin: disabled</span><br><span class="line">- registry: disabled</span><br><span class="line">- registry-creds: disabled</span><br><span class="line">- storage-provisioner: enabled</span><br><span class="line">- storage-provisioner-gluster: disabled</span><br><span class="line">➜</span><br></pre></td></tr></table></figure><h2 id="利用阿里云minikube-搭建k8s环境"><a href="#利用阿里云minikube-搭建k8s环境" class="headerlink" title="利用阿里云minikube 搭建k8s环境"></a>利用阿里云minikube 搭建k8s环境</h2><p>1 阿里云minikube搭建k8s环境 博客文章<br><a href="https://yq.aliyun.com/articles/221687" target="_blank" rel="noopener">https://yq.aliyun.com/articles/221687</a></p><p>2 安装版本阿里云minikube</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.2.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</span><br></pre></td></tr></table></figure><p>3 启动minikube 搭建k8s环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ minikube start --registry-mirror=https://registry.docker-cn.com</span><br><span class="line">😄  minikube v1.2.0 on darwin (amd64)</span><br><span class="line">✅  using image repository registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">🔥  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ...</span><br><span class="line">🐳  Configuring environment for Kubernetes v1.15.0 on Docker 18.09.6</span><br><span class="line">🚜  Pulling images ...</span><br><span class="line">🚀  Launching Kubernetes ...</span><br><span class="line">⌛  Verifying: apiserver proxy etcd scheduler controller dns</span><br><span class="line">🏄  Done! kubectl is now configured to use "minikube"</span><br><span class="line">➜  ~ minikube status</span><br><span class="line">mhost: Running</span><br><span class="line">kubelet: Running</span><br><span class="line">apiserver: Running</span><br><span class="line">kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.130</span><br><span class="line">➜  ~ kubectl cluster-info</span><br><span class="line">kubeKubernetes master is running at https://192.168.99.130:8443</span><br><span class="line">KubeDNS is running at https://192.168.99.130:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span><br><span class="line">➜  ~ kubectl get nodes -o wide</span><br><span class="line">NAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE              KERNEL-VERSION   CONTAINER-RUNTIME</span><br><span class="line">minikube   Ready    master   2m41s   v1.15.0   10.0.2.15     &lt;none&gt;        Buildroot 2018.05.3   4.15.0           docker://18.9.6</span><br><span class="line">➜  minikube dashboard</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>本文总结了通过官方和阿里云两个minikube版本搭建k8s环境的方式;</li><li>平时在mac上学习k8s推荐使用阿里云minikube版本，这样更方便下载镜像;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在mac上利用minikube 搭建k8s环境或多或少会因为pull镜像失败而搭建不成功, 本文介绍两种在mac上利用minikube 搭建k8s环境的方法; 一种是通过VPN翻墙利用官方minikube 搭建k8s 环境， 另外一种是借助阿里云的minikube版本来搭建k8s环境;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="k8s" scheme="http://researchlab.github.io/categories/k8s/"/>
    
    
    <category term="minikube" scheme="http://researchlab.github.io/tags/minikube/"/>
    
  </entry>
  
  <entry>
    <title>install rabbitmq for centos7</title>
    <link href="http://researchlab.github.io/2019/06/06/install-rabbitmq/"/>
    <id>http://researchlab.github.io/2019/06/06/install-rabbitmq/</id>
    <published>2019-06-06T09:16:04.000Z</published>
    <updated>2021-02-20T17:55:59.495Z</updated>
    
    <content type="html"><![CDATA[<p>RabbitMQ is the most widely deployed open source message broker.</p><p>RabbitMQ is lightweight and easy to deploy on premises and in the cloud. It supports multiple messaging protocols. RabbitMQ can be deployed in distributed and federated configurations to meet high-scale, high-availability requirements.</p><a id="more"></a><h2 id="basic-env"><a href="#basic-env" class="headerlink" title="basic env"></a>basic env</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure><h2 id="install-list"><a href="#install-list" class="headerlink" title="install list"></a>install list</h2><ul><li><p>erlang <a href="http://erlang.org/download/otp_src_22.0.tar.gz" target="_blank" rel="noopener">otp_src_22.0.tar.gz</a></p></li><li><p>rabbitmq-server <a href="https://github.com/rabbitmq/rabbitmq-server/releases/tag/v3.7.15" target="_blank" rel="noopener">rabbitmq-server-generic-unix-3.7.15.tar.xz</a></p></li></ul><blockquote><p>rabbitmq-server 版本要与erlang 版本相匹配，否则rabbitmq-server 启动失败，会提示<code>noproc</code>;</p></blockquote><h2 id="pre-env"><a href="#pre-env" class="headerlink" title="pre-env"></a>pre-env</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel</span><br></pre></td></tr></table></figure><blockquote><p>如果提示要安装 wxWidgets wx-config, 安装如下,</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/wxWidgets/wxWidgets/releases/download/v3.0.4/wxWidgets-3.0.4.tar.bz2</span><br><span class="line"></span><br><span class="line">yum install -y bzip2</span><br><span class="line"></span><br><span class="line">tar -jxvf wxWidgets-3.1.2.tar.bz2 &amp;&amp; cd wxWidgets-3.1.2</span><br><span class="line"></span><br><span class="line">./configure --prefix=/opt/wx &amp;&amp; make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line">export WX=/opt/wx</span><br><span class="line">export PATH=$PATH:$WX/bin:$WX/lib</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="install-erlang"><a href="#install-erlang" class="headerlink" title="install erlang"></a>install erlang</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf otp_src_22.0.tar.gz  &amp;&amp; cd otp_src_22.0</span><br><span class="line"></span><br><span class="line">./configure --prefix=/opt/erlang --without-javac</span><br><span class="line"></span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line">export ERLANG=/opt/erlang</span><br><span class="line">export PATH=$PATH:$ERLANG/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>验证erlang 是否按照成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># erl version</span><br><span class="line">Erlang/OTP 22 [erts-10.4] [source] [64-bit] [smp:16:16] [ds:16:16:10] [async-threads:1] [hipe]</span><br><span class="line"></span><br><span class="line">Eshell V10.4  (abort with ^G)</span><br><span class="line">1&gt; halt().</span><br><span class="line">#</span><br></pre></td></tr></table></figure><h2 id="install-rabbitmq-server"><a href="#install-rabbitmq-server" class="headerlink" title="install rabbitmq-server"></a>install rabbitmq-server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf rabbitmq-server-generic-unix-3.7.15.tar.xz &amp;&amp; mv rabbitmq-server-generic-unix-3.7.15 rabbitmq</span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:/opt/rabbitmq/sbin</span><br><span class="line">export RABBITMQ_HOME=/opt/rabbitmq</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="start-rabbitmq-server"><a href="#start-rabbitmq-server" class="headerlink" title="start rabbitmq-server"></a>start rabbitmq-server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmq-server -detached</span><br><span class="line"></span><br><span class="line">rabbitmqctl status</span><br><span class="line"></span><br><span class="line">rabbitmqctl cluster_status</span><br></pre></td></tr></table></figure><h2 id="enable-rabbitmq-web-management"><a href="#enable-rabbitmq-web-management" class="headerlink" title="enable rabbitmq-web-management"></a>enable rabbitmq-web-management</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl add_user admin 123456</span><br><span class="line">rabbitmqctl set_user tags admin administrator</span><br><span class="line">rabbitmqctl set_user_tags admin administrator</span><br><span class="line">rabbitmq-plugins enable rabbitmq_management</span><br></pre></td></tr></table></figure><p>上述操作后， 就可以在浏览器端访问 <a href="http://ip:15672" target="_blank" rel="noopener">http://ip:15672</a> 然后通过admin 123456 登录了；</p><h2 id="config"><a href="#config" class="headerlink" title="config"></a>config</h2><blockquote><p>官方参考文档: <a href="https://www.rabbitmq.com/configure.html#configuration-file" target="_blank" rel="noopener">https://www.rabbitmq.com/configure.html#configuration-file</a></p></blockquote><p>配置文件简单理解就是创建俩文件rabbitmq-env.conf，rabbitmq.config然后都扔到/etc/rabbitmq目录下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@test02 rabbitmq]# pwd</span><br><span class="line">/opt/rabbitmq/etc/rabbitmq</span><br><span class="line">[root@test02 rabbitmq]# ls</span><br><span class="line">enabled_plugins  rabbitmq.config  rabbitmq-env.conf</span><br><span class="line">[root@test02 rabbitmq]# more rabbitmq-env.conf</span><br><span class="line">RABBITMQ_MNESIA_BASE=/usr/local/rabbitmq-server/data</span><br><span class="line">RABBITMQ_LOG_BASE=/usr/local/rabbitmq-server/log</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;RabbitMQ is the most widely deployed open source message broker.&lt;/p&gt;
&lt;p&gt;RabbitMQ is lightweight and easy to deploy on premises and in the cloud. It supports multiple messaging protocols. RabbitMQ can be deployed in distributed and federated configurations to meet high-scale, high-availability requirements.&lt;/p&gt;</summary>
    
    
    
    <category term="MQ" scheme="http://researchlab.github.io/categories/MQ/"/>
    
    
    <category term="rabbitmq" scheme="http://researchlab.github.io/tags/rabbitmq/"/>
    
  </entry>
  
  <entry>
    <title>缓存穿透/击穿/雪崩 - 解决方案分析</title>
    <link href="http://researchlab.github.io/2019/02/26/cache-invalidation-analysis-solution/"/>
    <id>http://researchlab.github.io/2019/02/26/cache-invalidation-analysis-solution/</id>
    <published>2019-02-26T01:53:45.000Z</published>
    <updated>2021-02-20T17:55:59.460Z</updated>
    
    <content type="html"><![CDATA[<p>设计一个缓存系统，不得不要考虑的问题就是: 缓存穿透、缓存击穿与失效时的雪崩效应。<br><a id="more"></a></p><h2 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h2><p>缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。</p><h2 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h2><p>缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。</p><h2 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h2><p>缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线 程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。这里分享一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。</p><h2 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h2><p>对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题: 缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。</p><p>缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。</p><h2 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h2><p>1.使用互斥锁(mutex key)<br>业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。</p><p>SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现锁的效果。在redis2.6.1之前版本未实现setnx的过期时间，所以这里给出两种版本代码参考: </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//2.6.1前单机版本锁</span></span><br><span class="line"><span class="function">String <span class="title">get</span><span class="params">(String key)</span> </span>&#123;  </span><br><span class="line">   String value = redis.get(key);  </span><br><span class="line">   <span class="keyword">if</span> (value  == null) &#123;  </span><br><span class="line">    <span class="keyword">if</span> (redis.setnx(key_mutex, <span class="string">"1"</span>)) &#123;  </span><br><span class="line">        <span class="comment">// 3 min timeout to avoid mutex holder crash  </span></span><br><span class="line">        redis.expire(key_mutex, <span class="number">3</span> * <span class="number">60</span>)  </span><br><span class="line">        value = db.get(key);  </span><br><span class="line">        redis.<span class="built_in">set</span>(key, value);  </span><br><span class="line">        redis.<span class="keyword">delete</span>(key_mutex);  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="comment">//其他线程休息50毫秒后重试  </span></span><br><span class="line">        Thread.sleep(<span class="number">50</span>);  </span><br><span class="line">        get(key);  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最新版本代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">get</span><span class="params">(key)</span> </span>&#123;</span><br><span class="line">      String value = redis.get(key);</span><br><span class="line">      <span class="keyword">if</span> (value == null) &#123; <span class="comment">//代表缓存值过期</span></span><br><span class="line">          <span class="comment">//设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db</span></span><br><span class="line">  <span class="keyword">if</span> (redis.setnx(key_mutex, <span class="number">1</span>, <span class="number">3</span> * <span class="number">60</span>) == <span class="number">1</span>) &#123;  <span class="comment">//代表设置成功</span></span><br><span class="line">               value = db.get(key);</span><br><span class="line">                      redis.<span class="built_in">set</span>(key, value, expire_secs);</span><br><span class="line">                      redis.del(key_mutex);</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;  <span class="comment">//这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可</span></span><br><span class="line">                      sleep(<span class="number">50</span>);</span><br><span class="line">                      get(key);  <span class="comment">//重试</span></span><br><span class="line">              &#125;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="keyword">return</span> value;</span><br><span class="line">          &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>memcache代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (memcache.get(key) == null) &#123;  </span><br><span class="line">    <span class="comment">// 3 min timeout to avoid mutex holder crash  </span></span><br><span class="line">    <span class="keyword">if</span> (memcache.add(key_mutex, <span class="number">3</span> * <span class="number">60</span> * <span class="number">1000</span>) == <span class="literal">true</span>) &#123;  </span><br><span class="line">        value = db.get(key);  </span><br><span class="line">        memcache.<span class="built_in">set</span>(key, value);  </span><br><span class="line">        memcache.<span class="keyword">delete</span>(key_mutex);  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        sleep(<span class="number">50</span>);  </span><br><span class="line">        retry();  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="“提前”使用互斥锁-mutex-key-："><a href="#“提前”使用互斥锁-mutex-key-：" class="headerlink" title="“提前”使用互斥锁(mutex key)："></a>“提前”使用互斥锁(mutex key)：</h2><p>在value内部设置1个超时值(timeout1), timeout1比实际的memcache timeout(timeout2)小。当从cache读取到timeout1发现它已经过期时候，马上延长timeout1并重新设置到cache。然后再从数据库加载数据并设置到cache中。伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">v = memcache.get(key);  </span><br><span class="line">if (v == null) &#123;  </span><br><span class="line">    if (memcache.add(key_mutex, 3 * 60 * 1000) == true) &#123;  </span><br><span class="line">        value = db.get(key);  </span><br><span class="line">        memcache.set(key, value);  </span><br><span class="line">        memcache.delete(key_mutex);  </span><br><span class="line">    &#125; else &#123;  </span><br><span class="line">        sleep(50);  </span><br><span class="line">        retry();  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125; else &#123;  </span><br><span class="line">    if (v.timeout &lt;= now()) &#123;  </span><br><span class="line">        if (memcache.add(key_mutex, 3 * 60 * 1000) == true) &#123;  </span><br><span class="line">            // extend the timeout for other threads  </span><br><span class="line">            v.timeout += 3 * 60 * 1000;  </span><br><span class="line">            memcache.set(key, v, KEY_TIMEOUT * 2);  </span><br><span class="line">  </span><br><span class="line">            // load the latest value from db  </span><br><span class="line">            v = db.get(key);  </span><br><span class="line">            v.timeout = KEY_TIMEOUT;  </span><br><span class="line">            memcache.set(key, value, KEY_TIMEOUT * 2);  </span><br><span class="line">            memcache.delete(key_mutex);  </span><br><span class="line">        &#125; else &#123;  </span><br><span class="line">            sleep(50);  </span><br><span class="line">            retry();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="“永远不过期”："><a href="#“永远不过期”：" class="headerlink" title="“永远不过期”："></a>“永远不过期”：</h2><p>这里的“永远不过期”包含两层意思：</p><blockquote><p>(1) 从redis上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。</p></blockquote><blockquote><p>(2) 从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期</p></blockquote><p> 从实战看，这种方法对于性能非常友好，唯一不足的就是构建缓存时候，其余线程(非构建缓存的线程)可能访问的是老数据，但是对于一般的互联网功能来说这个还是可以忍受。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">String get(final String key) &#123;</span><br><span class="line">        V v = redis.get(key);</span><br><span class="line">        String value = v.getValue();</span><br><span class="line">        long timeout = v.getTimeout();</span><br><span class="line">        if (v.timeout &lt;= System.currentTimeMillis()) &#123;</span><br><span class="line">            // 异步更新后台异常执行</span><br><span class="line">            threadPool.execute(new Runnable() &#123;</span><br><span class="line">                public void run() &#123;</span><br><span class="line">                    String keyMutex = &quot;mutex:&quot; + key;</span><br><span class="line">                    if (redis.setnx(keyMutex, &quot;1&quot;)) &#123;</span><br><span class="line">                        // 3 min timeout to avoid mutex holder crash</span><br><span class="line">                        redis.expire(keyMutex, 3 * 60);</span><br><span class="line">                        String dbValue = db.get(key);</span><br><span class="line">                        redis.set(key, dbValue);</span><br><span class="line">                        redis.delete(keyMutex);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        return value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="资源保护："><a href="#资源保护：" class="headerlink" title="资源保护："></a>资源保护：</h2><p>采用netflix的hystrix，可以做资源的隔离保护主线程池，如果把这个应用到缓存的构建也未尝不可。</p><p>四种解决方案：没有最佳只有最合适</p><table><thead><tr><th>解决方案</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>简单分布式互斥锁（mutex key）</td><td>1.思路简单<br>2.保证一致性</td><td>1.代码复杂度增大<br>2.存在死锁的风险<br>3.存在线程池阻塞的风险</td></tr><tr><td>“提前”使用互斥锁</td><td>1.保证一致性<br></td><td>同上</td></tr><tr><td>不过期(本文)</td><td>1.异步构建缓存, 不会阻塞线程池</td><td>1.不保证一致性。<br>2.代码复杂度增大(每个value都要维护一个timekey)。<br>3.占用一定的内存空间(每个value都要维护一个timekey)。</td></tr><tr><td>资源隔离组件hystrix(本文)</td><td>1.hystrix技术成熟,有效保证后端。<br>2.hystrix监控强大。</td><td>1. 部分访问存在降级策略。</td></tr></tbody></table><p>四种方案来源网络，详文请链接：<a href="http://carlosfu.iteye.com/blog/2269687?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io" target="_blank" rel="noopener">http://carlosfu.iteye.com/blog/2269687?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>针对业务系统，永远都是具体情况具体分析，没有最好，只有最合适。<br>最后，对于缓存系统常见的缓存满了和数据丢失问题，需要根据具体业务分析，通常我们采用LRU策略处理溢出，Redis的RDB和AOF持久化策略来保证一定情况下的数据安全。</p><h2 id="参考来源"><a href="#参考来源" class="headerlink" title="参考来源"></a>参考来源</h2><p>[1] <a href="https://blog.csdn.net/zeb_perfect/article/details/54135506" target="_blank" rel="noopener">https://blog.csdn.net/zeb_perfect/article/details/54135506</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;设计一个缓存系统，不得不要考虑的问题就是: 缓存穿透、缓存击穿与失效时的雪崩效应。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="DevOps" scheme="http://researchlab.github.io/categories/DevOps/"/>
    
    
    <category term="cache-invalid" scheme="http://researchlab.github.io/tags/cache-invalid/"/>
    
  </entry>
  
  <entry>
    <title>从生产者消费者模型深入学习golang channel</title>
    <link href="http://researchlab.github.io/2018/11/08/producer-consumer-go-channel/"/>
    <id>http://researchlab.github.io/2018/11/08/producer-consumer-go-channel/</id>
    <published>2018-11-08T11:53:14.000Z</published>
    <updated>2021-02-20T17:55:59.521Z</updated>
    
    <content type="html"><![CDATA[<p>从生产者消费者模型探究回顾golang channel注意事项; 实例探究no buffer及buffer channel;<br><a id="more"></a></p><h2 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h2><p>golang channel 分为无缓冲channel及缓冲channel, 具体体现在创建channel时是否制定channel size, </p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//no buffer channel</span></span><br><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//buffer channel</span></span><br><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span> bufSize)</span><br></pre></td></tr></table></figure><ul><li>无缓冲channel 默认channel大小为1, 写入一个值后需要被读取之后才能继续写入, 否则写阻塞;</li><li>缓冲channel 的大小是初始时的bufSize, 可连续写入bufSize值, 然后等待读取, 当len(channel) &lt; bufSize时才可以继续写入, 否则写阻塞;</li><li>channel 中没有值时 则读阻塞;</li><li>channel 常用在同步, pipe, 无锁设计等场景中;</li></ul><h3 id="写值"><a href="#写值" class="headerlink" title="写值"></a>写值</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ch &lt;- in</span><br></pre></td></tr></table></figure><ul><li>非写阻塞时可以写入值;</li></ul><h3 id="读值"><a href="#读值" class="headerlink" title="读值"></a>读值</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out := &lt;-ch </span><br><span class="line"></span><br><span class="line">out, ok := &lt;- ch</span><br></pre></td></tr></table></figure><ul><li>非读阻塞时可以读取值;</li><li>当ok 为false时, 表示channel已被关闭;</li></ul><h3 id="关闭"><a href="#关闭" class="headerlink" title="关闭"></a>关闭</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">close</span>(ch)</span><br></pre></td></tr></table></figure><ul><li>只要当写channel写入完毕后则应立即关闭channel; 而读channel则可以不需要处理;</li></ul><blockquote><p>更多参考<a href="https://tour.golang.org/concurrency/4" target="_blank" rel="noopener">Range and close</a></p></blockquote><h3 id="for循环读"><a href="#for循环读" class="headerlink" title="for循环读"></a>for循环读</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x := <span class="keyword">range</span> ch &#123;</span><br><span class="line">        <span class="comment">//do something with x</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><ul><li>当使用for range 从管道读取数据的时候，管道没有数据，那么循环会阻塞，只有当管道被关闭的时候，for 循环才会结束</li></ul><h2 id="生产者消费者模型"><a href="#生产者消费者模型" class="headerlink" title="生产者消费者模型"></a>生产者消费者模型</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//producer_consumer.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">N = <span class="number">100000</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Producer</span><span class="params">(out <span class="keyword">chan</span>&lt;- <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">1</span>; i &lt; N; i++ &#123;</span><br><span class="line">out &lt;- i</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">close</span>(out)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Consumer</span><span class="params">(in &lt;-<span class="keyword">chan</span> <span class="keyword">int</span>, out <span class="keyword">chan</span>&lt;- <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> x := <span class="keyword">range</span> in &#123;</span><br><span class="line">out &lt;- x</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">close</span>(out)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试case </p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//producer_consumer_test.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">producerconsumer</span><span class="params">(in <span class="keyword">chan</span> <span class="keyword">int</span>, out <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">go</span> Producer(in)</span><br><span class="line"><span class="keyword">go</span> Consumer(in, out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x := <span class="keyword">range</span> out &#123;</span><br><span class="line">_ = x</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestNoBufferChan</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">in, out := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>), <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">producerconsumer(in, out)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestBufferChan</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line">bufLen := <span class="number">100</span></span><br><span class="line">in, out := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, bufLen), <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, bufLen)</span><br><span class="line">producerconsumer(in, out)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoBufferChan</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">in, out := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>), <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">producerconsumer(in, out)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkBufferChan</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">bufLen := <span class="number">100</span></span><br><span class="line">in, out := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, bufLen), <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, bufLen)</span><br><span class="line">producerconsumer(in, out)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>output </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">➜  make bench</span><br><span class="line">[Benchmark] running Benchmark</span><br><span class="line">=== RUN   TestCloseWriteChanException</span><br><span class="line">--- PASS: TestCloseWriteChanException (2.01s)</span><br><span class="line">=== RUN   TestCloseReadChanException</span><br><span class="line">--- PASS: TestCloseReadChanException (1.00s)</span><br><span class="line">=== RUN   TestNoBufferChan</span><br><span class="line">--- PASS: TestNoBufferChan (0.09s)</span><br><span class="line">=== RUN   TestBufferChan</span><br><span class="line">--- PASS: TestBufferChan (0.02s)</span><br><span class="line">goos: darwin</span><br><span class="line">goarch: amd64</span><br><span class="line">pkg: github.com/researchlab/experiments/channel</span><br><span class="line">BenchmarkNoBufferChan-8         20  84429085 ns/op</span><br><span class="line">BenchmarkBufferChan-8           50  22295688 ns/op</span><br><span class="line">PASS</span><br><span class="line">ok  github.com/researchlab/experiments/channel6.045s</span><br></pre></td></tr></table></figure><p>更多分析及完整源码可参见 <a href="https://github.com/researchlab/experiments/tree/master/channel" target="_blank" rel="noopener">github-channel</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>回顾了channel两种类型(no buffer/buffer channel);</li><li>只有写channel需要close, 而读channel 则可以不需要关心; 需要注意channel 在多个goroutine中的close问题;</li><li>channel 是非常常用的一种结果, 常见于业务同步, buffer pipe, timeout, 无锁设计等场景中; </li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;从生产者消费者模型探究回顾golang channel注意事项; 实例探究no buffer及buffer channel;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="golang" scheme="http://researchlab.github.io/categories/golang/"/>
    
    
    <category term="golang" scheme="http://researchlab.github.io/tags/golang/"/>
    
    <category term="channel" scheme="http://researchlab.github.io/tags/channel/"/>
    
  </entry>
  
  <entry>
    <title>mysql专题16 事务隔离级别及ACID知识回顾</title>
    <link href="http://researchlab.github.io/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/"/>
    <id>http://researchlab.github.io/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/</id>
    <published>2018-11-07T11:10:48.000Z</published>
    <updated>2021-02-20T17:55:59.514Z</updated>
    
    <content type="html"><![CDATA[<p>事务指的是满足<code>ACID</code>特性的一组操作,<code>mysql</code>中可以通过<code>commit</code>提交一个事务,也可以使用<code>rollback</code>进行回滚。 在并发场景中很难保证事务的<code>Isolation</code>特性, 即无法保证临界资源的排它性操作, 从而引发数据一致性问题, 临界资源互斥问题显然需要借助加锁来解决, 在并发事务中就需要用锁的并发控制来处理;<br><a id="more"></a><br>根据在事务处理中对临界资源加锁及释放锁的阶段不同,可分为三种加锁方式, 即<code>mysql</code>的<code>三级封锁协议</code>, <code>三级封锁协议</code>可分别解决数据丢失, 脏读, 不可重复读问题,即<code>mysql</code>事务隔离级别的读未提交, 读已提交, 可重读; </p><p>此外还有第四种隔离级别, 即事务串行化, 即把并行转换为串行; 下文将通过实际案例分析精要回顾事务及事务隔离级别等相关知识; </p><blockquote><p> 实际情况下, 在读已提交及可重读读两中隔离级别下, <code>mysql</code>/Oracle/PgSQL等是利用MVCC来处理事务，防止加锁，来提高访问效率;</p></blockquote><h2 id="事务命令"><a href="#事务命令" class="headerlink" title="事务命令"></a>事务命令</h2><h3 id="查看当前隔离级别"><a href="#查看当前隔离级别" class="headerlink" title="查看当前隔离级别"></a>查看当前隔离级别</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dev@testdb&gt;show variables like '%iso%';</span><br><span class="line">+<span class="comment">-----------------------+-----------------+</span></span><br><span class="line">| Variable_name         | Value           |</span><br><span class="line">+<span class="comment">-----------------------+-----------------+</span></span><br><span class="line">| transaction_isolation | REPEATABLE-READ |</span><br><span class="line">+<span class="comment">-----------------------+-----------------+</span></span><br><span class="line"></span><br><span class="line">dev@testdb&gt;select @@transaction_isolation;</span><br><span class="line">+<span class="comment">-------------------------+</span></span><br><span class="line">| @@transaction_isolation |</span><br><span class="line">+<span class="comment">-------------------------+</span></span><br><span class="line">| REPEATABLE-READ         |</span><br><span class="line">+<span class="comment">-------------------------+</span></span><br><span class="line">1 row in <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="设置事务隔离级别"><a href="#设置事务隔离级别" class="headerlink" title="设置事务隔离级别"></a>设置事务隔离级别</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dev@testdb&gt;set transaction isolation level repeatable read;</span><br><span class="line">dev@testdb&gt;set transaction_isolation='read-uncommitted';</span><br><span class="line">dev@testdb&gt;set transaction_isolation='read-committed';</span><br><span class="line">dev@testdb&gt;set transaction_isolation='serializable';</span><br><span class="line">dev@testdb&gt;set transaction_isolation='repeatable-read';</span><br></pre></td></tr></table></figure><h3 id="查看系统锁情况"><a href="#查看系统锁情况" class="headerlink" title="查看系统锁情况"></a>查看系统锁情况</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dev@testdb&gt;show status like 'innodb_row_lock%';</span><br><span class="line">+<span class="comment">-------------------------------+-------+</span></span><br><span class="line">| Variable_name                 | Value |</span><br><span class="line">+<span class="comment">-------------------------------+-------+</span></span><br><span class="line">| Innodb_row_lock_current_waits | 0     |</span><br><span class="line">| Innodb_row_lock_time          | 0     |</span><br><span class="line">| Innodb_row_lock_time_avg      | 0     |</span><br><span class="line">| Innodb_row_lock_time_max      | 0     |</span><br><span class="line">| Innodb_row_lock_waits         | 0     |</span><br><span class="line">+<span class="comment">-------------------------------+-------+</span></span><br><span class="line">5 rows in <span class="keyword">set</span> (<span class="number">0.04</span> sec)</span><br></pre></td></tr></table></figure><ul><li>查看当前系统锁的情况, 当系统锁争用比较严重的时候, <code>Innodb_row_lock_waits</code>和<code>Innodb_row_lock_time_avg</code>的值会比较高;</li></ul><h3 id="事务提交及回滚"><a href="#事务提交及回滚" class="headerlink" title="事务提交及回滚"></a>事务提交及回滚</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">commit, rollback用来确保数据库有足够的剩余空间;</span></span><br><span class="line"><span class="comment">commit,rollback只能用于DML操作, 即insert、update、delet;</span></span><br><span class="line"><span class="comment">rollback操作撤销上一个commit、rollback之后的事务。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span></span><br><span class="line">(</span><br><span class="line"> PROD_ID <span class="built_in">varchar</span>(<span class="number">10</span>) <span class="keyword">not</span> <span class="literal">null</span>,</span><br><span class="line"> PROD_DESC <span class="built_in">varchar</span>(<span class="number">25</span>)  <span class="literal">null</span>,</span><br><span class="line"> <span class="keyword">COST</span> <span class="built_in">decimal</span>(<span class="number">6</span>,<span class="number">2</span>)  <span class="literal">null</span></span><br><span class="line">);</span><br><span class="line"> </span><br><span class="line"><span class="comment">/*禁止自动提交*/</span></span><br><span class="line"><span class="keyword">set</span> autocommit=<span class="number">0</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/*设置事务特性,必须在所有事务开始前设置*/</span></span><br><span class="line"><span class="keyword">set</span> <span class="keyword">transaction</span> <span class="keyword">read</span> <span class="keyword">only</span>;  <span class="comment">/*设置事务只读*/</span></span><br><span class="line"><span class="keyword">set</span> <span class="keyword">transaction</span> <span class="keyword">read</span> write;  <span class="comment">/*设置事务可读、写*/</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">/*开始一次事务*/</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">values</span>(<span class="string">'4456'</span>,<span class="string">'mr right'</span>,<span class="number">46.97</span>);</span><br><span class="line"><span class="keyword">commit</span>;     <span class="comment">/*位置1*/</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">values</span>(<span class="string">'3345'</span>,<span class="string">'mr wrong'</span>,<span class="number">54.90</span>);</span><br><span class="line"><span class="keyword">rollback</span>;    <span class="comment">/*回到位置1,(位置2);上次commit处*/</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">values</span>(<span class="string">'1111'</span>,<span class="string">'mr wan'</span>,<span class="number">89.76</span>);</span><br><span class="line"><span class="keyword">rollback</span>;    <span class="comment">/*回到位置2,上次rollback处*/</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">/*测试保存点savepoint*/</span></span><br><span class="line"><span class="keyword">savepoint</span> point1;</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">set</span> PROD_ID=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">rollback</span> <span class="keyword">to</span> point1;  <span class="comment">/*回到保存点point1*/</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">release</span> <span class="keyword">savepoint</span> point1; <span class="comment">/*删除保存点*/</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure><h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><h3 id="原子性-Atomicity"><a href="#原子性-Atomicity" class="headerlink" title="原子性(Atomicity)"></a><strong>原子性(Atomicity)</strong></h3><p>原子性是指事务包含的所有操作要么全部成功,要么全部失败回滚。失败回滚的操作事务,将不能对事务有任何影响。</p><h3 id="一致性-Consistency"><a href="#一致性-Consistency" class="headerlink" title="一致性(Consistency)"></a><strong>一致性(Consistency)</strong></h3><p>一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态,也就是说一个事务执行之前和执行之后都必须处于一致性状态。<br>例如: A和B进行转账操作,A有200块钱,B有300块钱;当A转了100块钱给B之后,他们2个人的总额还是500块钱,不会改变。</p><h3 id="隔离性-Isolation"><a href="#隔离性-Isolation" class="headerlink" title="隔离性(Isolation)"></a><strong>隔离性(Isolation)</strong></h3><p>隔离性是指当多个用户并发访问数据库时,比如同时访问一张表,数据库每一个用户开启的事务,不能被其他事务所做的操作干扰(也就是事务之间的隔离),<font color="red">多个并发事务之间,应当相互隔离</font>。<br>例如:同时有T1和T2两个并发事务,从T1角度来看,T2要不在T1执行之前就已经结束,要么在T1执行完成后才开始。将多个事务隔离开,每个事务都不能访问到其他事务操作过程中的状态;就好比上锁操作,只有一个事务做完了,另外一个事务才能执行。</p><h3 id="持久性-Durability"><a href="#持久性-Durability" class="headerlink" title="持久性(Durability)"></a><strong>持久性(Durability)</strong></h3><p>持久性是指事务的操作,一旦提交,对于数据库中数据的改变是永久性的,即使数据库发生故障也不能丢失已提交事务所完成的改变。</p><h2 id="隔离级别"><a href="#隔离级别" class="headerlink" title="隔离级别"></a>隔离级别</h2><h3 id="未提交读-READ-UNCOMMITTED"><a href="#未提交读-READ-UNCOMMITTED" class="headerlink" title="未提交读(READ UNCOMMITTED)"></a><strong>未提交读(READ UNCOMMITTED)</strong></h3><p>不存在事务隔离级别的时, 会造成数据丢失问题,<br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/ru0.png" alt=""><br>很明显的看出,旺财对A添加的20块不翼而飞了,这就是”数据丢失”,对事务不加任何锁(不存在事务隔离),就会导致这种问题。</p><p>未提交读事务隔离级别,<br>未提交事务隔离级别满足<font color="red">一级封锁协议, 即写数据的时候添加一个X锁(排他锁),也就是在写数据的时候不允许其他事务进行写操作,但是读不受限制,读不加锁</font>。<br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/ru1.png" alt=""><br>这样就可以解决了多个人一起写数据而导致了”数据丢失”的问题,但是会引发新的问题——脏读。</p><p><font color="red">脏读:读取了别人未提交的数据</font>。<br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/ru2.png" alt=""><br>因而引入了另外一个事务隔离级别——读已提交,</p><h3 id="读已提交-READ-COMMITTED"><a href="#读已提交-READ-COMMITTED" class="headerlink" title="读已提交(READ COMMITTED)"></a><strong>读已提交(READ COMMITTED)</strong></h3><p>读已提交满足<font color="red">二级封锁协议, 即写数据的时候加上X锁(排他锁),读数据的时候添加S锁(共享锁),且如果一个数据加了X锁就没法加S锁;同理如果加了S锁就没法加X锁,但是一个数据可以同时存在多个S锁(因为只是读数据),并且规定S锁读取数据,一旦读取完成就立刻释放S锁(不管后续是否还有很多其他的操作,只要是读取了S锁的数据后,就立刻释放S锁)。</font><br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/rc1.png" alt=""><br>这样就解决了脏读的问题,但是又有新的问题出现——不可重复读。</p><p>不可重复读:同一个事务对数据的多次读取的结果不一致。<br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/rc2.png" alt=""><br>解决方法——引入隔离级别更高事务隔离:可重复读</p><h3 id="可重复读-REPEATABLE-READ"><a href="#可重复读-REPEATABLE-READ" class="headerlink" title="可重复读(REPEATABLE READ)"></a><strong>可重复读(REPEATABLE READ)</strong></h3><p>可重复读满足<font color="red">第三级封锁协议, 即对S锁进行修改,之前的S锁是:读取了数据之后就立刻释放S锁,现在修改是:在读取数据的时候加上S锁,但是要直到事务准备提交了才释放该S锁,X锁还是一致。</font><br><img src="/2018/11/07/mysql-16-transaction-isolation-level-and-acid-review/rr1.png" alt=""><br>这样就解决了不可重复读的问题了,但是又有新的问题出现——幻读。</p><p>例如: 有一次旺财对一个”学生表”进行操作,选取了年龄是18岁的所有行, 用X锁锁住, 并且做了修改。</p><p>改完以后旺财再次选择所有年龄是18岁的行, 想做一个确认, 没想到有一行竟然没有修改！</p><p>原来就在旺财查询并修改的的时候, 小强也对学生表进行操作, 他插入了一个新的行,其中的年龄也是18岁！ 虽然两个人的修改都没有问题, 互不影响, 但最终结果并非预期, 即幻读问题;</p><p>解决幻读的方式——串行化</p><h3 id="可串行化-SERIALIZABLE"><a href="#可串行化-SERIALIZABLE" class="headerlink" title="可串行化(SERIALIZABLE)"></a><strong>可串行化(SERIALIZABLE)</strong></h3><p>事务只能一件一件的进行,不能并发进行。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><table><thead><tr><th style="text-align:left">隔离级别</th><th style="text-align:left">数据丢失</th><th style="text-align:left">脏读</th><th style="text-align:left">不可重复读</th><th style="text-align:left">幻读</th></tr></thead><tbody><tr><td style="text-align:left">读未提交</td><td style="text-align:left">NO</td><td style="text-align:left">YES</td><td style="text-align:left">YES</td><td style="text-align:left">YES</td></tr><tr><td style="text-align:left">读已提交</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td><td style="text-align:left">YES</td><td style="text-align:left">YES</td></tr><tr><td style="text-align:left">可重复读</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td><td style="text-align:left">YES</td></tr><tr><td style="text-align:left">事务串行化执行</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td><td style="text-align:left">NO</td></tr></tbody></table><blockquote><p><code>mysql</code>默认的隔离级别是:可重复读。</p></blockquote><blockquote><p>oracle中只支持2个隔离级别:读已提交和串行化, 默认是读已提交。</p></blockquote><blockquote><p>隔离级别的设置只对当前链接有效;</p></blockquote><h2 id="锁粒度"><a href="#锁粒度" class="headerlink" title="锁粒度"></a>锁粒度</h2><p><code>mysql</code>不同存储引擎支持的锁粒度不同, InnoDB存储引擎支持表锁及行锁, InnoDB存储引擎可支持三种行锁定方式, <font color="red">默认加锁方式是next-key 锁</font>。</p><ol><li>行锁(Record Lock):锁直接加在索引记录上面，锁住的是key。 行锁又分为共享锁(S)与排他锁(X);</li><li>间隙锁(Gap Lock):锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而已的。</li><li>Next-Key Lock: 行锁和间隙锁组合起来就叫Next-Key Lock。 </li></ol><p>默认情况下，InnoDB工作在可重复读(Repeatable Read)隔离级别下，并且会以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上行锁（Record Lock），再对索引记录两边的间隙加上间隙锁（Gap Lock）。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。</p><h2 id="间隙锁"><a href="#间隙锁" class="headerlink" title="间隙锁"></a>间隙锁</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>间隙锁指对某个记录行范围进行锁定, 如锁定第一行到第三行, 即锁定(1,3) 那第一行至第三行中间就不能进行写入操作, 同时第一行至第三行数据不能进行update/delete等任何修改操作;</p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>间隙锁的目的是为了防止幻读，其主要通过两个方面实现这个目:</p><ol><li>防止间隙内有新数据被插入;</li><li>防止已存在的数据, 更新成间隙内的数据;</li></ol><p>间隙锁在InnoDB的唯一作用就是防止其它事务的插入操作，以此来达到防止幻读的发生，所以间隙锁不分什么共享锁与排它锁。 默认情况下，InnoDB工作在Repeatable Read隔离级别下，并且以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁与间隙锁的组合，当对数据进行条件，范围检索时，对其范围内也许并存在的值进行加锁！当查询的索引含有唯一属性（唯一索引，主键索引）时，Innodb存储引擎会对next-key lock进行优化，将其降为record lock,即仅锁住索引本身，而不是范围！若是普通辅助索引，则会使用传统的next-key lock进行范围锁定！</p><p>要禁止间隙锁的话，可以把隔离级别降为Read Committed，或者开启参数innodb_locks_unsafe_for_binlog。</p><h2 id="RR级别-MVCC-GL-解决幻读问题"><a href="#RR级别-MVCC-GL-解决幻读问题" class="headerlink" title="RR级别+MVCC+GL 解决幻读问题"></a>RR级别+MVCC+GL 解决幻读问题</h2><p>在MVCC并发控制中，读操作可以分成两类:快照读 (snapshot read)与当前读 (current read)。</p><ol><li>快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。</li><li>当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。 </li></ol><p>在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以<code>mysql</code> InnoDB为例: </p><p>快照读:简单的select操作，属于快照读，不加锁。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> ?;</span><br></pre></td></tr></table></figure><p>当前读:特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> ? <span class="keyword">lock</span> <span class="keyword">in</span> <span class="keyword">share</span> <span class="keyword">mode</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> ? <span class="keyword">for</span> <span class="keyword">update</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> <span class="keyword">values</span> (…);</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">table</span> <span class="keyword">set</span> ? <span class="keyword">where</span> ?;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> ?;</span><br></pre></td></tr></table></figure><p>所有以上的语句，都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。 </p><p>回顾4种隔离级别:<br>Read Uncommited<br>可以读取未提交记录。此隔离级别，不会使用，忽略。</p><p>Read Committed (RC)<br>快照读忽略，本文不考虑。</p><p>针对当前读，RC隔离级别保证对读取到的记录加锁 (record lock)，存在幻读现象。</p><p>Repeatable Read (RR)<br>快照读忽略，本文不考虑。</p><p>针对当前读，RR隔离级别保证对读取到的记录加锁 (记录锁)，同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入 (间隙锁)，不存在幻读现象。</p><p>Serializable<br>从MVCC并发控制退化为基于锁的并发控制。不区别快照读与当前读，所有的读操作均为当前读，读加读锁 (S锁)，写加写锁 (X锁)。</p><p>Serializable隔离级别下，读写冲突，因此并发度急剧下降，在<code>mysql/InnoDB</code>下不建议使用。<br>对于快照读来说，幻读的解决是依赖mvcc解决。而对于当前读则依赖于gap-lock解决。</p><p>此外, 对于快照读来说，幻读的解决是依赖mvcc解决。而对于当前读则依赖于gap-lock解决。</p><p>更多参考: <a href="https://www.cnblogs.com/aspirant/p/6920987.html" target="_blank" rel="noopener">Mysql 的InnoDB事务方面的 多版本并发控制如何实现 MVCC</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>回顾了事务相关操作命令;</li><li>回顾了事务的ACID特性及四级隔离级别, 三级封锁协议相关知识;</li><li>回顾锁粒度(表锁/行锁(S/X锁)), 及行锁的三种方式(RL,GL, NKL);</li><li>进一步回顾了Gap Lock相关知识;</li><li>回顾了Next Key Lock + MVCC 在RR隔离级别下解决幻读原理;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;事务指的是满足&lt;code&gt;ACID&lt;/code&gt;特性的一组操作,&lt;code&gt;mysql&lt;/code&gt;中可以通过&lt;code&gt;commit&lt;/code&gt;提交一个事务,也可以使用&lt;code&gt;rollback&lt;/code&gt;进行回滚。 在并发场景中很难保证事务的&lt;code&gt;Isolation&lt;/code&gt;特性, 即无法保证临界资源的排它性操作, 从而引发数据一致性问题, 临界资源互斥问题显然需要借助加锁来解决, 在并发事务中就需要用锁的并发控制来处理;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="mysql专题" scheme="http://researchlab.github.io/categories/mysql专题/"/>
    
    
    <category term="mysql" scheme="http://researchlab.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>redis-22-configuration</title>
    <link href="http://researchlab.github.io/2018/10/26/redis-22-configuration/"/>
    <id>http://researchlab.github.io/2018/10/26/redis-22-configuration/</id>
    <published>2018-10-26T06:06:46.000Z</published>
    <updated>2021-02-20T17:55:59.527Z</updated>
    
    <content type="html"><![CDATA[<p>为深入学习了解<code>redis</code>, 本文将对<code>redis5.0.0</code>源码包目录下的<code>redis.conf</code>配置文件进行完整翻译学习, 翻译顺序与配置文件保持一致;<br><a id="more"></a></p><p><code>Redis</code>配置文件示例</p><p>为程序能正确读取配置文件, 配置文件路径必须作为程序启动的第一个参数输入,  </p><blockquote><p>./redis-server /path/to/redis.conf </p></blockquote><p>默认的配置文件中, 首先约定了存储单位:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1k  =&gt; 1000 bytes</span><br><span class="line">1kb =&gt; 1024 bytes</span><br><span class="line">1m  =&gt; 1000000 bytes</span><br><span class="line">1mb =&gt; 1024*1024 bytes</span><br><span class="line">1g  =&gt; 1000000000 bytes</span><br><span class="line">1gb =&gt; 1024*1024*1024 bytes</span><br></pre></td></tr></table></figure></p><p>Redis配置中对单位的大小写不敏感, 1GB、1Gb和1gB都是相同的。由此也说明, Redis只支持bytes, 不支持bit单位。</p><h5 id="INCLUDES"><a href="#INCLUDES" class="headerlink" title="INCLUDES"></a>INCLUDES</h5><p>################################## INCLUDES ###################################</p><p>Redis支持以<code>include</code>的方式引入一个或多个其他配置文件, 比如:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">include /path/to/local.conf</span><br><span class="line">include /path/to/other.conf</span><br></pre></td></tr></table></figure></p><p>当存在一个标准配置模板可以应用到所有的<code>redis server</code>服务实例中, 但又需要自定义配置其中某些服务实例的配置项, 则很适合用<code>include</code>引入这些自定义配置文件, 这些自定义配置文件中也可以通过<code>include</code>引入其他配置文件;<br>需要注意的是,<code>include</code>引入的配置项并不会被<code>admin</code>通过<code>CONFIG REWRITE</code>命令或<code>Redis Sentinel</code>重写; 但如一个配置项在不同配置文件中都有定义, 则以最后一行读入的为准, 就是说后面的配置项会覆盖前面的配置项。<br>在标准配置模板开始位置引入<code>include</code>配置文件, 则引入的这些配置文件不会覆盖标准配置模板中的配置项;<br>在标准配置模板最后位置引入<code>include</code>配置文件, 则引入的这些配置文件会覆盖标准配置模板中的配置项;</p><h5 id="MODULES"><a href="#MODULES" class="headerlink" title="MODULES"></a>MODULES</h5><p>################################## MODULES #####################################</p><p>从<code>redis4.0</code>版本开始支持载入模块功能, 大大扩展了<code>redis</code>功能,  <code>redis4.0</code>以上版本可以通过<code>loadmodule</code>配置在服务启动时载入相关模块功能,若当前<code>redis</code>服务器版本不支持加载模块, 则不会响应<code>loadmodule</code>指令; 可以使用多个<code>loadmodule</code>指令;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loadmodule /path/to/my_module.so</span><br><span class="line">loadmodule /path/to/other_module.so</span><br></pre></td></tr></table></figure><h5 id="NETWORK"><a href="#NETWORK" class="headerlink" title="NETWORK"></a>NETWORK</h5><p>################################## NETWORK #####################################<br>默认情况下, 如果没有具体指定<code>bind</code>配置指令, Redis会响应本机所有可用网卡的连接请求。Redis允许通过<code>bind</code>配置项来指定要绑定的一个或多个IP,<br>示例,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bind 192.168.1.100 10.0.0.1</span><br><span class="line">bind 127.0.0.1 ::1</span><br></pre></td></tr></table></figure></p><p>注意, 将设置<code>bind</code>为响应所有网卡请求<code>redis</code>实例的机器直接暴露在公网上是很危险的行为, 这样做会将数据库实例暴露给公网上的每个用户; 所以默认配置为<code>bind 127.0.0.1</code>，这将强制<code>redis</code>服务实例只能响应与<code>redis</code>服务运行在同一台机器上的客户端请求; 如果你确认安全的情况下, 可以<code>bind</code>为响应所有网卡请求;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind 127.0.0.1</span><br></pre></td></tr></table></figure></p><p>保护模式是为避免<code>Redis</code>实例运行在公网上和使用的一层保护行为;</p><p>当开启保护模式后, 存在如下情况,</p><ol><li>服务器没有通过<code>bind</code>指令明确绑定到一组网络地址上;</li><li>没有配置访问密码;</li></ol><p>则<code>redis</code>服务器只接受来自IPv4和IPv6环回地址127.0.0.1和:: 1上客户端的连接请求以及Unix sockets请求;</p><p>默认情况下, 保护模式是开启的, 在你确认在上述两种情况下需要响应来自其它机器上<code>redis</code>连接请求时, 可以自行保护模式;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">protected-mode yes</span><br></pre></td></tr></table></figure></p><p><code>redis</code>默认监听6379服务端口(IANA #815344), 当端口被设置为0时, <code>redis</code>服务器则不再监听TCP套接字连接了;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">port 6379</span><br></pre></td></tr></table></figure><blockquote><p>可是, 如果Redis不监听端口, 还怎么与外界通信呢？其实Redis还支持通过unix socket方式来接收请求。可以通过unix socket配置项来指定unix socket文件的路径, 并通过unix socket perm来指定文件的权限。</p></blockquote><p>TCP listen() backlog值<br>在高QPS场景下需要提高backlog值来避免TCP的慢连接问题; 若backlog值大于系统<code>/proc/sys/net/core/somaxconn</code>最大值, Linux kernel默认会将backlog值截断保存到somaxconn中, 即backlog值受到somaxconn的最大值影响, 所以为了达到理想效果请注意同时将<code>somaxconn</code>和<code>tcp_max_syn_backlog</code>设置为一个比较大的合理值; <code>redis.conf</code>默认设置tcp-backlog为511,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp-backlog 511</span><br></pre></td></tr></table></figure></p><blockquote><p><code>tcp_max_syn_backlog</code> 为tcp三次握手中未完成连接队列的最大值限制, 默认值为128;<br><code>somaxconn</code> 定义了系统中每一个端口最大的监听队列的长度,这是个全局的参数, 默认值为128;<br>更多说明可参考 <a href="http://researchlab.github.io/2018/09/23/tcp-net-programing/">TCP网络编程过程完整分析</a></p></blockquote><p>Unix socket<br>没有设置默认Unix socket 套接字路径, 所以Redis默认不会监听来自Unix socket的请求, 只有具体配置指定了Unix socket套接字路径, 才监听这个套接字上的请求;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> unixsocket /tmp/redis.sock  指定 unix socket 的路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> unixsocketperm 700 指定 unix socket file 的权限</span></span><br></pre></td></tr></table></figure></p><p>当客户端闲置多少秒后关闭连接，如果设置为0表示关闭该功能, 默认为0;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeout 0</span><br></pre></td></tr></table></figure></p><p> TCP keepalive.<br>TCP连接保活策略, 可以通过tcp-keepalive配置项来进行设置, 单位为秒, 假如设置为60秒, 则server端会每60秒向连接空闲的客户端发起一次ACK请求, 以检查客户端是否已经挂掉, 对于无响应的客户端则会关闭其连接。所以关闭一个连接最长需要120秒的时间。如果设置为0, 则不会进行保活检测。<br>官方建议值为300s, 并从redis3.2.1开始默认设置tcp-keepalive值;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp-keepalive 300</span><br></pre></td></tr></table></figure></p><h5 id="GENERAL"><a href="#GENERAL" class="headerlink" title="GENERAL"></a>GENERAL</h5><p>################################# GENERAL #####################################<br>默认情况, redis实例不会以守护进程运行, 如果需要运行为守护进程， 则应配置其值为<code>yes</code>, 当以守护进程运行时，redis会将其pid默认写入到<code>/var/run/redis.pid</code>文件中<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">daemonize no</span><br></pre></td></tr></table></figure></p><p>可以通过upstart和systemd管理Redis守护进程, 具体响应如下,<br><code>supervised no</code>      - 不使用 supervision 管理<br><code>supervised upstart</code> - <code>upstart</code>信号以<code>SIGSTOP</code>模式启动<code>redis</code><br><code>supervised systemd</code> - <code>systemd</code> 将READY=1 写入 $NOTIFY_SOCKET<br><code>supervised auto</code>    - 通过检测<code>UPSTART_JOB</code>或者<code>NOTIFY_SOCKET</code>环境参数来决定使用<code>upstart</code>或者<code>systemd</code>方法启动;<br>注意上述只能管理守护进程存在,并不会检测redis连接是否依然有效;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">supervised no</span><br></pre></td></tr></table></figure></p><p>当redis以守护模式启动时, 如果没有配置pidfile, pidfile默认值是/var/run/redis.pid, redis会在启动时将pid值记录到指定的pidfile中, 并且在服务关闭时删除这个pid记录;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pidfile /var/run/redis_6379.pid</span><br></pre></td></tr></table></figure></p><p>Redis支持通过loglevel配置项设置日志等级, 共分四级, 即debug、verbose、notice、warning, 记录的日志信息依次递减,重要程度依次递增, 默认为notice<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loglevel notice</span><br></pre></td></tr></table></figure></p><p>Redis也支持通过logfile配置项来设置日志文件的生成位置。如果设置为空字符串, 则Redis会将日志输出到标准输出。假如在daemon情况下将日志设置为输出到标准输出, 则日志会被写到/dev/null中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logfile ""</span><br></pre></td></tr></table></figure><!--# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT <dbid> where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING  ################################## Save the DB on disk:##   save <seconds> <changes>##   Will save the DB if both the given number of seconds and the given#   number of write operations against the DB occurred.##   In the example below the behaviour will be to save:#   after 900 sec (15 min) if at least 1 key changed#   after 300 sec (5 min) if at least 10 keys changed#   after 60 sec if at least 10000 keys changed##   Note: you can disable saving completely by commenting out all "save" lines.##   It is also possible to remove all the previously configured save#   points by adding a save directive with a single empty string argument#   like in the following example:##   save ""save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.##   +------------------+      +---------------+#   |      Master      | ---> |    Replica    |<br>#   | (receive writes) |      |  (exact copy) |<br>#   +——————+      +—————+<br>#<br># 1) Redis replication is asynchronous, but you can configure a master to<br>#    stop accepting writes if it appears to be not connected with at least<br>#    a given number of replicas.<br># 2) Redis replicas are able to perform a partial resynchronization with the<br>#    master if the replication link is lost for a relatively small amount of<br>#    time. You may want to configure the replication backlog size (see the next<br>#    sections of this file) with a sensible value depending on your needs.<br># 3) Replication is automatic and does not need user intervention. After a<br>#    network partition replicas automatically try to reconnect to masters<br>#    and resynchronize with them.<br>#<br># replicaof <masterip> <masterport><br><br># If the master is password protected (using the “requirepass” configuration<br># directive below) it is possible to tell the replica to authenticate before<br># starting the replication synchronization process, otherwise the master will<br># refuse the replica request.<br>#<br># masterauth <master-password><br><br># When a replica loses its connection with the master, or when the replication<br># is still in progress, the replica can act in two different ways:<br>#<br># 1) if replica-serve-stale-data is set to ‘yes’ (the default) the replica will<br>#    still reply to client requests, possibly with out of date data, or the<br>#    data set may just be empty if this is the first synchronization.<br>#<br># 2) if replica-serve-stale-data is set to ‘no’ the replica will reply with<br>#    an error “SYNC with master in progress” to all the kind of commands<br>#    but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,<br>#    SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,<br>#    COMMAND, POST, HOST: and LATENCY.<br>#<br>replica-serve-stale-data yes<br><br># You can configure a replica instance to accept writes or not. Writing against<br># a replica instance may be useful to store some ephemeral data (because data<br># written on a replica will be easily deleted after resync with the master) but<br># may also cause problems if clients are writing to it because of a<br># misconfiguration.<br>#<br># Since Redis 2.6 by default replicas are read-only.<br>#<br># Note: read only replicas are not designed to be exposed to untrusted clients<br># on the internet. It’s just a protection layer against misuse of the instance.<br># Still a read only replica exports by default all the administrative commands<br># such as CONFIG, DEBUG, and so forth. To a limited extent you can improve<br># security of read only replicas using ‘rename-command’ to shadow all the<br># administrative / dangerous commands.<br>replica-read-only yes<br><br># Replication SYNC strategy: disk or socket.<br>#<br># ——————————————————-<br># WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY<br># ——————————————————-<br>#<br># New replicas and reconnecting replicas that are not able to continue the replication<br># process just receiving differences, need to do what is called a “full<br># synchronization”. An RDB file is transmitted from the master to the replicas.<br># The transmission can happen in two different ways:<br>#<br># 1) Disk-backed: The Redis master creates a new process that writes the RDB<br>#                 file on disk. Later the file is transferred by the parent<br>#                 process to the replicas incrementally.<br># 2) Diskless: The Redis master creates a new process that directly writes the<br>#              RDB file to replica sockets, without touching the disk at all.<br>#<br># With disk-backed replication, while the RDB file is generated, more replicas<br># can be queued and served with the RDB file as soon as the current child producing<br># the RDB file finishes its work. With diskless replication instead once<br># the transfer starts, new replicas arriving will be queued and a new transfer<br># will start when the current one terminates.<br>#<br># When diskless replication is used, the master waits a configurable amount of<br># time (in seconds) before starting the transfer in the hope that multiple replicas<br># will arrive and the transfer can be parallelized.<br>#<br># With slow disks and fast (large bandwidth) networks, diskless replication<br># works better.<br>repl-diskless-sync no<br><br># When diskless replication is enabled, it is possible to configure the delay<br># the server waits in order to spawn the child that transfers the RDB via socket<br># to the replicas.<br>#<br># This is important since once the transfer starts, it is not possible to serve<br># new replicas arriving, that will be queued for the next RDB transfer, so the server<br># waits a delay in order to let more replicas arrive.<br>#<br># The delay is specified in seconds, and by default is 5 seconds. To disable<br># it entirely just set it to 0 seconds and the transfer will start ASAP.<br>repl-diskless-sync-delay 5<br><br># Replicas send PINGs to server in a predefined interval. It’s possible to change<br># this interval with the repl_ping_replica_period option. The default value is 10<br># seconds.<br>#<br># repl-ping-replica-period 10<br><br># The following option sets the replication timeout for:<br>#<br># 1) Bulk transfer I/O during SYNC, from the point of view of replica.<br># 2) Master timeout from the point of view of replicas (data, pings).<br># 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).<br>#<br># It is important to make sure that this value is greater than the value<br># specified for repl-ping-replica-period otherwise a timeout will be detected<br># every time there is low traffic between the master and the replica.<br>#<br># repl-timeout 60<br><br># Disable TCP_NODELAY on the replica socket after SYNC?<br>#<br># If you select “yes” Redis will use a smaller number of TCP packets and<br># less bandwidth to send data to replicas. But this can add a delay for<br># the data to appear on the replica side, up to 40 milliseconds with<br># Linux kernels using a default configuration.<br>#<br># If you select “no” the delay for data to appear on the replica side will<br># be reduced but more bandwidth will be used for replication.<br>#<br># By default we optimize for low latency, but in very high traffic conditions<br># or when the master and replicas are many hops away, turning this to “yes” may<br># be a good idea.<br>repl-disable-tcp-nodelay no<br><br># Set the replication backlog size. The backlog is a buffer that accumulates<br># replica data when replicas are disconnected for some time, so that when a replica<br># wants to reconnect again, often a full resync is not needed, but a partial<br># resync is enough, just passing the portion of data the replica missed while<br># disconnected.<br>#<br># The bigger the replication backlog, the longer the time the replica can be<br># disconnected and later be able to perform a partial resynchronization.<br>#<br># The backlog is only allocated once there is at least a replica connected.<br>#<br># repl-backlog-size 1mb<br><br># After a master has no longer connected replicas for some time, the backlog<br># will be freed. The following option configures the amount of seconds that<br># need to elapse, starting from the time the last replica disconnected, for<br># the backlog buffer to be freed.<br>#<br># Note that replicas never free the backlog for timeout, since they may be<br># promoted to masters later, and should be able to correctly “partially<br># resynchronize” with the replicas: hence they should always accumulate backlog.<br>#<br># A value of 0 means to never release the backlog.<br>#<br># repl-backlog-ttl 3600<br><br># The replica priority is an integer number published by Redis in the INFO output.<br># It is used by Redis Sentinel in order to select a replica to promote into a<br># master if the master is no longer working correctly.<br>#<br># A replica with a low priority number is considered better for promotion, so<br># for instance if there are three replicas with priority 10, 100, 25 Sentinel will<br># pick the one with priority 10, that is the lowest.<br>#<br># However a special priority of 0 marks the replica as not able to perform the<br># role of master, so a replica with priority of 0 will never be selected by<br># Redis Sentinel for promotion.<br>#<br># By default the priority is 100.<br>replica-priority 100<br><br># It is possible for a master to stop accepting writes if there are less than<br># N replicas connected, having a lag less or equal than M seconds.<br>#<br># The N replicas need to be in “online” state.<br>#<br># The lag in seconds, that must be &lt;= the specified value, is calculated from<br># the last ping received from the replica, that is usually sent every second.<br>#<br># This option does not GUARANTEE that N replicas will accept the write, but<br># will limit the window of exposure for lost writes in case not enough replicas<br># are available, to the specified number of seconds.<br>#<br># For example to require at least 3 replicas with a lag &lt;= 10 seconds use:<br>#<br># min-replicas-to-write 3<br># min-replicas-max-lag 10<br>#<br># Setting one or the other to 0 disables the feature.<br>#<br># By default min-replicas-to-write is set to 0 (feature disabled) and<br># min-replicas-max-lag is set to 10.<br><br># A Redis master is able to list the address and port of the attached<br># replicas in different ways. For example the “INFO replication” section<br># offers this information, which is used, among other tools, by<br># Redis Sentinel in order to discover replica instances.<br># Another place where this info is available is in the output of the<br># “ROLE” command of a master.<br>#<br># The listed IP and address normally reported by a replica is obtained<br># in the following way:<br>#<br>#   IP: The address is auto detected by checking the peer address<br>#   of the socket used by the replica to connect with the master.<br>#<br>#   Port: The port is communicated by the replica during the replication<br>#   handshake, and is normally the port that the replica is using to<br>#   listen for connections.<br>#<br># However when port forwarding or Network Address Translation (NAT) is<br># used, the replica may be actually reachable via different IP and port<br># pairs. The following two options can be used by a replica in order to<br># report to its master a specific set of IP and port, so that both INFO<br># and ROLE will report those values.<br>#<br># There is no need to use both the options if you need to override just<br># the port or the IP address.<br>#<br># replica-announce-ip 5.5.5.5<br># replica-announce-port 1234<br><br>################################## SECURITY ###################################<br><br># Require clients to issue AUTH <password> before processing any other<br># commands.  This might be useful in environments in which you do not trust<br># others with access to the host running redis-server.<br>#<br># This should stay commented out for backward compatibility and because most<br># people do not need auth (e.g. they run their own servers).<br>#<br># Warning: since Redis is pretty fast an outside user can try up to<br># 150k passwords per second against a good box. This means that you should<br># use a very strong password otherwise it will be very easy to break.<br>#<br># requirepass foobared<br><br># Command renaming.<br>#<br># It is possible to change the name of dangerous commands in a shared<br># environment. For instance the CONFIG command may be renamed into something<br># hard to guess so that it will still be available for internal-use tools<br># but not available for general clients.<br>#<br># Example:<br>#<br># rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52<br>#<br># It is also possible to completely kill a command by renaming it into<br># an empty string:<br>#<br># rename-command CONFIG “”<br>#<br># Please note that changing the name of commands that are logged into the<br># AOF file or transmitted to replicas may cause problems.<br><br>################################### CLIENTS ####################################<br><br># Set the max number of connected clients at the same time. By default<br># this limit is set to 10000 clients, however if the Redis server is not<br># able to configure the process file limit to allow for the specified limit<br># the max number of allowed clients is set to the current file limit<br># minus 32 (as Redis reserves a few file descriptors for internal uses).<br>#<br># Once the limit is reached Redis will close all the new connections sending<br># an error ‘max number of clients reached’.<br>#<br># maxclients 10000<br><br>############################## MEMORY MANAGEMENT ################################<br><br># Set a memory usage limit to the specified amount of bytes.<br># When the memory limit is reached Redis will try to remove keys<br># according to the eviction policy selected (see maxmemory-policy).<br>#<br># If Redis can’t remove keys according to the policy, or if the policy is<br># set to ‘noeviction’, Redis will start to reply with errors to commands<br># that would use more memory, like SET, LPUSH, and so on, and will continue<br># to reply to read-only commands like GET.<br>#<br># This option is usually useful when using Redis as an LRU or LFU cache, or to<br># set a hard memory limit for an instance (using the ‘noeviction’ policy).<br>#<br># WARNING: If you have replicas attached to an instance with maxmemory on,<br># the size of the output buffers needed to feed the replicas are subtracted<br># from the used memory count, so that network problems / resyncs will<br># not trigger a loop where keys are evicted, and in turn the output<br># buffer of replicas is full with DELs of keys evicted triggering the deletion<br># of more keys, and so forth until the database is completely emptied.<br>#<br># In short… if you have replicas attached it is suggested that you set a lower<br># limit for maxmemory so that there is some free RAM on the system for replica<br># output buffers (but this is not needed if the policy is ‘noeviction’).<br>#<br># maxmemory <bytes><br><br># MAXMEMORY POLICY: how Redis will select what to remove when maxmemory<br># is reached. You can select among five behaviors:<br>#<br># volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.<br># allkeys-lru -&gt; Evict any key using approximated LRU.<br># volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.<br># allkeys-lfu -&gt; Evict any key using approximated LFU.<br># volatile-random -&gt; Remove a random key among the ones with an expire set.<br># allkeys-random -&gt; Remove a random key, any key.<br># volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)<br># noeviction -&gt; Don’t evict anything, just return an error on write operations.<br>#<br># LRU means Least Recently Used<br># LFU means Least Frequently Used<br>#<br># Both LRU, LFU and volatile-ttl are implemented using approximated<br># randomized algorithms.<br>#<br># Note: with any of the above policies, Redis will return an error on write<br>#       operations, when there are no suitable keys for eviction.<br>#<br>#       At the date of writing these commands are: set setnx setex append<br>#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd<br>#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby<br>#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby<br>#       getset mset msetnx exec sort<br>#<br># The default is:<br>#<br># maxmemory-policy noeviction<br><br># LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated<br># algorithms (in order to save memory), so you can tune it for speed or<br># accuracy. For default Redis will check five keys and pick the one that was<br># used less recently, you can change the sample size using the following<br># configuration directive.<br>#<br># The default of 5 produces good enough results. 10 Approximates very closely<br># true LRU but costs more CPU. 3 is faster but not very accurate.<br>#<br># maxmemory-samples 5<br><br># Starting from Redis 5, by default a replica will ignore its maxmemory setting<br># (unless it is promoted to master after a failover or manually). It means<br># that the eviction of keys will be just handled by the master, sending the<br># DEL commands to the replica as keys evict in the master side.<br>#<br># This behavior ensures that masters and replicas stay consistent, and is usually<br># what you want, however if your replica is writable, or you want the replica to have<br># a different memory setting, and you are sure all the writes performed to the<br># replica are idempotent, then you may change this default (but be sure to understand<br># what you are doing).<br>#<br># Note that since the replica by default does not evict, it may end using more<br># memory than the one set via maxmemory (there are certain buffers that may<br># be larger on the replica, or data structures may sometimes take more memory and so<br># forth). So make sure you monitor your replicas and make sure they have enough<br># memory to never hit a real out-of-memory condition before the master hits<br># the configured maxmemory setting.<br>#<br># replica-ignore-maxmemory yes<br><br>############################# LAZY FREEING ####################################<br><br># Redis has two primitives to delete keys. One is called DEL and is a blocking<br># deletion of the object. It means that the server stops processing new commands<br># in order to reclaim all the memory associated with an object in a synchronous<br># way. If the key deleted is associated with a small object, the time needed<br># in order to execute the DEL command is very small and comparable to most other<br># O(1) or O(log_N) commands in Redis. However if the key is associated with an<br># aggregated value containing millions of elements, the server can block for<br># a long time (even seconds) in order to complete the operation.<br>#<br># For the above reasons Redis also offers non blocking deletion primitives<br># such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and<br># FLUSHDB commands, in order to reclaim memory in background. Those commands<br># are executed in constant time. Another thread will incrementally free the<br># object in the background as fast as possible.<br>#<br># DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.<br># It’s up to the design of the application to understand when it is a good<br># idea to use one or the other. However the Redis server sometimes has to<br># delete keys or flush the whole database as a side effect of other operations.<br># Specifically Redis deletes objects independently of a user call in the<br># following scenarios:<br>#<br># 1) On eviction, because of the maxmemory and maxmemory policy configurations,<br>#    in order to make room for new data, without going over the specified<br>#    memory limit.<br># 2) Because of expire: when a key with an associated time to live (see the<br>#    EXPIRE command) must be deleted from memory.<br># 3) Because of a side effect of a command that stores data on a key that may<br>#    already exist. For example the RENAME command may delete the old key<br>#    content when it is replaced with another one. Similarly SUNIONSTORE<br>#    or SORT with STORE option may delete existing keys. The SET command<br>#    itself removes any old content of the specified key in order to replace<br>#    it with the specified string.<br># 4) During replication, when a replica performs a full resynchronization with<br>#    its master, the content of the whole database is removed in order to<br>#    load the RDB file just transferred.<br>#<br># In all the above cases the default is to delete objects in a blocking way,<br># like if DEL was called. However you can configure each case specifically<br># in order to instead release memory in a non-blocking way like if UNLINK<br># was called, using the following configuration directives:<br><br>lazyfree-lazy-eviction no<br>lazyfree-lazy-expire no<br>lazyfree-lazy-server-del no<br>replica-lazy-flush no<br><br>############################## APPEND ONLY MODE ###############################<br><br># By default Redis asynchronously dumps the dataset on disk. This mode is<br># good enough in many applications, but an issue with the Redis process or<br># a power outage may result into a few minutes of writes lost (depending on<br># the configured save points).<br>#<br># The Append Only File is an alternative persistence mode that provides<br># much better durability. For instance using the default data fsync policy<br># (see later in the config file) Redis can lose just one second of writes in a<br># dramatic event like a server power outage, or a single write if something<br># wrong with the Redis process itself happens, but the operating system is<br># still running correctly.<br>#<br># AOF and RDB persistence can be enabled at the same time without problems.<br># If the AOF is enabled on startup Redis will load the AOF, that is the file<br># with the better durability guarantees.<br>#<br># Please check <a href="http://redis.io/topics/persistence" target="_blank" rel="noopener">http://redis.io/topics/persistence</a> for more information.<br><br>appendonly no<br><br># The name of the append only file (default: “appendonly.aof”)<br><br>appendfilename “appendonly.aof”<br><br># The fsync() call tells the Operating System to actually write data on disk<br># instead of waiting for more data in the output buffer. Some OS will really flush<br># data on disk, some other OS will just try to do it ASAP.<br>#<br># Redis supports three different modes:<br>#<br># no: don’t fsync, just let the OS flush the data when it wants. Faster.<br># always: fsync after every write to the append only log. Slow, Safest.<br># everysec: fsync only one time every second. Compromise.<br>#<br># The default is “everysec”, as that’s usually the right compromise between<br># speed and data safety. It’s up to you to understand if you can relax this to<br># “no” that will let the operating system flush the output buffer when<br># it wants, for better performances (but if you can live with the idea of<br># some data loss consider the default persistence mode that’s snapshotting),<br># or on the contrary, use “always” that’s very slow but a bit safer than<br># everysec.<br>#<br># More details please check the following article:<br># <a href="http://antirez.com/post/redis-persistence-demystified.html" target="_blank" rel="noopener">http://antirez.com/post/redis-persistence-demystified.html</a><br>#<br># If unsure, use “everysec”.<br><br># appendfsync always<br>appendfsync everysec<br># appendfsync no<br><br># When the AOF fsync policy is set to always or everysec, and a background<br># saving process (a background save or AOF log background rewriting) is<br># performing a lot of I/O against the disk, in some Linux configurations<br># Redis may block too long on the fsync() call. Note that there is no fix for<br># this currently, as even performing fsync in a different thread will block<br># our synchronous write(2) call.<br>#<br># In order to mitigate this problem it’s possible to use the following option<br># that will prevent fsync() from being called in the main process while a<br># BGSAVE or BGREWRITEAOF is in progress.<br>#<br># This means that while another child is saving, the durability of Redis is<br># the same as “appendfsync none”. In practical terms, this means that it is<br># possible to lose up to 30 seconds of log in the worst scenario (with the<br># default Linux settings).<br>#<br># If you have latency problems turn this to “yes”. Otherwise leave it as<br># “no” that is the safest pick from the point of view of durability.<br><br>no-appendfsync-on-rewrite no<br><br># Automatic rewrite of the append only file.<br># Redis is able to automatically rewrite the log file implicitly calling<br># BGREWRITEAOF when the AOF log size grows by the specified percentage.<br>#<br># This is how it works: Redis remembers the size of the AOF file after the<br># latest rewrite (if no rewrite has happened since the restart, the size of<br># the AOF at startup is used).<br>#<br># This base size is compared to the current size. If the current size is<br># bigger than the specified percentage, the rewrite is triggered. Also<br># you need to specify a minimal size for the AOF file to be rewritten, this<br># is useful to avoid rewriting the AOF file even if the percentage increase<br># is reached but it is still pretty small.<br>#<br># Specify a percentage of zero in order to disable the automatic AOF<br># rewrite feature.<br><br>auto-aof-rewrite-percentage 100<br>auto-aof-rewrite-min-size 64mb<br><br># An AOF file may be found to be truncated at the end during the Redis<br># startup process, when the AOF data gets loaded back into memory.<br># This may happen when the system where Redis is running<br># crashes, especially when an ext4 filesystem is mounted without the<br># data=ordered option (however this can’t happen when Redis itself<br># crashes or aborts but the operating system still works correctly).<br>#<br># Redis can either exit with an error when this happens, or load as much<br># data as possible (the default now) and start if the AOF file is found<br># to be truncated at the end. The following option controls this behavior.<br>#<br># If aof-load-truncated is set to yes, a truncated AOF file is loaded and<br># the Redis server starts emitting a log to inform the user of the event.<br># Otherwise if the option is set to no, the server aborts with an error<br># and refuses to start. When the option is set to no, the user requires<br># to fix the AOF file using the “redis-check-aof” utility before to restart<br># the server.<br>#<br># Note that if the AOF file will be found to be corrupted in the middle<br># the server will still exit with an error. This option only applies when<br># Redis will try to read more data from the AOF file but not enough bytes<br># will be found.<br>aof-load-truncated yes<br><br># When rewriting the AOF file, Redis is able to use an RDB preamble in the<br># AOF file for faster rewrites and recoveries. When this option is turned<br># on the rewritten AOF file is composed of two different stanzas:<br>#<br>#   [RDB file][AOF tail]<br>#<br># When loading Redis recognizes that the AOF file starts with the “REDIS”<br># string and loads the prefixed RDB file, and continues loading the AOF<br># tail.<br>aof-use-rdb-preamble yes<br><br>################################ LUA SCRIPTING  ###############################<br><br># Max execution time of a Lua script in milliseconds.<br>#<br># If the maximum execution time is reached Redis will log that a script is<br># still in execution after the maximum allowed time and will start to<br># reply to queries with an error.<br>#<br># When a long running script exceeds the maximum execution time only the<br># SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be<br># used to stop a script that did not yet called write commands. The second<br># is the only way to shut down the server in the case a write command was<br># already issued by the script but the user doesn’t want to wait for the natural<br># termination of the script.<br>#<br># Set it to 0 or a negative value for unlimited execution without warnings.<br>lua-time-limit 5000<br><br>################################ REDIS CLUSTER  ###############################<br>#<br># ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br># WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however<br># in order to mark it as “mature” we need to wait for a non trivial percentage<br># of users to deploy it in production.<br># ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>#<br># Normal Redis instances can’t be part of a Redis Cluster; only nodes that are<br># started as cluster nodes can. In order to start a Redis instance as a<br># cluster node enable the cluster support uncommenting the following:<br>#<br># cluster-enabled yes<br><br># Every cluster node has a cluster configuration file. This file is not<br># intended to be edited by hand. It is created and updated by Redis nodes.<br># Every Redis Cluster node requires a different cluster configuration file.<br># Make sure that instances running in the same system do not have<br># overlapping cluster configuration file names.<br>#<br># cluster-config-file nodes-6379.conf<br><br># Cluster node timeout is the amount of milliseconds a node must be unreachable<br># for it to be considered in failure state.<br># Most other internal time limits are multiple of the node timeout.<br>#<br># cluster-node-timeout 15000<br><br># A replica of a failing master will avoid to start a failover if its data<br># looks too old.<br>#<br># There is no simple way for a replica to actually have an exact measure of<br># its “data age”, so the following two checks are performed:<br>#<br># 1) If there are multiple replicas able to failover, they exchange messages<br>#    in order to try to give an advantage to the replica with the best<br>#    replication offset (more data from the master processed).<br>#    Replicas will try to get their rank by offset, and apply to the start<br>#    of the failover a delay proportional to their rank.<br>#<br># 2) Every single replica computes the time of the last interaction with<br>#    its master. This can be the last ping or command received (if the master<br>#    is still in the “connected” state), or the time that elapsed since the<br>#    disconnection with the master (if the replication link is currently down).<br>#    If the last interaction is too old, the replica will not try to failover<br>#    at all.<br>#<br># The point “2” can be tuned by user. Specifically a replica will not perform<br># the failover if, since the last interaction with the master, the time<br># elapsed is greater than:<br>#<br>#   (node-timeout <em> replica-validity-factor) + repl-ping-replica-period<br>#<br># So for example if node-timeout is 30 seconds, and the replica-validity-factor<br># is 10, and assuming a default repl-ping-replica-period of 10 seconds, the<br># replica will not try to failover if it was not able to talk with the master<br># for longer than 310 seconds.<br>#<br># A large replica-validity-factor may allow replicas with too old data to failover<br># a master, while a too small value may prevent the cluster from being able to<br># elect a replica at all.<br>#<br># For maximum availability, it is possible to set the replica-validity-factor<br># to a value of 0, which means, that replicas will always try to failover the<br># master regardless of the last time they interacted with the master.<br># (However they’ll always try to apply a delay proportional to their<br># offset rank).<br>#<br># Zero is the only value able to guarantee that when all the partitions heal<br># the cluster will always be able to continue.<br>#<br># cluster-replica-validity-factor 10<br><br># Cluster replicas are able to migrate to orphaned masters, that are masters<br># that are left without working replicas. This improves the cluster ability<br># to resist to failures as otherwise an orphaned master can’t be failed over<br># in case of failure if it has no working replicas.<br>#<br># Replicas migrate to orphaned masters only if there are still at least a<br># given number of other working replicas for their old master. This number<br># is the “migration barrier”. A migration barrier of 1 means that a replica<br># will migrate only if there is at least 1 other working replica for its master<br># and so forth. It usually reflects the number of replicas you want for every<br># master in your cluster.<br>#<br># Default is 1 (replicas migrate only if their masters remain with at least<br># one replica). To disable migration just set it to a very large value.<br># A value of 0 can be set but is useful only for debugging and dangerous<br># in production.<br>#<br># cluster-migration-barrier 1<br><br># By default Redis Cluster nodes stop accepting queries if they detect there<br># is at least an hash slot uncovered (no available node is serving it).<br># This way if the cluster is partially down (for example a range of hash slots<br># are no longer covered) all the cluster becomes, eventually, unavailable.<br># It automatically returns available as soon as all the slots are covered again.<br>#<br># However sometimes you want the subset of the cluster which is working,<br># to continue to accept queries for the part of the key space that is still<br># covered. In order to do so, just set the cluster-require-full-coverage<br># option to no.<br>#<br># cluster-require-full-coverage yes<br><br># This option, when set to yes, prevents replicas from trying to failover its<br># master during master failures. However the master can still perform a<br># manual failover, if forced to do so.<br>#<br># This is useful in different scenarios, especially in the case of multiple<br># data center operations, where we want one side to never be promoted if not<br># in the case of a total DC failure.<br>#<br># cluster-replica-no-failover no<br><br># In order to setup your cluster make sure to read the documentation<br># available at <a href="http://redis.io" target="_blank" rel="noopener">http://redis.io</a> web site.<br><br>########################## CLUSTER DOCKER/NAT support  ########################<br><br># In certain deployments, Redis Cluster nodes address discovery fails, because<br># addresses are NAT-ted or because ports are forwarded (the typical case is<br># Docker and other containers).<br>#<br># In order to make Redis Cluster working in such environments, a static<br># configuration where each node knows its public address is needed. The<br># following two options are used for this scope, and are:<br>#<br># </em> cluster-announce-ip<br># <em> cluster-announce-port<br># </em> cluster-announce-bus-port<br>#<br># Each instruct the node about its address, client port, and cluster message<br># bus port. The information is then published in the header of the bus packets<br># so that other nodes will be able to correctly map the address of the node<br># publishing the information.<br>#<br># If the above options are not used, the normal Redis Cluster auto-detection<br># will be used instead.<br>#<br># Note that when remapped, the bus port may not be at the fixed offset of<br># clients port + 10000, so you can specify any port and bus-port depending<br># on how they get remapped. If the bus-port is not set, a fixed offset of<br># 10000 will be used as usually.<br>#<br># Example:<br>#<br># cluster-announce-ip 10.1.1.5<br># cluster-announce-port 6379<br># cluster-announce-bus-port 6380<br><br>################################## SLOW LOG ###################################<br><br># The Redis Slow Log is a system to log queries that exceeded a specified<br># execution time. The execution time does not include the I/O operations<br># like talking with the client, sending the reply and so forth,<br># but just the time needed to actually execute the command (this is the only<br># stage of command execution where the thread is blocked and can not serve<br># other requests in the meantime).<br>#<br># You can configure the slow log with two parameters: one tells Redis<br># what is the execution time, in microseconds, to exceed in order for the<br># command to get logged, and the other parameter is the length of the<br># slow log. When a new command is logged the oldest one is removed from the<br># queue of logged commands.<br><br># The following time is expressed in microseconds, so 1000000 is equivalent<br># to one second. Note that a negative number disables the slow log, while<br># a value of zero forces the logging of every command.<br>slowlog-log-slower-than 10000<br><br># There is no limit to this length. Just be aware that it will consume memory.<br># You can reclaim memory used by the slow log with SLOWLOG RESET.<br>slowlog-max-len 128<br><br>################################ LATENCY MONITOR ##############################<br><br># The Redis latency monitoring subsystem samples different operations<br># at runtime in order to collect data related to possible sources of<br># latency of a Redis instance.<br>#<br># Via the LATENCY command this information is available to the user that can<br># print graphs and obtain reports.<br>#<br># The system only logs operations that were performed in a time equal or<br># greater than the amount of milliseconds specified via the<br># latency-monitor-threshold configuration directive. When its value is set<br># to zero, the latency monitor is turned off.<br>#<br># By default latency monitoring is disabled since it is mostly not needed<br># if you don’t have latency issues, and collecting data has a performance<br># impact, that while very small, can be measured under big load. Latency<br># monitoring can easily be enabled at runtime using the command<br># “CONFIG SET latency-monitor-threshold <milliseconds>“ if needed.<br>latency-monitor-threshold 0<br><br>############################# EVENT NOTIFICATION ##############################<br><br># Redis can notify Pub/Sub clients about events happening in the key space.<br># This feature is documented at <a href="http://redis.io/topics/notifications" target="_blank" rel="noopener">http://redis.io/topics/notifications</a><br>#<br># For instance if keyspace events notification is enabled, and a client<br># performs a DEL operation on key “foo” stored in the Database 0, two<br># messages will be published via Pub/Sub:<br>#<br># PUBLISH <strong>keyspace@0</strong>:foo del<br># PUBLISH <strong>keyevent@0</strong>:del foo<br>#<br># It is possible to select the events that Redis will notify among a set<br># of classes. Every class is identified by a single character:<br>#<br>#  K     Keyspace events, published with <strong>keyspace@<db></db></strong> prefix.<br>#  E     Keyevent events, published with <strong>keyevent@<db></db></strong> prefix.<br>#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, …<br>#  $     String commands<br>#  l     List commands<br>#  s     Set commands<br>#  h     Hash commands<br>#  z     Sorted set commands<br>#  x     Expired events (events generated every time a key expires)<br>#  e     Evicted events (events generated when a key is evicted for maxmemory)<br>#  A     Alias for g$lshzxe, so that the “AKE” string means all the events.<br>#<br>#  The “notify-keyspace-events” takes as argument a string that is composed<br>#  of zero or multiple characters. The empty string means that notifications<br>#  are disabled.<br>#<br>#  Example: to enable list and generic events, from the point of view of the<br>#           event name, use:<br>#<br>#  notify-keyspace-events Elg<br>#<br>#  Example 2: to get the stream of the expired keys subscribing to channel<br>#             name <strong>keyevent@0</strong>:expired use:<br>#<br>#  notify-keyspace-events Ex<br>#<br>#  By default all notifications are disabled because most users don’t need<br>#  this feature and the feature has some overhead. Note that if you don’t<br>#  specify at least one of K or E, no events will be delivered.<br>notify-keyspace-events “”<br><br>############################### ADVANCED CONFIG ###############################<br><br># Hashes are encoded using a memory efficient data structure when they have a<br># small number of entries, and the biggest entry does not exceed a given<br># threshold. These thresholds can be configured using the following directives.<br>hash-max-ziplist-entries 512<br>hash-max-ziplist-value 64<br><br># Lists are also encoded in a special way to save a lot of space.<br># The number of entries allowed per internal list node can be specified<br># as a fixed maximum size or a maximum number of elements.<br># For a fixed maximum size, use -5 through -1, meaning:<br># -5: max size: 64 Kb  &lt;– not recommended for normal workloads<br># -4: max size: 32 Kb  &lt;– not recommended<br># -3: max size: 16 Kb  &lt;– probably not recommended<br># -2: max size: 8 Kb   &lt;– good<br># -1: max size: 4 Kb   &lt;– good<br># Positive numbers mean store up to <em>exactly</em> that number of elements<br># per list node.<br># The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),<br># but if your use case is unique, adjust the settings as necessary.<br>list-max-ziplist-size -2<br><br># Lists may also be compressed.<br># Compress depth is the number of quicklist ziplist nodes from <em>each</em> side of<br># the list to <em>exclude</em> from compression.  The head and tail of the list<br># are always uncompressed for fast push/pop operations.  Settings are:<br># 0: disable all list compression<br># 1: depth 1 means “don’t start compressing until after 1 node into the list,<br>#    going from either the head or tail”<br>#    So: [head]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[tail]<br>#    [head], [tail] will always be uncompressed; inner nodes will compress.<br># 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[prev]-&gt;[tail]<br>#    2 here means: don’t compress head or head-&gt;next or tail-&gt;prev or tail,<br>#    but compress all nodes between them.<br># 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;…-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]<br># etc.<br>list-compress-depth 0<br><br># Sets have a special encoding in just one case: when a set is composed<br># of just strings that happen to be integers in radix 10 in the range<br># of 64 bit signed integers.<br># The following configuration setting sets the limit in the size of the<br># set in order to use this special memory saving encoding.<br>set-max-intset-entries 512<br><br># Similarly to hashes and lists, sorted sets are also specially encoded in<br># order to save a lot of space. This encoding is only used when the length and<br># elements of a sorted set are below the following limits:<br>zset-max-ziplist-entries 128<br>zset-max-ziplist-value 64<br><br># HyperLogLog sparse representation bytes limit. The limit includes the<br># 16 bytes header. When an HyperLogLog using the sparse representation crosses<br># this limit, it is converted into the dense representation.<br>#<br># A value greater than 16000 is totally useless, since at that point the<br># dense representation is more memory efficient.<br>#<br># The suggested value is ~ 3000 in order to have the benefits of<br># the space efficient encoding without slowing down too much PFADD,<br># which is O(N) with the sparse encoding. The value can be raised to<br># ~ 10000 when CPU is not a concern, but space is, and the data set is<br># composed of many HyperLogLogs with cardinality in the 0 - 15000 range.<br>hll-sparse-max-bytes 3000<br><br># Streams macro node max size / items. The stream data structure is a radix<br># tree of big nodes that encode multiple items inside. Using this configuration<br># it is possible to configure how big a single node can be in bytes, and the<br># maximum number of items it may contain before switching to a new node when<br># appending new stream entries. If any of the following settings are set to<br># zero, the limit is ignored, so for instance it is possible to set just a<br># max entires limit by setting max-bytes to 0 and max-entries to the desired<br># value.<br>stream-node-max-bytes 4096<br>stream-node-max-entries 100<br><br># Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in<br># order to help rehashing the main Redis hash table (the one mapping top-level<br># keys to values). The hash table implementation Redis uses (see dict.c)<br># performs a lazy rehashing: the more operation you run into a hash table<br># that is rehashing, the more rehashing “steps” are performed, so if the<br># server is idle the rehashing is never complete and some more memory is used<br># by the hash table.<br>#<br># The default is to use this millisecond 10 times every second in order to<br># actively rehash the main dictionaries, freeing memory when possible.<br>#<br># If unsure:<br># use “activerehashing no” if you have hard latency requirements and it is<br># not a good thing in your environment that Redis can reply from time to time<br># to queries with 2 milliseconds delay.<br>#<br># use “activerehashing yes” if you don’t have such hard requirements but<br># want to free memory asap when possible.<br>activerehashing yes<br><br># The client output buffer limits can be used to force disconnection of clients<br># that are not reading data from the server fast enough for some reason (a<br># common reason is that a Pub/Sub client can’t consume messages as fast as the<br># publisher can produce them).<br>#<br># The limit can be set differently for the three different classes of clients:<br>#<br># normal -&gt; normal clients including MONITOR clients<br># replica  -&gt; replica clients<br># pubsub -&gt; clients subscribed to at least one pubsub channel or pattern<br>#<br># The syntax of every client-output-buffer-limit directive is the following:<br>#<br># client-output-buffer-limit <class> <hard limit=""> <soft limit=""> <soft seconds=""><br>#<br># A client is immediately disconnected once the hard limit is reached, or if<br># the soft limit is reached and remains reached for the specified number of<br># seconds (continuously).<br># So for instance if the hard limit is 32 megabytes and the soft limit is<br># 16 megabytes / 10 seconds, the client will get disconnected immediately<br># if the size of the output buffers reach 32 megabytes, but will also get<br># disconnected if the client reaches 16 megabytes and continuously overcomes<br># the limit for 10 seconds.<br>#<br># By default normal clients are not limited because they don’t receive data<br># without asking (in a push way), but just after a request, so only<br># asynchronous clients may create a scenario where data is requested faster<br># than it can read.<br>#<br># Instead there is a default limit for pubsub and replica clients, since<br># subscribers and replicas receive data in a push fashion.<br>#<br># Both the hard or the soft limit can be disabled by setting them to zero.<br>client-output-buffer-limit normal 0 0 0<br>client-output-buffer-limit replica 256mb 64mb 60<br>client-output-buffer-limit pubsub 32mb 8mb 60<br><br># Client query buffers accumulate new commands. They are limited to a fixed<br># amount by default in order to avoid that a protocol desynchronization (for<br># instance due to a bug in the client) will lead to unbound memory usage in<br># the query buffer. However you can configure it here if you have very special<br># needs, such us huge multi/exec requests or alike.<br>#<br># client-query-buffer-limit 1gb<br><br># In the Redis protocol, bulk requests, that are, elements representing single<br># strings, are normally limited ot 512 mb. However you can change this limit<br># here.<br>#<br># proto-max-bulk-len 512mb<br><br># Redis calls an internal function to perform many background tasks, like<br># closing connections of clients in timeout, purging expired keys that are<br># never requested, and so forth.<br>#<br># Not all tasks are performed with the same frequency, but Redis checks for<br># tasks to perform according to the specified “hz” value.<br>#<br># By default “hz” is set to 10. Raising the value will use more CPU when<br># Redis is idle, but at the same time will make Redis more responsive when<br># there are many keys expiring at the same time, and timeouts may be<br># handled with more precision.<br>#<br># The range is between 1 and 500, however a value over 100 is usually not<br># a good idea. Most users should use the default of 10 and raise this up to<br># 100 only in environments where very low latency is required.<br>hz 10<br><br># Normally it is useful to have an HZ value which is proportional to the<br># number of clients connected. This is useful in order, for instance, to<br># avoid too many clients are processed for each background task invocation<br># in order to avoid latency spikes.<br>#<br># Since the default HZ value by default is conservatively set to 10, Redis<br># offers, and enables by default, the ability to use an adaptive HZ value<br># which will temporary raise when there are many connected clients.<br>#<br># When dynamic HZ is enabled, the actual configured HZ will be used as<br># as a baseline, but multiples of the configured HZ value will be actually<br># used as needed once more clients are connected. In this way an idle<br># instance will use very little CPU time while a busy instance will be<br># more responsive.<br>dynamic-hz yes<br><br># When a child rewrites the AOF file, if the following option is enabled<br># the file will be fsync-ed every 32 MB of data generated. This is useful<br># in order to commit the file to the disk more incrementally and avoid<br># big latency spikes.<br>aof-rewrite-incremental-fsync yes<br><br># When redis saves RDB file, if the following option is enabled<br># the file will be fsync-ed every 32 MB of data generated. This is useful<br># in order to commit the file to the disk more incrementally and avoid<br># big latency spikes.<br>rdb-save-incremental-fsync yes<br><br># Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good<br># idea to start with the default settings and only change them after investigating<br># how to improve the performances and how the keys LFU change over time, which<br># is possible to inspect via the OBJECT FREQ command.<br>#<br># There are two tunable parameters in the Redis LFU implementation: the<br># counter logarithm factor and the counter decay time. It is important to<br># understand what the two parameters mean before changing them.<br>#<br># The LFU counter is just 8 bits per key, it’s maximum value is 255, so Redis<br># uses a probabilistic increment with logarithmic behavior. Given the value<br># of the old counter, when a key is accessed, the counter is incremented in<br># this way:<br>#<br># 1. A random number R between 0 and 1 is extracted.<br># 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).<br># 3. The counter is incremented only if R &lt; P.<br>#<br># The default lfu-log-factor is 10. This is a table of how the frequency<br># counter changes with a different number of accesses with different<br># logarithmic factors:<br>#<br># +——–+————+————+————+————+————+<br># | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |<br># +——–+————+————+————+————+————+<br># | 0      | 104        | 255        | 255        | 255        | 255        |<br># +——–+————+————+————+————+————+<br># | 1      | 18         | 49         | 255        | 255        | 255        |<br># +——–+————+————+————+————+————+<br># | 10     | 10         | 18         | 142        | 255        | 255        |<br># +——–+————+————+————+————+————+<br># | 100    | 8          | 11         | 49         | 143        | 255        |<br># +——–+————+————+————+————+————+<br>#<br># NOTE: The above table was obtained by running the following commands:<br>#<br>#   redis-benchmark -n 1000000 incr foo<br>#   redis-cli object freq foo<br>#<br># NOTE 2: The counter initial value is 5 in order to give new objects a chance<br># to accumulate hits.<br>#<br># The counter decay time is the time, in minutes, that must elapse in order<br># for the key counter to be divided by two (or decremented if it has a value<br># less &lt;= 10).<br>#<br># The default value for the lfu-decay-time is 1. A Special value of 0 means to<br># decay the counter every time it happens to be scanned.<br>#<br># lfu-log-factor 10<br># lfu-decay-time 1<br><br>########################### ACTIVE DEFRAGMENTATION #######################<br>#<br># WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested<br># even in production and manually tested by multiple engineers for some<br># time.<br>#<br># What is active defragmentation?<br># ——————————-<br>#<br># Active (online) defragmentation allows a Redis server to compact the<br># spaces left between small allocations and deallocations of data in memory,<br># thus allowing to reclaim back memory.<br>#<br># Fragmentation is a natural process that happens with every allocator (but<br># less so with Jemalloc, fortunately) and certain workloads. Normally a server<br># restart is needed in order to lower the fragmentation, or at least to flush<br># away all the data and create it again. However thanks to this feature<br># implemented by Oran Agra for Redis 4.0 this process can happen at runtime<br># in an “hot” way, while the server is running.<br>#<br># Basically when the fragmentation is over a certain level (see the<br># configuration options below) Redis will start to create new copies of the<br># values in contiguous memory regions by exploiting certain specific Jemalloc<br># features (in order to understand if an allocation is causing fragmentation<br># and to allocate it in a better place), and at the same time, will release the<br># old copies of the data. This process, repeated incrementally for all the keys<br># will cause the fragmentation to drop back to normal values.<br>#<br># Important things to understand:<br>#<br># 1. This feature is disabled by default, and only works if you compiled Redis<br>#    to use the copy of Jemalloc we ship with the source code of Redis.<br>#    This is the default with Linux builds.<br>#<br># 2. You never need to enable this feature if you don’t have fragmentation<br>#    issues.<br>#<br># 3. Once you experience fragmentation, you can enable this feature when<br>#    needed with the command “CONFIG SET activedefrag yes”.<br>#<br># The configuration parameters are able to fine tune the behavior of the<br># defragmentation process. If you are not sure about what they mean it is<br># a good idea to leave the defaults untouched.<br><br># Enabled active defragmentation<br># activedefrag yes<br><br># Minimum amount of fragmentation waste to start active defrag<br># active-defrag-ignore-bytes 100mb<br><br># Minimum percentage of fragmentation to start active defrag<br># active-defrag-threshold-lower 10<br><br># Maximum percentage of fragmentation at which we use maximum effort<br># active-defrag-threshold-upper 100<br><br># Minimal effort for defrag in CPU percentage<br># active-defrag-cycle-min 5<br><br># Maximal effort for defrag in CPU percentage<br># active-defrag-cycle-max 75<br><br># Maximum number of set/hash/zset/list fields that will be processed from<br># the main dictionary scan<br># active-defrag-max-scan-fields 1000<br>–&gt;</soft></soft></hard></class></milliseconds></bytes></password></master-password></masterport></masterip>]]></content>
    
    
    <summary type="html">&lt;p&gt;为深入学习了解&lt;code&gt;redis&lt;/code&gt;, 本文将对&lt;code&gt;redis5.0.0&lt;/code&gt;源码包目录下的&lt;code&gt;redis.conf&lt;/code&gt;配置文件进行完整翻译学习, 翻译顺序与配置文件保持一致;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>redis专题17 主从同步系列问题</title>
    <link href="http://researchlab.github.io/2018/10/15/redis-17-replication/"/>
    <id>http://researchlab.github.io/2018/10/15/redis-17-replication/</id>
    <published>2018-10-15T11:03:25.000Z</published>
    <updated>2021-02-20T17:55:59.527Z</updated>
    
    <content type="html"><![CDATA[<p>在生产环境中需要用到<code>redis</code>做数据持久化落地数据库时,  一般应搭建专属的<code>redis</code>集群来避免单点故障及单点读写性能问题, 如不是重度<code>redis</code>用户, 数据量压力不是特别大时, 也可以考虑采用<code>redis</code>主从同步架构代替, 本文将试图对<code>redis</code>主从同步原理, 步骤, 配置项, 实践等方面进行学习总结;<br><a id="more"></a></p><h5 id="主从同步目的"><a href="#主从同步目的" class="headerlink" title="主从同步目的"></a>主从同步目的</h5><blockquote><p>一旦主节点宕机, 从节点作为主节点的备份可以随时顶上来;<br>扩展主节点的读能力, 分担主节点读压力;</p></blockquote><h5 id="主从同步原理"><a href="#主从同步原理" class="headerlink" title="主从同步原理"></a>主从同步原理</h5><p><code>redis</code>支持主从复制, <code>redis</code>的主从结构可以采用一主多从或者级联结构, <code>redis</code>主从复制可以根据是否是全量分为全量同步和增量同步,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">       /  slave-1  /  slave-2-1</span><br><span class="line">Master -  slave-2--   slave-2-2</span><br><span class="line">       \  slave-3  \  slave-2-3</span><br><span class="line">        \ ...      \  ...</span><br><span class="line">          slave-n     slave-2-n</span><br></pre></td></tr></table></figure></p><h6 id="全量同步"><a href="#全量同步" class="headerlink" title="全量同步"></a>全量同步</h6><p><code>redis</code>全量复制一般发生在<code>slave</code>初始化阶段, 这时<code>slave</code>需要将Master上的所有数据都复制一份, 具体过程如下,</p><ul><li>1.从服务器连接主服务器, 发送<code>sync</code>命令;</li><li>2.主服务器接收到<code>sync</code>命名后, 开始执行<code>bgsave</code>命令生成RDB文件并使用复制积压缓冲区记录此后执行的所有写命令;</li><li>3.主服务器<code>bgsave</code>执行完后, 向所有从服务器发送快照文件, 并在发送期间继续记录被执行的写命令;</li><li>4.从服务器收到快照文件后丢弃所有旧数据, 载入收到的快照;</li><li>5.主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令;</li><li>6.从服务器完成对快照的载入, 开始接收命令请求, 并执行来自主服务器缓冲区的写命令;</li></ul><p>完成上面几个步骤后就完成了从服务器数据初始化的所有操作, 从服务器此时可以接收来自用户的读请求。</p><blockquote><p>若多个从服务器同时发来<code>sync</code>指令, 主服务器也只会执行一次<code>bgsave</code>, 然后把持久化好的RDB文件发给多个下游;</p></blockquote><h6 id="增量同步"><a href="#增量同步" class="headerlink" title="增量同步"></a>增量同步</h6><p>在redis2.8之前, redis仅支持<code>sync</code>全量同步操作, <code>sync</code>命令是一个非常耗费资源的操作, 为了解决主从服务器断线重来带来的<code>sync</code>重复复制问题, <code>redis</code>从2.8版本开始, 使用<code>psync</code>命令代替<code>sync</code>命令来执行复制时的同步操作。<br><code>psync</code>命令具有完整重同步<code>(full resynchronization)</code>和增量重同步<code>(partial resynchronization)</code>两种模式, 而增量同步策略大大降低了连接断开的恢复成本。增量同步过程如下,</p><ul><li>1.<code>master</code>端为复制流维护一个内存缓冲区<code>(in-memory backlog)</code>, 记录最近发送的复制流命令;</li><li>2.同时, Master和<code>slave</code>之间都维护一个复制偏移量<code>(replication offset)</code>和当前<code>master</code>服务器<code>ID(Masterrun id)</code>。</li><li>3.当网络断开, <code>slave</code>尝试重连时:<ul><li>a. 如果<code>masterID</code>相同(即仍是断网前的<code>master</code>服务器), 并且从断开时到当前时刻的历史命令依然在<code>master</code>的内存缓冲区中存在, 则Master会将缺失的这段时间的所有命令发送给<code>slave</code>执行, 然后复制工作就可以继续执行了;</li><li>b. 否则, 依然需要全量复制操作;</li></ul></li></ul><blockquote><p>增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令, 从服务器接收并执行收到的写命令。</p></blockquote><p>可见增量重同步功能由以下三个部分构成,</p><ul><li>1.主服务器的复制偏移量<code>(replication offset)</code>和从服务器的复制偏移量;</li><li>2.主服务器的复制积压缓冲区<code>(replication backlog)</code>;</li><li>3.服务器的运行<code>ID(run ID)</code>。</li></ul><p><strong>1.复制偏移量</strong></p><p>执行复制的双方——主服务器和从服务器会分别维护一个复制偏移量,</p><ul><li>主服务器每次向从服务器传播<code>N</code>个字节的数据时, 就将自己的复制偏移量的值加上<code>N</code>;</li><li>从服务器每次收到主服务器传播来的<code>N</code>个字节的数据时, 就将自己的复制偏移量的值加上<code>N</code>;</li></ul><blockquote><p>主从偏移量相同, 则当前主从处于一致状态, 反之则主从状态不一致;</p></blockquote><p>示例分析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">                          --(网络故障断线  )--&gt; slaveA ( offset=10086 )</span><br><span class="line">                        /</span><br><span class="line">                       /</span><br><span class="line">Master ( offset=10119 )-----(成功发送33字节)--&gt; slaveB ( offset=10119 )</span><br><span class="line">                       \</span><br><span class="line">                        \</span><br><span class="line">                          --(成功发送33字节)--&gt; slaveC ( offset=10119 )</span><br></pre></td></tr></table></figure><p>如上示例, 假设从服务器<code>A</code>在断线之后就立即重新连接主服务器成功, 那么接下来, 从服务器将向主服务器发送<code>psync</code>命令, 报告从服务器A当前的复制偏移量为<code>10086</code>, 那么这时, 主服务器应该对从服务器执行完整重同步还是部分重同步呢？如果执行部分重同步的话, 主服务器又如何补偿从服务器<code>A</code>在断线期间丢失的那部分数据呢？以上问题的答案都和复制积压缓冲区有关。</p><p><strong>2.复制积压缓冲区</strong><br>复制积压缓冲区是由主服务器维护的一个固定长度<code>(fixed-size)</code>先进先出<code>(FIFO)</code>队列, 默认大小为<code>1MB</code>。</p><p>和普通先进先出队列随着元素的增加和减少而动态调整长度不同, 固定长度先进先出队列的长度是固定的, 当入队元素的数量大于队列长度时, 最先入队的元素会被弹出, 而新元素会被放入队列。</p><p>当主服务器向从服务器发送命令时, 它不仅会将写命令发送给所有从服务器, 还会将写命令入队到复制积压缓冲区里面, 示意图,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">          --(向从服务器发送命令)--&gt; slaveA</span><br><span class="line">        /</span><br><span class="line">       /</span><br><span class="line">Master -----(向从服务器发送命令)--&gt; slave...</span><br><span class="line">       \</span><br><span class="line">        \</span><br><span class="line">          --(同步将命令写入队列)--&gt; | 复制积压缓冲区队列 |</span><br></pre></td></tr></table></figure><p>因此, 主服务器的复制积压缓冲区里面会保存着一部分最近发送的写命令, 并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量, 就像下表所示的那样,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">|偏移量| 10086 | 10087 | 10088 | 10089 |</span><br><span class="line">|字节值|   &apos;*&apos; |   5   | &apos;\r&apos;  | &apos;\n&apos;  |</span><br></pre></td></tr></table></figure><p>当从服务器重新连上主服务器时, 从服务器会通过<code>psync</code>命令将自己的复制偏移量offset发送给主服务器, 主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作:</p><blockquote><p>如果<code>offset</code>偏移量之后的数据(也即是偏移量<code>offset+1</code>开始的数据)仍然存在于复制积压缓冲区里面, 那么主服务器将对从服务器执行部分重同步操作;</p></blockquote><blockquote><p>相反, 如果<code>offset</code>偏移量之后的数据已经不存在于复制积压缓冲区, 那么主服务器将对从服务器执行完整重同步操作。</p></blockquote><p><strong>根据需要调整复制积压缓冲区的大小</strong></p><p><code>redis</code>为复制积压缓冲区设置的默认大小为<code>1MB</code>, 如果主服务器需要执行大量写命令, 又或者主从服务器断线后重连接所需的时间比较长, 那么这个大小也许并不合适。如果复制积压缓冲区的大小设置得不恰当, 那么<code>psync</code>命令的复制重同步模式就不能正常发挥作用, 因此, 正确估算和设置复制积压缓冲区的大小非常重要。<br>复制积压缓冲区的最小大小可以根据公式<code>second*write_size_per_second</code>来估算:</p><blockquote><p>其中<code>second</code>为从服务器断线后重新连接上主服务器所需的平均时间(以秒计算);</p></blockquote><blockquote><p>而<code>write_size_per_second</code>则是主服务器平均每秒产生的写命令数据量(协议格式的写命令的长度总和);</p></blockquote><p>例如, 如果主服务器平均每秒产生<code>1MB</code>的写数据, 而从服务器断线之后平均要<code>5秒</code>才能重新连接上主服务器, 那么复制积压缓冲区的大小就不能低于<code>5MB</code>。<br>为了安全起见, 可以将复制积压缓冲区的大小设为<code>2*second*write_size_per_second</code>, 这样可以保证绝大部分断线情况都能用部分重同步来处理。<br>可以根据实际需要, 修改配置文件中的<code>repl-backlog-size</code>选项来修改复制积压缓冲区的大小;</p><p><strong>3.服务器运行ID</strong><br>除了复制偏移量和复制积压缓冲区之外, 实现部分重同步还需要用到服务器运行<code>ID(run ID)</code>:</p><blockquote><p>每个<code>redis</code>服务器, 不论主服务器还是从服务, 都会有自己的运行<code>ID</code>;</p></blockquote><blockquote><p>运行<code>ID</code>在服务器启动时自动生成, 由40个随机的十六进制字符组成, 例如<code>53b9b28df8042fdc9ab5e3fcbbbabff1d5dce2b3</code>;</p></blockquote><p>当从服务器对主服务器进行初次复制时, 主服务器会将自己的运行<code>ID</code>传送给从服务器, 而从服务器则会将这个运行<code>ID</code>保存起来(注意哦, 是从服务器保存了主服务器的<code>ID</code>)。</p><p>当从服务器断线并重新连上一个主服务器时, 从服务器将向当前连接的主服务器发送之前保存的运行<code>ID</code>:</p><blockquote><p>如果从服务器保存的运行<code>ID</code>和当前连接的主服务器的运行<code>ID</code>相同, 那么说明从服务器断线之前复制的就是当前连接的这个主服务器, 主服务器可以继续尝试执行部分重同步操作;</p></blockquote><blockquote><p>相反地, 如果从服务器保存的运行<code>ID</code>和当前连接的主服务器的运行<code>ID</code>并不相同, 那么说明从服务器断线之前复制的主服务器并不是当前连接的这个主服务器, 主服务器将对从服务器执行完整重同步操作。</p></blockquote><p><strong>psync命令</strong><br><code>psync</code>命令的调用方法有两种,</p><blockquote><p>如果从服务器以前没有复制过任何主服务器, 或者之前执行过<code>SLAVEOF NO ONE</code>命令, 那么从服务器在开始一次新的复制时将向主服务器发送<code>psync ? -1</code>命令, 主动请求主服务器进行完整重同步(因为这时不可能执行部分重同步);</p></blockquote><blockquote><p>相反地, 如果从服务器已经复制过某个主服务器, 那么从服务器在开始一次新的复制时将向主服务器发送<code>psync &lt;runid&gt; &lt;offset&gt;</code>命令: 其中<code>runid</code>是上一次复制的主服务器的运行<code>ID</code>, 而<code>offset</code>则是从服务器当前的复制偏移量, 接收到这个命令的主服务器会通过这两个参数来判断应该对从服务器执行哪种同步操作。</p></blockquote><h5 id="主从同步策略"><a href="#主从同步策略" class="headerlink" title="主从同步策略"></a>主从同步策略</h5><p><code>redis</code> 的复制是异步进行的, <code>redis3.0</code>开始提供的<code>wait</code>指令可以让异步复制变身同步复制, 确保系统的强一致性,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; set key value</span><br><span class="line">OK</span><br><span class="line">&gt; wait 1 0</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure></p><p>wait 提供两个参数, 第一个参数是从库的数量<code>N</code>, 第二个参数是时间<code>t</code>, 以毫秒为单位。它表示等待<code>wait</code>指令之前的所有写操作同步到<code>N</code>个从库 (也就是确保<code>N</code>个从库的同步没有滞后), 最多等待时间<code>t</code>。如果时间<code>t=0</code>, 表示无限等待直到<code>N</code>个从库同步完成达成一致。</p><p>假设此时出现了网络分区,<code>wait</code>指令第二个参数时间<code>t=0</code>, 主从同步无法继续进行, <code>wait</code>指令会永远阻塞, <code>redis</code>服务器将丧失可用性。</p><p><strong>主从刚刚连接的时候, 进行全量同步; 全同步结束后, 进行增量同步。当然, 如果有需要, slave在任何时候都可以发起全量同步。redis策略是, 无论如何, 首先会尝试进行增量同步, 如不成功, 要求从机进行全量同步。</strong></p><h5 id="主从同步过程"><a href="#主从同步过程" class="headerlink" title="主从同步过程"></a>主从同步过程</h5><p>从服务器每次启动时, 会立即通过<code>slaveof master-host  master-port</code> 向主服务器发起主从复制同步请求; <code>SLAVEOF</code>命令是一个异步命令, 在完成<code>master-host</code>属性和<code>master-port</code>属性的设置工作之后, 从服务器将向发送<code>SLAVEOF</code>命令的客户端返回<code>OK</code>, 表示复制指令已经被接收, 而实际的复制工作将在<code>OK</code>返回之后才真正开始执行。从服务器开始发起主从复制请求到开始复制主要经历了如下7个步骤,</p><p><strong>步骤1</strong>: 设置主服务器的地址和端口, 通过<code>slaveof</code>指令发起主从复制同步请求,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:12345&gt; SLAVEOF 127.0.0.1 6379</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p><strong>步骤2</strong>: 建立套接字连接</p><p>在SLAVEOF命令执行之后, 从服务器将根据命令所设置的IP地址和端口, 创建连向主服务器的套接字连接, 如果从服务器创建的套接字能成功连接(connect)到主服务器, 那么从服务器将为这个套接字关联一个专门用于处理复制工作的文件事件处理器, 这个处理器将负责执行后续的复制工作, 比如接收RDB文件, 以及接收主服务器传播来的写命令, 诸如此类。而主服务器在接受(accept)从服务器的套接字连接之后, 将为该套接字创建相应的客户端状态, 并将从服务器看作是一个连接到主服务器的客户端来对待, 这时从服务器将同时具有服务器(server)和客户端(client)两个身份: 从服务器可以向主服务器发送命令请求, 而主服务器则会向从服务器返回命令回复。</p><p><strong>步骤3</strong>: 发送PING命令</p><p>从服务器成为主服务器的客户端之后, 做的第一件事就是向主服务器发送一个PING命令。</p><blockquote><p>通过发送PING命令检查套接字的读写状态;</p></blockquote><blockquote><p>通过PING命令可以检查主服务器能否正常处理命令。</p></blockquote><p>从服务器在发送PING命令之后可能遇到以下三种情况:</p><blockquote><p>主服务器向从服务器返回了一个命令回复, 但从服务器却不能在规定的时限内读取命令回复的内容(timeout), 说明网络连接状态不佳, 从服务器将断开并重新创建连向主服务器的套接字;</p></blockquote><blockquote><p>如果主服务器返回一个错误, 那么表示主服务器暂时没有办法处理从服务器的命令请求, 从服务器也将断开并重新创建连向主服务器的套接字;</p></blockquote><blockquote><p>如果从服务器读取到”PONG”回复, 那么表示主从服务器之间的网络连接状态正常, 那就继续执行下面的复制步骤。</p></blockquote><p><strong>步骤4</strong>: 身份验证</p><p>从服务器在收到主服务器返回的”PONG”回复之后, 下一步要做的就是决定是否进行身份验证:</p><p>如果从服务器设置了masterauth选项, 那么进行身份验证。否则不进行身份认证;<br>在需要进行身份验证的情况下, 从服务器将向主服务器发送一条AUTH命令, 命令的参数为从服务器masterauth选项的值。</p><p>从服务器在身份验证阶段可能遇到的情况有以下几种:</p><blockquote><p>主服务器没有设置requirepass选项, 从服务器没有设置masterauth,那么就继续后面的复制工作;</p></blockquote><blockquote><p>如果从服务器的通过AUTH命令发送的密码和主服务器requirepass选项所设置的密码相同, 那么也继续后面的工作, 否则返回错误invaild password;</p></blockquote><blockquote><p>如果主服务器设置了requireoass选项, 但从服务器没有设置masterauth选项, 那么服务器将返回NOAUTH错误。反过来如果主服务器没有设置requirepass选项, 但是从服务器却设置了materauth选项, 那么主服务器返回no password is set错误;</p></blockquote><p>所有错误到只有一个结果: 中止目前的复制工作, 并从创建套接字开始重新执行复制, 直到身份验证通过, 或者从服务器放弃执行复制为止。</p><p><strong>步骤5</strong>: 发送端口信息</p><p>身份验证步骤之后, 从服务器将执行命令<code>REPLCONF listening-port &lt;port-number&gt;</code>, 向主服务器发送从服务器的监听端口号。</p><p>主服务器在接收到这个命令之后, 会将端口号记录在从服务器所对应的客户端状态的<code>slave_listening_port</code>属性,</p><blockquote><p><code>slave_listening_port</code>属性目前唯一的作用就是在主服务器执行<code>INFO replication</code>命令时打印出从服务器的端口号。</p></blockquote><p><strong>步骤6</strong>: 同步</p><p>在这一步, 从服务器将向主服务器发送<code>psync</code>命令, 执行同步操作, 并将自己的数据库更新至主服务器数据库当前所处的状态。</p><p>需要注意的是在执行同步操作前, 只有从服务器是主服务器的客户端。但是执行同步操作之后, 主服务器也会成为从服务器的客户端,</p><blockquote><p>如果<code>psync</code>命令执行的是完整同步操作, 那么主服务器只有成为了从服务器的客户端才能将保存在缓冲区中的写命令发送给从服务器执行;</p></blockquote><blockquote><p>如果<code>psync</code>命令执行的是部分同步操作, 那么主服务器只有成为了从服务器的客户端才能将保存在复制积压缓冲区中的写命令发送给从服务器执行;</p></blockquote><p><strong>步骤7</strong>: 命令传播</p><p>当完成了同步之后, 主从服务器就会进入命令传播阶段, 这时主服务器只要一直将自己执行的写命令发送给从服务器, 而从服务器只要一直接收并执行主服务器发来的写命令, 就可以保证主从服务器一直保持一致了。</p><p><strong>心跳检测</strong><br>在命令传播阶段, 从服务器默认会以每秒一次的频率, 向主服务器发送命令: <code>REPLCONF ACK &lt;replication_offset&gt;</code></p><p>其中<code>replication_offset</code>是从服务器当前的复制偏移量。</p><p>发送<code>REPLCONF ACK</code>命令对于主从服务器有三个作用:</p><blockquote><p>检测主从服务器的网络连接状态;</p></blockquote><blockquote><p>辅助实现min-slaves选项;</p></blockquote><blockquote><p>检测命令丢失。</p></blockquote><p>检测主从服务器的网络连接状态</p><p>如果主服务器超过一秒钟没有收到从服务器发来的<code>REPLCONF ACK</code>命令, 那么主服务器就知道主从服务器之间的连接出现问题了。</p><p>通过向主服务器发送<code>INFO replication</code>命令, 在列出的从服务器列表的<code>lag</code>一栏中, 我们可以看到相应从服务器最后一次向主服务器发送<code>REPLCONF ACK</code>命令距离现在过了多少秒;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">10.1.195.19:8001&gt; info replication</span><br><span class="line"><span class="meta">#</span><span class="bash"> Replication</span></span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">slave0:ip=172.28.0.1,port=6379,state=online,offset=0,lag=1</span><br><span class="line">master_replid:10c63aebd8f09c749d1b26eccb52856b6d894292</span><br><span class="line">master_replid2:0000000000000000000000000000000000000000</span><br><span class="line">master_repl_offset:0</span><br><span class="line">second_repl_offset:-1</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:1</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">10.1.195.19:8001&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">刚刚发送过 REPLCONF ACK命令</span></span><br><span class="line">slave1:ip=171.28.0.1,port=6379,state=online,offset=197,lag=15</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">15秒之前发送过REPLCONF ACK命令</span></span><br><span class="line"></span><br><span class="line">master_repl_offset:211</span><br><span class="line">repl_backlog_active:1</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:2</span><br><span class="line">repl_backlog_histlen:210</span><br></pre></td></tr></table></figure><p>在一般情况下, <code>lag</code>的值应该在0秒或者1秒之间跳动, 如果超过1秒的话, 那么说明主从服务器之间的连接出现了故障。</p><p><strong>辅助实现min-slaves配置选项</strong></p><p><code>redis</code>的<code>min-slaves-to-write</code>和<code>min-slaves-max-lag</code>两个选项可以防止主服务器在不安全的情况下执行写命令。</p><p>举个例子, 如果我们向主服务器提供以下设置:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">min-slaves-to-write 3</span><br><span class="line">min-slaves-max-lag 10</span><br></pre></td></tr></table></figure></p><p>那么在从服务器的数量少于3个, 或者三个从服务器的延迟<code>(lag)</code>值都大于或等于10秒时, 主服务器将拒绝执行写命令, 这里的延迟值就是上面提到的<code>INFO replication</code>命令的<code>(lag)</code>值。</p><p><strong>检测命令丢失</strong></p><p>我们从命令: <code>REPLCONF ACK &lt;replication_offset&gt;</code>就可以知道, 每发送一次这个命令从服务器都会向主服务器报告一次自己的复制偏移量。那此时尽管主服务器发送给从服务器的<code>SET key value</code>丢失了。也无所谓, 主服务器马上就知道了。</p><h5 id="同步实践"><a href="#同步实践" class="headerlink" title="同步实践"></a>同步实践</h5><p><strong>0.环境准备</strong><br>通过构建如下主从级联架构来进一步实践<code>redis</code>主从复制同步机制,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master---&gt; slave01 --&gt; slave02</span><br></pre></td></tr></table></figure></p><p>即<code>master</code>为主服务器, <code>slave01</code>为<code>master</code>从服务器, 同时也是<code>slave02</code>主服务器,  由于要构建多个<code>redis container</code>环境, 为方便起见通过<code>docker-compose</code>来实践,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">version: '3'</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  master-redis:</span><br><span class="line">    image: redis</span><br><span class="line">    container_name: master</span><br><span class="line">    ports:</span><br><span class="line">      - "8001:6379"</span><br><span class="line">    volumes:</span><br><span class="line">      - ~/workbench/docker/docker-compose/redis/conf/redis.conf:/etc/redis/redis.conf</span><br><span class="line">    entrypoint: ["redis-server", "/etc/redis/redis.conf"]</span><br><span class="line"></span><br><span class="line">  slave-redis-01:</span><br><span class="line">    image: redis</span><br><span class="line">    container_name: slave01</span><br><span class="line">    ports:</span><br><span class="line">      - "8002:6379"</span><br><span class="line">    volumes:</span><br><span class="line">      - ~/workbench/docker/docker-compose/redis/conf/redis.conf:/etc/redis/redis.conf</span><br><span class="line">    entrypoint: ["redis-server", "/etc/redis/redis.conf"]</span><br><span class="line"></span><br><span class="line">  slave-redis-02:</span><br><span class="line">    image: redis</span><br><span class="line">    container_name: slave02</span><br><span class="line">    ports:</span><br><span class="line">      - "8003:6379"</span><br><span class="line">    volumes:</span><br><span class="line">      - ~/workbench/docker/docker-compose/redis/conf/redis.conf:/etc/redis/redis.conf</span><br><span class="line">    entrypoint: ["redis-server", "/etc/redis/redis.conf"]</span><br></pre></td></tr></table></figure></p><p>将<code>redis.conf</code>配置做如下修改,</p><ul><li>将<code>bind 127.0.0.1</code> 修改为<code>bind 0.0.0.0</code>;</li><li>将<code>daemonize yes</code> 修改为<code>daemonize no</code>, 让<code>redis</code>运行在前台;</li><li>将<code>protected-mode yes</code>修改为<code>protected-mode no</code>, 在<code>protected-mode yes</code>模式下需要设置密码才能远程访问, 否则<code>redis</code>只接受本地访问;</li></ul><p><strong>1.发送<code>slaveof</code>指令,请求主从同步</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  redis docker-compose up -d</span><br><span class="line">Creating network "redis_default" with the default driver</span><br><span class="line">Creating master  ... done</span><br><span class="line">Creating slave02 ... done</span><br><span class="line">Creating slave01 ... done</span><br><span class="line">➜  redis docker-compose ps</span><br><span class="line"> Name                Command               State           Ports</span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">master    redis-server /etc/redis/re ...   Up      0.0.0.0:8001-&gt;6379/tcp</span><br><span class="line">slave01   redis-server /etc/redis/re ...   Up      0.0.0.0:8002-&gt;6379/tcp</span><br><span class="line">slave02   redis-server /etc/redis/re ...   Up      0.0.0.0:8003-&gt;6379/tcp</span><br><span class="line">➜  redis ifconfig |grep inet |grep -v 127.0.0.1</span><br><span class="line">        inet6 ::1 prefixlen 128</span><br><span class="line">        inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1</span><br><span class="line">        inet6 fe80::475:d879:2a90:ac35%en0 prefixlen 64 secured scopeid 0x5</span><br><span class="line">        inet 10.1.195.19 netmask 0xffffff00 broadcast 10.1.195.255</span><br></pre></td></tr></table></figure><p>上述命令, 分别启动了一台<code>master</code>主机, 两台<code>slave01</code>,<code>slave02</code>从机, 并得知当前宿主机ip为<code>10.1.195.19</code></p><p>登录从机发送<code>slaveof</code>指令, 开始主从复制同步,<br>主机<code>master</code> 执行命令,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  docker exec -it master redis-cli -h 10.1.195.19 -p 8001</span><br><span class="line">10.1.195.19:8001&gt; flushdb</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8001&gt; hmset info name mike city shanghai code 110</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8001&gt; hgetall info</span><br><span class="line">1) "name"</span><br><span class="line">2) "mike"</span><br><span class="line">3) "city"</span><br><span class="line">4) "shanghai"</span><br><span class="line">5) "code"</span><br><span class="line">6) "110"</span><br><span class="line">10.1.195.19:8001&gt;</span><br></pre></td></tr></table></figure></p><p>从机<code>slave01</code> 查询命令,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">➜  docker exec -it slave01 redis-cli -h 10.1.195.19 -p 8002</span><br><span class="line">10.1.195.19:8002&gt; slaveof no one</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8002&gt; flushdb</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8002&gt; slaveof 10.1.195.19 8001</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8002&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "info"</span><br><span class="line">10.1.195.19:8002&gt; hgetall info</span><br><span class="line">1) "name"</span><br><span class="line">2) "mike"</span><br><span class="line">3) "city"</span><br><span class="line">4) "shanghai"</span><br><span class="line">5) "code"</span><br><span class="line">6) "110"</span><br><span class="line">10.1.195.19:8002&gt;</span><br></pre></td></tr></table></figure></p><p>从机<code>slave02</code>查询命令,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  docker exec -it slave02 redis-cli -h 10.1.195.19 -p 8003</span><br><span class="line">10.1.195.19:8003&gt; slaveof no one</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8003&gt; flushdb</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8003&gt; slaveof 10.1.195.19 8002</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8003&gt; hgetall info</span><br><span class="line">1) "name"</span><br><span class="line">2) "mike"</span><br><span class="line">3) "city"</span><br><span class="line">4) "shanghai"</span><br><span class="line">5) "code"</span><br><span class="line">6) "110"</span><br><span class="line">10.1.195.19:8003&gt;</span><br></pre></td></tr></table></figure></p><ul><li><code>docker exec -it ${docker-name} redis-cli -h ${localhost-ip} -p ${port}</code>命令登录到各个<code>redis</code>实例服务;</li><li><code>slaveof no one</code>命令将当前<code>redis</code>实例的主从服务;</li><li>从上述实例可见, 通过在<code>master</code>主机中执行<code>hmset</code>命令, 相应的从机以及级联从机也都同步了主机的执行命令, 上述示例显示主从复制同步成功;</li></ul><p>从机配置默认开启了只读模式<code>slave-read-only yes</code>,所以对从机进行修改操作是不许可的, 也不建议这么做;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.1.195.19:8003&gt; flushdb</span><br><span class="line">(error) READONLY You can't write against a read only replicx</span><br><span class="line">10.1.195.19:8003&gt;</span><br></pre></td></tr></table></figure></p><p>当对主机操作<code>flushdb</code>时, 可见从机也执行了<code>flushdb</code>命令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">10.1.195.19:8001&gt; flushdb</span><br><span class="line">OK</span><br><span class="line">10.1.195.19:8001&gt;</span><br><span class="line"></span><br><span class="line">10.1.195.19:8002&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) (empty list or set)</span><br><span class="line">10.1.195.19:8002&gt;</span><br><span class="line"></span><br><span class="line">10.1.195.19:8003&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) (empty list or set)</span><br><span class="line">10.1.195.19:8003&gt;</span><br></pre></td></tr></table></figure></p><p>当关闭<code>master</code>服务后, 对从机服务无影响,  所以从机服务的启动顺序及服务提供 与<code>master</code>主机服务是否已启动及启动顺序无关;</p><p>当然上面<code>docker-compose</code>配置也可以直接配置<code>slaveof</code>参数， 就不需要手动去发起命令, 可参看 <a href="https://github.com/researchlab/docker-envs/tree/master/redis/docker-compose-redis-replication" target="_blank" rel="noopener">docker-compose-redis-replication</a></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li><code>redis</code>的主从同步是异步进行的, 这意味着主从同步不会影响主逻辑, 也不会降低<code>redis</code>的处理性能。</li><li>主从架构中, 可以考虑关闭主服务器的数据持久化功能, 只让从服务器进行持久化, 这样可以提高主服务器的处理性能。</li><li>在主从架构中, 从服务器通常被设置为只读模式, 这样可以避免从服务器的数据被误修改。但是从服务器仍然可以接受<code>config</code>等指令, 所以还是不应该将从服务器直接暴露到不安全的网络环境中。如果必须如此, 那可以考虑给重要指令进行重命名, 来避免命令被外人误执行。</li><li>主从同步分为<code>全量同步</code>和<code>增量同步</code>, 文中对<code>全量同步</code>和<code>增量同步</code>原理,实现过程进行了阐述分析;</li><li>进一步阐述了主从不同执行策略, 即从机初始连接主机时, 先进行增量同步, 若增量同步失败, 则进行全量同步,  同时可以利用<code>wait</code>指令 将redis主从同步的异步行为转变为同步行为;</li><li>分析了主从同步过程,  主从同步开始至复制, 大致进来7个过程, 1.发起<code>slaveof</code>指令;2.建立套接字连接;3.发送ping指令测试连接状态;4.身份认证;5.发送端口信息;6.同步初始化阶段,从机载入主机RDB,准备接受主机命令;7.增量同步;</li><li>通过docker-compose 构建<code>主-从-从</code>架, 深入实践分析<code>redis</code>主从复制同步过程;</li><li>主从复制是 <code>redis</code> 分布式的基础, <code>redis</code> 的高可用离开了主从复制将无从进行;</li><li>不过复制功能也不是必须的, 如果你将 <code>redis</code> 只用来做缓存, 跟<code>memcache</code>一样来对待, 也就无需要从库做备份, 挂掉了重新启动一下就行。但是只要你使用了 <code>redis</code> 的持久化功能, 就必须认真对待主从复制, 它是系统数据安全的基础保障;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在生产环境中需要用到&lt;code&gt;redis&lt;/code&gt;做数据持久化落地数据库时,  一般应搭建专属的&lt;code&gt;redis&lt;/code&gt;集群来避免单点故障及单点读写性能问题, 如不是重度&lt;code&gt;redis&lt;/code&gt;用户, 数据量压力不是特别大时, 也可以考虑采用&lt;code&gt;redis&lt;/code&gt;主从同步架构代替, 本文将试图对&lt;code&gt;redis&lt;/code&gt;主从同步原理, 步骤, 配置项, 实践等方面进行学习总结;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis专题16 数据持久化实践</title>
    <link href="http://researchlab.github.io/2018/10/13/redis-16-storage-solution-opt/"/>
    <id>http://researchlab.github.io/2018/10/13/redis-16-storage-solution-opt/</id>
    <published>2018-10-13T11:12:11.000Z</published>
    <updated>2021-02-20T17:55:59.526Z</updated>
    
    <content type="html"><![CDATA[<p>通过示例分析，深入了解<code>redis</code>数据持久化策略执行机制；<br><a id="more"></a></p><h5 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h5><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ docker run -itd --name lredis -p7002:6379 -v ~/workbench/docker/redis/conf/redis.conf:/etc/redis/redis.conf redis redis-server /etc/redis/redis.conf</span><br></pre></td></tr></table></figure><h5 id="版本日志"><a href="#版本日志" class="headerlink" title="版本日志"></a>版本日志</h5><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ docker logs -f lredis</span><br><span class="line">1:C 24 Oct 2018 09:24:03.601 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">1:C 24 Oct 2018 09:24:03.602 # Redis version=5.0.0, bits=64, commit=00000000, modified=0, pid=1, just started</span><br><span class="line">...</span><br><span class="line">1:M 24 Oct 2018 09:24:03.606 # Server initialized</span><br><span class="line">1:M 24 Oct 2018 09:24:03.606 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span><br><span class="line">1:M 24 Oct 2018 09:24:03.606 * Ready to accept connections</span><br></pre></td></tr></table></figure><h5 id="RDB方式持久化"><a href="#RDB方式持久化" class="headerlink" title="RDB方式持久化"></a>RDB方式持久化</h5><hr><p><code>Redis</code>默认的持久化方式是<code>RDB</code>，并且默认是打开的。<code>RDB</code>的保存有方式分为<code>主动保存</code>与<code>被动保存</code>。<code>主动保存</code>可以在<code>redis-cli</code>中输入<code>save</code>即可；<code>被动保存</code>需要满足配置文件中设定的触发条件，满足触发条件后，数据才会被保存为快照，正是因为这样才说<code>RDB</code>的数据完整性是比不上<code>AOF</code>;<br>触发保存条件后，会在指定的目录生成一个名为<code>dump.rdb</code>的文件，等到下一次启动<code>Redis</code>时，<code>Redis</code>会去读取该目录下的<code>dump.rdb</code>文件，将里面的数据恢复到<code>Redis</code>;</p><p><code>dump.rdb</code>文件路径,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; config get dir</span><br><span class="line">1) "dir"</span><br><span class="line">2) "/data"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure></p><p>目前官方默认的触发条件可以在<code>redis.conf</code>中看到,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">save 900 1              #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 300 10            #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 60 10000        #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。</span><br></pre></td></tr></table></figure></p><p>为了便于测试， 把上面默认的配置修改为，<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">save 900 1              #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 300 10            #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 20 5        #在20秒之后，如果至少有5个key发生变化，则dump内存快照。</span><br></pre></td></tr></table></figure></p><ol><li><strong><code>RDB</code>被动触发保存实践</strong><br>input<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; set a 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set b 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set c 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set d 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set e 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set f 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "a"</span><br><span class="line">   2) "e"</span><br><span class="line">   3) "b"</span><br><span class="line">   4) "d"</span><br><span class="line">   5) "f"</span><br><span class="line">   6) "c"</span><br></pre></td></tr></table></figure></li></ol><p><code>RDB</code>被动保存成功生成了<code>rdb</code>文件及日志,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker exec -it lredis ls /data</span><br><span class="line">dump.rdb</span><br></pre></td></tr></table></figure></p><p>日志记录,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1:M 24 Oct 2018 09:47:49.394 * DB loaded from disk: 0.000 seconds</span><br><span class="line">1:M 24 Oct 2018 09:47:49.394 * Ready to accept connections</span><br><span class="line">1:M 24 Oct 2018 09:48:48.758 * 5 changes in 20 seconds. Saving...</span><br><span class="line">1:M 24 Oct 2018 09:48:48.759 * Background saving started by pid 25</span><br><span class="line">25:C 24 Oct 2018 09:48:48.768 * DB saved on disk</span><br><span class="line">25:C 24 Oct 2018 09:48:48.769 * RDB: 0 MB of memory used by copy-on-write</span><br><span class="line">1:M 24 Oct 2018 09:48:48.860 * Background saving terminated with success</span><br></pre></td></tr></table></figure></p><p>日志提示<code>redis</code>检测到<code>20</code>秒内有至少<code>5</code>条记录被改动，满足<code>redis.conf</code>中对于<code>RDB</code>数据保存的条件，所以这里执行数据保存操作，并且提示开辟了一个<code>25</code>的进程出来执行保存操作，最后提示保存成功;</p><p>现在将redis进程kill，哪些数据会被保存？</p><p>通过命令<code>docker restart lredis</code>模拟<code>Redis</code>异常关闭，然后再启动<code>Redis</code>，再次查看之前<code>set</code>设置的内容,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "b"</span><br><span class="line">   2) "e"</span><br><span class="line">   3) "f"</span><br><span class="line">   4) "d"</span><br><span class="line">   5) "a"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure></p><p>发现<code>c</code>不见了， 可见<code>RDB</code>方式的数据完整性是不可靠的，除非断掉的那一刻正好是满足触发条件的条数;</p><p>关闭<code>RDB</code>方式持久化<br>修改<code>redis.conf</code>配置为,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save ""</span><br><span class="line"><span class="meta">#</span><span class="bash">save 900 1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">save 300 10</span></span><br><span class="line"><span class="meta">#</span><span class="bash">save 20 5</span></span><br></pre></td></tr></table></figure></p><p>重复上述<code>RDB</code>被动保存过程,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">➜ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "a"</span><br><span class="line">   2) "d"</span><br><span class="line">   3) "c"</span><br><span class="line">   4) "e"</span><br><span class="line">   5) "b"</span><br><span class="line">127.0.0.1:6379&gt; set f 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set i 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set j 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "a"</span><br><span class="line">   2) "j"</span><br><span class="line">   3) "f"</span><br><span class="line">   4) "d"</span><br><span class="line">   5) "c"</span><br><span class="line">   6) "e"</span><br><span class="line">   7) "b"</span><br><span class="line">   8) "i"</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br><span class="line">➜ docker restart lredis</span><br><span class="line">lredis</span><br><span class="line">➜ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "e"</span><br><span class="line">   2) "d"</span><br><span class="line">   3) "b"</span><br><span class="line">   4) "c"</span><br><span class="line">   5) "a"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure></p><p>发现后面添加的<code>3</code>条记录并没有被保存，恢复数据的时候仅仅只是恢复了之前的<code>5</code>条。并且观察<code>Redis</code>服务端窗口日志，并未发现像之前一样的触发保存的提示，证明<code>RDB</code>方式已经被关闭;</p><p>通过配置文件关闭被动触发，那么主动关闭是否还会生效呢？</p><ol start="2"><li><strong><code>RDB</code>主动保存实践</strong><br>通过<code>del</code>命令删除几条记录，然后输入<code>save</code>命令执行保存操作,<br>input opt<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "e"</span><br><span class="line">   2) "d"</span><br><span class="line">   3) "b"</span><br><span class="line">   4) "c"</span><br><span class="line">   5) "a"</span><br><span class="line">127.0.0.1:6379&gt; del e d</span><br><span class="line">(integer) 2</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "b"</span><br><span class="line">   2) "c"</span><br><span class="line">   3) "a"</span><br><span class="line">127.0.0.1:6379&gt; save</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li></ol><p>log output<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1:M 24 Oct 2018 10:09:55.295 * Ready to accept connections</span><br><span class="line">1:M 24 Oct 2018 10:14:28.366 * DB saved on disk</span><br></pre></td></tr></table></figure></p><p>然后执行 <code>docker restart lredis</code>,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker restart lredis</span><br><span class="line">lredis</span><br><span class="line">➜  ~ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "a"</span><br><span class="line">   2) "c"</span><br><span class="line">   3) "b"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure></p><p>可以看到当<code>RDB</code>被动保存关闭后，可以通过主动<code>save</code>保存成功, 证明主动关闭不受 配置文件的影响;</p><p>除了save还有其他的保存方式么？</p><p><code>Redis</code>提供了<code>save</code>和<code>bgsave</code>这两种不同的保存方式, 并且这两个方式在执行的时候都会调用<code>rdbSave</code>函数，但它们调用的方式各有不同:</p><blockquote><p><code>save</code>直接调用<code>rdbSave</code>方法, 阻塞<code>Redis</code>主进程，直到保存完成为止。在主进程阻塞期间，服务器不能处理客户端的任何请求。</p></blockquote><blockquote><p><code>bgsave</code>则<code>fork</code>出一个子进程，子进程负责调用<code>rdbSave</code>，并在保存完成之后向主进程发送信号，通知保存已完成。因为<code>rdbSave</code>在子进程被调用，所以 <code>Redis</code>服务器在<code>bgsave</code>执行期间仍然可以继续处理客户端的请求。</p></blockquote><p>显然， <code>save</code>是同步操作, <code>bgsave</code>是异步操作。bgsave命令的使用方法和save命令的使用方法是一样的,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "a"</span><br><span class="line">   2) "c"</span><br><span class="line">   3) "b"</span><br><span class="line">127.0.0.1:6379&gt; set e 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set d 1</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; bgsave</span><br><span class="line">Background saving started</span><br></pre></td></tr></table></figure></p><p><code>redis</code>除了提供<code>save</code>和<code>bgsave</code>来保存数据外, 还可以通过<code>shutdown</code>命令来保存数据，不过要让<code>shutdown</code>保存数据生效需要打开持久化配置才行，即应将<code>redis.conf</code>中的配置从<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">save ""</span><br></pre></td></tr></table></figure></p><p>修改为如下，打开被动保存持久化配置才生效;<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br></pre></td></tr></table></figure></p><p>总结<code>RDB</code>持久化有<code>被动保存</code>和<code>主动保存</code>两种方式，</p><blockquote><p>被动保存即通过<code>redis.conf</code>通过保存条件触发被动保存，这种情况数据有可能丢失;<br>主动保存即通过显示执行<code>save</code>, <code>bgsave</code>,<code>shutdown(在被动保存配置下才生效)</code>命令，这种情况数据不会丢失;</p></blockquote><h5 id="AOF方式持久化"><a href="#AOF方式持久化" class="headerlink" title="AOF方式持久化"></a>AOF方式持久化</h5><hr><p><code>redis</code>默认没有开启<code>AOF</code>，需要修改redis.conf配置文件中开启,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将appendonly改为 yes</span></span><br><span class="line">appendonly yes</span><br></pre></td></tr></table></figure></p><p><code>AOF</code>可以需要设置同步方式<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">appendfsync always  # 每次有数据修改发生时都会写入AOF文件（安全但是费时）。</span><br><span class="line">appendfsync everysec  # 每秒钟同步一次，该策略为AOF的缺省策略。</span><br><span class="line">appendfsync no  # 从不同步。高效但是数据不会被持久化。</span><br></pre></td></tr></table></figure></p><p>根据上面的设置重启<code>redis</code>后，可见<code>data</code>目录下已创建了<code>aof</code>空文件,<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker exec -it lredis ls -lh /data</span><br><span class="line">total 4.0K</span><br><span class="line">-rw-r--r-- 1 redis redis   0 Oct 24 10:31 appendonly.aof</span><br><span class="line">-rw-r--r-- 1 redis redis 122 Oct 24 10:20 dump.rdb</span><br></pre></td></tr></table></figure></p><p>input<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; set name china</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set age 80</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set city "shanghai.china"</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "age"</span><br><span class="line">   2) "name"</span><br><span class="line">   3) "city"</span><br><span class="line">127.0.0.1:6379&gt; quit</span><br><span class="line">➜  ~ docker exec -it lredis cat /data/appendonly.aof</span><br><span class="line">*2</span><br><span class="line"><span class="meta">$</span><span class="bash">6</span></span><br><span class="line">SELECT</span><br><span class="line"><span class="meta">$</span><span class="bash">1</span></span><br><span class="line">0</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">name</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">china</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">age</span><br><span class="line"><span class="meta">$</span><span class="bash">2</span></span><br><span class="line">80</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">city</span><br><span class="line"><span class="meta">$</span><span class="bash">14</span></span><br><span class="line">shanghai.china</span><br><span class="line">➜  ~ docker exec -it lredis ls -lh /data</span><br><span class="line">total 8.0K</span><br><span class="line">-rw-r--r-- 1 redis redis 131 Oct 24 10:35 appendonly.aof</span><br><span class="line">-rw-r--r-- 1 redis redis 122 Oct 24 10:20 dump.rdb</span><br></pre></td></tr></table></figure></p><p>由上可见, <code>appendonly.aof</code>文件由<code>0</code>增大到<code>131</code> bytes, 可见<code>AOF</code>方式持久化成功了， 可以看见<code>appendonly.aof</code>文件中不仅仅保存了设置的变量及值，这些变量及值前后还有一些特殊的符号，这正是根据<code>redis</code>采用的<code>RESP</code>文本协议生成的， 详情可见之前总结的 <a href="http://researchlab.github.io/2018/10/09/redis-12-resp/">redis专题12 redis通信协议</a><br>分析<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">14</span></span><br><span class="line">shanghai.china</span><br><span class="line">多字符用$开头， $后边紧跟字符串的长度, 然后是 \r\n, 然后是字符串值本身</span><br></pre></td></tr></table></figure></p><blockquote><p><code>AOF</code> 同样也会把<code>del</code>等执行命令保存到<code>AOF</code>文件中;</p></blockquote><blockquote><p>当关闭<code>RDB</code>持久化方式， 只打开<code>AOF</code>方式时， 显示执行<code>save</code>和<code>bgsave</code> 都会将当前数据同时保存到<code>AOF</code>文件和<code>RDB</code>文件中;</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">➜ docker exec -it lredis ls -lh /data</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 redis redis 0 Oct 24 10:55 appendonly.aof</span><br><span class="line">➜ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; set name china</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set age 80</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; quit</span><br><span class="line">➜ docker exec -it lredis ls -lh /data</span><br><span class="line">total 4.0K</span><br><span class="line">-rw-r--r-- 1 redis redis 87 Oct 24 10:55 appendonly.aof</span><br><span class="line">➜ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; set city shanghai</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; save</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; quit</span><br><span class="line">➜ docker exec -it lredis ls -lh /data</span><br><span class="line">total 8.0K</span><br><span class="line">-rw-r--r-- 1 redis redis 124 Oct 24 10:56 appendonly.aof</span><br><span class="line">-rw-r--r-- 1 redis redis 131 Oct 24 10:56 dump.rdb</span><br><span class="line">➜ docker exec -it lredis redis-cli</span><br><span class="line">127.0.0.1:6379&gt; set local library</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; bgsave</span><br><span class="line">Background saving started</span><br><span class="line">127.0.0.1:6379&gt; quit</span><br><span class="line">➜ docker exec -it lredis ls -lh /data</span><br><span class="line">total 8.0K</span><br><span class="line">-rw-r--r-- 1 redis redis 161 Oct 24 10:56 appendonly.aof</span><br><span class="line">-rw-r--r-- 1 redis redis 146 Oct 24 10:56 dump.rdb</span><br><span class="line">➜ docker exec -it lredis cat /data/appendonly.aof</span><br><span class="line">*2</span><br><span class="line"><span class="meta">$</span><span class="bash">6</span></span><br><span class="line">SELECT</span><br><span class="line"><span class="meta">$</span><span class="bash">1</span></span><br><span class="line">0</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">name</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">china</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">age</span><br><span class="line"><span class="meta">$</span><span class="bash">2</span></span><br><span class="line">80</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">4</span></span><br><span class="line">city</span><br><span class="line"><span class="meta">$</span><span class="bash">8</span></span><br><span class="line">shanghai</span><br><span class="line">*3</span><br><span class="line"><span class="meta">$</span><span class="bash">3</span></span><br><span class="line">set</span><br><span class="line"><span class="meta">$</span><span class="bash">5</span></span><br><span class="line">local</span><br><span class="line"><span class="meta">$</span><span class="bash">7</span></span><br><span class="line">library</span><br></pre></td></tr></table></figure><p>从<code>RDB</code>方式切换到<code>AOF</code>方式<br>在<code>Redis2.2</code>或以上版本，可以在不重启的情况下，从<code>RDB</code>切换到<code>AOF</code>,<br>为最新的<code>dump.rdb</code>文件创建一个备份、将备份放到一个安全的地方。执行以下两条命令:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">redis-cli config set appendonly yes</span><br><span class="line">redis-cli config set save “”</span><br></pre></td></tr></table></figure></p><blockquote><p>执行的第一条命令开启了<code>AOF</code>功能: <code>Redis</code>会阻塞直到初始<code>AOF</code>文件创建完成为止, 之后<code>Redis</code>会继续处理命令请求, 并开始将写入命令追加到<code>AOF</code>文件末尾;<br>通过上述<code>CONFIG SET</code>命令设置的配置， 在重启<code>redis</code>服务器之后将失效，重启会依然按照之前的配置启动，所以建议在<code>redis.conf</code>配置中也应同步修改;</p></blockquote><h5 id="备份建议"><a href="#备份建议" class="headerlink" title="备份建议"></a>备份建议</h5><hr><p>确保你的数据有完整的备份，磁盘故障、节点失效等问题问题可能让你的数据消失不见， 不进行备份是非常危险的。<br><code>Redis</code>对于数据备份是非常友好的， 因为你可以在服务器运行的时候对<code>RDB</code>文件进行复制, RDB 文件一旦被创建, 就不会进行任何修改。 当服务器要创建一个新的<code>RDB</code>文件时，先将文件的内容保存在一个临时文件里面, 当临时文件写入完毕时, 程序才使用<code>rename(2)</code>原子地用临时文件替换原来的<code>RDB</code>文件。<br>即无论何时, 复制<code>RDB</code>文件都是绝对安全的。</p><blockquote><p>创建一个定期任务, 每小时将一个<code>RDB</code>文件备份到一个文件夹, 并且每天将一个<code>RDB</code>文件备份到另一个文件夹;</p></blockquote><blockquote><p>确保快照的备份都带有相应的日期和时间信息, 每次执行定期任务脚本时, 使用 find 命令来删除过期的快照, 比如保留最近<code>48</code>小时内的每小时快照, 还可以保留最近一两个月的每日快照;</p></blockquote><blockquote><p>至少每天一次, 将<code>RDB</code> 备份到你的数据中心之外, 或者至少是备份到你运行<code>Redis</code>服务器的物理机器之外;</p></blockquote><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><hr><ul><li>为实践<code>redis</code>持久化机制，创建了<code>docker</code>容器环境;</li><li>针对<code>RDB</code>方式持久化，分别测试了其主动保存和被动保存机制， 被动保存存在丢数据的可能，而主动保存则不会， 被动保存通过配置触发保存条件实现, 主动保存主要通过显示执行<code>save</code>,<code>bgsave</code>,<code>shutdown(需开启被动保存)</code>来执行数据保存操作;</li><li>针对<code>AOF</code>方式持久化进行了实例分析测试，<code>AOF</code> 开启后自动保存操作记录到<code>AOF</code>文件， 当显示执行<code>save</code>, <code>bgsave</code>,<code>shutdown</code>操作时也会自动保存数据到<code>AOF</code>和<code>RDB</code>文件中；</li><li>探讨了在不停服情况下从<code>RDB</code>方式切换至<code>AOF</code>的方法，并给出了几点备份建议;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过示例分析，深入了解&lt;code&gt;redis&lt;/code&gt;数据持久化策略执行机制；&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis专题15 事务系列问题</title>
    <link href="http://researchlab.github.io/2018/10/12/redis-15-transaction/"/>
    <id>http://researchlab.github.io/2018/10/12/redis-15-transaction/</id>
    <published>2018-10-12T07:39:41.000Z</published>
    <updated>2021-02-20T17:55:59.526Z</updated>
    
    <content type="html"><![CDATA[<p>为了确保连续多个操作的原子性, 一个成熟的数据库通常都会有事务支持, <code>Redis</code> 也不例外, <code>Redis</code> 通过<code>MULTI</code>, <code>DISCARD</code>, <code>EXEC</code>, <code>WATCH</code>和<code>UNWATCH</code> 五个命令来实现事务功能;<br><a id="more"></a></p><h5 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h5><p>事务提供了一种”将多个命令打包， 然后一次性、按顺序地执行”的机制, 并且事务在执行的期间不会主动中断 —— 服务器在执行完事务中的所有命令之后, 才会继续处理其他客户端的其他命令。</p><h5 id="Redis事务"><a href="#Redis事务" class="headerlink" title="Redis事务"></a><code>Redis</code>事务</h5><p>一个基本<code>Redis</code>事务从MULIT命令开始一个事务, 然后将多个命令入队到事务中， 最后由<code>EXEC</code>命令触发事务,</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; `MULTI`</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set name mike</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; incr name</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; set city shanghai</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; get name</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; get city</span><br><span class="line">QUEUED</span><br><span class="line">127.0.0.1:6379&gt; `EXEC`</span><br><span class="line">1) OK</span><br><span class="line">2) (error) ERR value is not an integer or out of range</span><br><span class="line">3) OK</span><br><span class="line">4) "mike"</span><br><span class="line">5) "shanghai"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><ul><li><code>MULTI</code>命令唯一做的就是, 将客户端的 <code>Redis</code>_<code>MULTI</code> 选项打开, 让客户端从非事务状态切换到事务状态;</li><li>当客户端处于非事务状态下时， 所有发送给服务器端的命令都会立即被服务器执行; 当客户端进入事务状态之后, 服务器在收到来自客户端的命令时, 不会立即执行命令, 而是将这些命令全部放进一个事务队列里, 然后返回QUEUED, 表示命令已入队;</li><li>事务队列里的所有命令被执行完之后, <code>EXEC</code>命令会将回复队列作为自己的执行结果返回给客户端, 客户端从事务状态返回到非事务状态, 事务执行完毕;</li><li>从上述示例中可得当<code>incr name</code>失败后, 依然继续执行后继的<code>set city</code>等命令，可见<code>Redis</code>事务不保证执行<code>原子性</code>操作, 仅满足<code>隔离性</code>执行;</li></ul><p>事务与非事务状态的区别<br>事务中的命令和普通命令在执行上还是有一点区别的，其中最重要的两点是：</p><blockquote><p>非事务状态下的命令以单个命令为单位执行，前一个命令和后一个命令的客户端不一定是同一个; 而事务状态则是以一个事务为单位，执行事务队列中的所有命令:除非当前事务执行完毕，否则服务器不会中断事务，也不会执行其他客户端的其他命令;</p></blockquote><blockquote><p>在非事务状态下, 执行命令所得的结果会立即被返回给客户端; 而事务则是将所有命令的结果集合到回复队列，再作为<code>EXEC</code>命令的结果返回给客户端;</p></blockquote><ul><li>并不是所有的命令都会被放进事务队列, 其中的例外就是<code>EXEC</code>, <code>DISCARD</code>, <code>MULTI</code>和<code>WATCH</code>及<code>UNWATCH</code>命令;</li></ul><blockquote><p><code>DISCARD</code> 命令用于取消一个事务, 它清空客户端的整个事务队列, 然后将客户端从事务状态调整回非事务状态, 最后返回字符串OK给客户端, 说明事务已被取消;</p></blockquote><blockquote><p><code>Redis</code>的事务是不可嵌套的, 当客户端已经处于事务状态, 而客户端又再向服务器发送<code>MULTI</code>时, 服务器只是简单地向客户端发送一个错误, 然后继续等待其他命令的入队; <code>MULTI</code>命令的发送不会造成整个事务失败, 也不会修改事务队列中已有的数据;</p></blockquote><blockquote><p><code>WATCH</code>只能在客户端进入事务状态之前执行, 在事务状态下发送 <code>WATCH</code> 命令会引发一个错误, 但它不会造成整个事务失败, 也不会修改事务队列中已有的数据(和前面处理 <code>MULTI</code> 的情况一样);</p></blockquote><blockquote><p><code>WATCH</code>命令用于在事务开始之前监视任意数量的键, 当调用<code>EXEC</code>命令执行事务时, 如果任意一个被监视的键已经被其他客户端修改了, 那么整个事务不再执行, 直接返回失败;</p></blockquote><h5 id="与MySQL事务的区别"><a href="#与MySQL事务的区别" class="headerlink" title="与MySQL事务的区别"></a>与<code>MySQL</code>事务的区别</h5><p>在<code>MySQL</code>中只有使用了Innodb数据库引擎的数据库或表才支持事务;</p><blockquote><p>事务使用的目的是统一管理insert, update, delete 这些write操作，以此来维护数据完整性。</p></blockquote><p><strong>命令区别</strong><br><code>MySQL</code></p><blockquote><p><code>BEGIN</code>: 显式地开启一个事务;<br><code>COMMIT</code>: 提交事务，将对数据库进行的所有修改变成为永久性的;<br><code>ROLLBACK</code>: 结束用户的事务，并撤销正在进行的所有未提交的修改;</p></blockquote><p><code>Redis</code></p><blockquote><p><code>MULTI</code>: 标记事务的开始;<br><code>EXEC</code>: 执行事务的commands队列;<br><code>DISCARD</code>: 结束事务，并清除commands队列;</p></blockquote><p><strong>默认状态</strong><br><code>MySQL</code></p><blockquote><p><code>MySQL</code>会默认开启一个事务，且缺省设置是自动提交，即，每成功执行一个SQL，一个事务就会马上 <code>COMMIT</code>。所以不能<code>ROLLBACK</code>。</p></blockquote><p><code>Redis</code></p><blockquote><p><code>Redis</code>默认不会开启事务，即command会立即执行，而不会排队。并不支持<code>ROLLBACK</code></p></blockquote><p><strong>使用方式</strong><br><code>MySQL</code>包含两种</p><blockquote><p>用<code>BEGIN</code>, <code>ROLLBACK</code>, <code>COMMIT</code> 显式开启并控制一个 <code>新的</code> Transaction。<br>执行命令<code>SET AUTOCOMMIT=0</code>, 用来禁止当前会话自动<code>COMMIT</code>, 控制默认开启的事务。</p></blockquote><p><code>Redis</code></p><blockquote><p>用<code>MULTI</code>, <code>EXEC</code>, <code>DISCARD</code>, 显式开启并控制一个<code>Transaction</code>(注意这里没有强调<code>新的</code>, 因为默认是不会开启事务的);</p></blockquote><p><strong>实现原理</strong><br>显然<code>Redis</code>与<code>MySQL</code>中事务的区别其根本原因就是实现不同方式造成的;</p><p><code>MySQL</code></p><blockquote><p><code>MySQL</code>实现事务，是基于<code>UNDO/REDO</code>日志;<br><code>UNDO日志</code>记录修改前状态, <code>ROLLBACK</code>基于<code>UNDO日志</code>实现;<br><code>REDO日志</code>记录修改后的状态, <code>COMMIT</code>基于<code>REDO日志</code>实现;<br>在<code>MySQL</code>中无论是否开启事务, 每一个<code>SQL</code>都会被立即执行并返回执行结果。但是事务开启后执行后的状态只是记录在<code>REDO日志</code>, 执行<code>COMMIT</code>, 数据才会被写入磁盘。</p></blockquote><p><code>Redis</code></p><blockquote><p><code>Redis</code>实现事务, 是基于<code>COMMANDS</code>队列;<br>如果没有开启事务, <code>command</code>将会被立即执行并返回执行结果, 并且直接写入磁盘;<br>如果事务开启, <code>command</code>不会被立即执行, 而是排入队列并返回排队状态。调用<code>EXCE</code>才会执行<code>COMMANDS</code>队列;</p></blockquote><h5 id="不支持回滚"><a href="#不支持回滚" class="headerlink" title="不支持回滚"></a>不支持回滚</h5><p><code>Redis</code>事务不支持回滚, 官方解释, </p><blockquote><p><code>Redis</code>命令只会因为错误的语法而失败(并且这些问题不能在入队时发现), 或是命令用在了错误类型的键上面; 从实用性的角度来说, 失败的命令是由编程错误造成的, 而这些错误应该在开发的过程中被发现, 而不应该出现在生产环境中;<br>因为不需要对回滚进行支持，所以<code>Redis</code>的内部可以保持简单且快速;</p></blockquote><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>事务提供了一种将多个命令打包, 然后一次性, 有序地执行的机制; 且在执行过程中不会被中断, 所有事务命令执行完之后, 事务才能结束;</li><li>多个命令会被入队到事务队列中，然后按先进先出（FIFO）的顺序执行;</li><li><code>Redis</code>事务仅保证了事务的隔离执行; 不保证原子性：<code>Redis</code>同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚;</li><li>可以通过管道技术对事务进行优化;</li><li>通过<code>WATCH</code>命令在事务执行之前监控了多个Keys，倘若在<code>WATCH</code>之后有任何Key的值发生了变化，<code>EXEC</code>命令执行的事务都将被放弃，同时返回Null<code>MULTI</code>-bulk应答以通知调用者事务执行失败</li><li>悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁;</li><li>乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改, 所以不会上锁, 但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，乐观锁策略:提交版本必须大于记录当前版本才能执行更新;</li><li>对比分析了<code>Redis</code>事务与<code>MySQL</code>事务的异同点;</li><li>从官方解释中阐述为何<code>Redis</code>事务没有必要支持回滚机制;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;为了确保连续多个操作的原子性, 一个成熟的数据库通常都会有事务支持, &lt;code&gt;Redis&lt;/code&gt; 也不例外, &lt;code&gt;Redis&lt;/code&gt; 通过&lt;code&gt;MULTI&lt;/code&gt;, &lt;code&gt;DISCARD&lt;/code&gt;, &lt;code&gt;EXEC&lt;/code&gt;, &lt;code&gt;WATCH&lt;/code&gt;和&lt;code&gt;UNWATCH&lt;/code&gt; 五个命令来实现事务功能;&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis专题14 性能提升之管道技术</title>
    <link href="http://researchlab.github.io/2018/10/11/redis-14-pipeline/"/>
    <id>http://researchlab.github.io/2018/10/11/redis-14-pipeline/</id>
    <published>2018-10-11T07:37:51.000Z</published>
    <updated>2021-02-20T17:55:59.526Z</updated>
    
    <content type="html"><![CDATA[<p>用<code>redis管道技术</code>对执行结果没有互相依赖，对结果响应也无需立即获得的命令集批量提交到<code>redis</code>服务器的方式，能在一定程度上提升<code>redis</code>性能，性能提升的原因主要是TCP连接中减少了<code>交互往返</code>的时间。<br><a id="more"></a></p><blockquote><p><code>Redis管道(Pipeline)</code>本身并不是<code>Redis</code>服务器直接提供的技术，这个技术本质上是由客户端提供的，跟服务器没有什么直接的关系。</p></blockquote><h5 id="消息交互"><a href="#消息交互" class="headerlink" title="消息交互"></a>消息交互</h5><p><code>redis</code>是使用客户端-服务器模型的TCP服务器，称为请求/响应协议。<br>这意味着通常一个请求是通过以下步骤完成的:</p><blockquote><p>客户端向服务器发送查询，并通常以阻塞的方式从套接字读取服务器响应。<br>服务器处理命令并将响应发送回客户端。</p></blockquote><p>每一个<code>redis</code>命令request/response都需要经历一个<code>RTT</code>(Round-Trip Time 往返时间), 如果需要执行很多短小的命令，这些往返时间的开销是很大的，在此情形下，redis提出了<code>管道</code>来提高执行效率。</p><h5 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h5><blockquote><p>如果client执行一些相互之间无关的命令或者不需要获取命令的返回值，那么redis允许你连续发送多条命令，而不需要等待前面命令执行完毕;</p></blockquote><blockquote><p>比如我们执行3条INCR命令，如果使用管道，理论上只需要一个RTT+3条命令的执行时间即可，如果不适用管道，那么可能需要额外的两个RTT时间;</p></blockquote><blockquote><p>因此，管道相当于批处理脚本，相当于是命令集;</p></blockquote><blockquote><p>执行管道命令中, <code>redis</code>必须在处理完所有命令前先缓存起所有命令的处理结果。打包的命令越多，缓存消耗内存也越多。所以并不是打包的命令越多越好。具体多少合适需要根据具体情况测试。</p></blockquote><p><strong>注意</strong></p><blockquote><p>执行<code>管道</code>命令期间将<code>独占</code>链接，此期间将不能进行非<code>管道</code>类型的其他操作，直到管道关闭；</p></blockquote><blockquote><p>如果<code>管道</code>的指令集很庞大，为了不干扰链接中的其他操作，建议为<code>管道</code>操作新建Client链接，让<code>管道</code>和其他正常操作分离在2个client中; </p></blockquote><blockquote><p>不过<code>管道</code>事实上所能容忍的操作个数，和socket-output缓冲区大小/返回结果的数据尺寸都有很大的关系；同时也意味着每个redis-server同时所能支撑的管道链接的个数，也是有限的，这将受限于server的物理内存或网络接口的缓冲能力。</p></blockquote><h5 id="压力测试"><a href="#压力测试" class="headerlink" title="压力测试"></a>压力测试</h5><p>Redis 自带了一个压力测试工具redis-benchmark，使用这个工具就可以进行管道测试。</p><p>对一个普通的<code>set</code>指令进行压测，QPS 大约<code>6w/s</code>。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  02 docker exec -it redisbloom redis-benchmark -t set -q</span><br><span class="line">SET: 64350.06 requests per second</span><br></pre></td></tr></table></figure></p><p><code>-P</code>参数，表示单个管道内并行的请求数量，看下面<code>P=2</code>，QPS 达到了<code>8w/s</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it redisbloom redis-benchmark -t set -q -P 2</span><br><span class="line">SET: 89365.51 requests per second</span><br></pre></td></tr></table></figure><p>再看看<code>P=5</code>，<code>QPS</code>达到了<code>10w/s</code>。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  02 docker exec -it redisbloom redis-benchmark -t set -q -P 5</span><br><span class="line">SET: 106723.59 requests per second</span><br></pre></td></tr></table></figure></p><p>但如果再继续提升<code>P</code>参数，发现<code>QPS</code>已经上不去了。这是为什么呢？</p><p>因为这里<code>CPU</code>处理能力已经达到了瓶颈，<code>Redis</code>的单线程<code>CPU</code>已经飙到了<code>100%</code>，所以无法再继续提升了。</p><p>深入理解管道本质<br>接下来我们深入分析一个请求交互的流程，真实的情况是它很复杂，因为要经过网络协议栈，这个就得深入内核了。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">client ---&gt; request  ---&gt; send buffer ---&gt; NIC ---&gt; Gateway Router ---&gt; NIC ---&gt; recv buffer ---&gt; request  ---&gt; server </span><br><span class="line">                                                                                                                 |</span><br><span class="line">client &lt;--- response &lt;--- recv buffer &lt;--- NIC &lt;--- Gateway Router &lt;--- NIC &lt;--- send buffer &lt;--- response &lt;---  V</span><br></pre></td></tr></table></figure></p><p>上图就是一个完整的请求交互流程图,</p><ol><li>客户端进程调用<code>write</code>将消息写到操作系统内核为套接字分配的发送缓冲<code>send buffer</code>;</li><li>客户端操作系统内核将发送缓冲的内容发送到<code>网卡</code>，<code>网卡</code>硬件将数据通过「路由」送到服务器的<code>网卡</code>;</li><li>服务器操作系统内核将<code>网卡</code>的数据放到内核为套接字分配的接收缓冲<code>recv buffer</code>;</li><li>服务器进程调用<code>read</code>从接收缓冲中取出消息进行处理;</li><li>服务器进程调用<code>write</code>将响应消息写到内核为套接字分配的发送缓冲<code>send buffer</code>;</li><li>服务器操作系统内核将发送缓冲的内容发送到<code>网卡</code>，<code>网卡</code>硬件将数据通过「路由」送到客户端的<code>网卡</code>。</li><li>客户端操作系统内核将<code>网卡</code>的数据放到内核为套接字分配的接收缓冲<code>recv buffer</code>;</li><li>客户端进程调用<code>read</code>从接收缓冲中取出消息返回给上层业务逻辑进行处理;</li></ol><blockquote><p><code>write</code>操作不是等到对方收到消息才会返回。<code>write</code>操作只负责将数据写到本地操作系统内核的发送缓冲然后就返回了。剩下的事交给操作系统内核异步将数据送到目标机器。但是如果发送缓冲满了，那么就需要等待缓冲空出空闲空间来，这个就是写操作<code>IO</code>操作的真正耗时。</p></blockquote><blockquote><p><code>read</code>操作不是从目标机器拉取数据。<code>read</code>操作只负责将数据从本地操作系统内核的接收缓冲中取出来就了事了。但是如果缓冲是空的，那么就需要等待数据到来，这个就是读操作<code>IO</code>操作的真正耗时。</p></blockquote><p>所以对于<code>value = redis.get(key)</code>这样一个简单的请求来说，<code>write</code>操作几乎没有耗时，直接写到发送缓冲就返回，而<code>read</code>就会比较耗时了，因为它要等待消息经过网络路由到目标机器处理后的响应消息,再回送到当前的内核读缓冲才可以返回。这才是一个网络来回的真正开销。</p><p>而对于管道来说，连续的<code>write</code>操作根本就没有耗时，之后第一个<code>read</code>操作会等待一个网络的来回开销，然后所有的响应消息就都已经回送到内核的读缓冲了，后续的<code>read</code>操作直接就可以从缓冲拿到结果，瞬间就返回了。</p><h5 id="管道VS事务"><a href="#管道VS事务" class="headerlink" title="管道VS事务"></a>管道VS事务</h5><blockquote><p>管道和事务是不同的，pipeline只是表达”交互”中操作的传递的方向性，pipeline也可以在事务中运行，也可以不在。</p></blockquote><blockquote><p>无论如何，pipeline中发送的每个command都会被server立即执行，如果执行失败，将会在此后的相应结果集中得到信息；也就是pipeline并不是表达”所有command都一起成功”的语义，管道中前面命令失败，后面命令不会有影响，继续执行。</p></blockquote><blockquote><p>简单来说就是管道中的命令是没有关系的，它们只是像管道一样流水发给server，而不是串行执行，仅此而已; 但是如果pipeline的操作被封装在事务中，那么将有事务来确保操作的成功与失败。</p></blockquote><blockquote><p>pipeline 只是把多个redis指令一起发出去，redis并没有保证这些指定的执行是原子的；multi相当于一个redis的transaction的，保证整个操作的原子性，避免由于中途出错而导致最后产生的数据不一致</p></blockquote><h5 id="管道VS脚本"><a href="#管道VS脚本" class="headerlink" title="管道VS脚本"></a>管道VS脚本</h5><blockquote><p>使用管道可能在效率上比使用script要好，但是有的情况下只能使用script。因为在执行后面的命令时, 无法得到前面命令的结果，就像事务一样，所以如果需要在后面命令中使用前面命令的value等结果，则只能使用script或者事务+watch;</p></blockquote><blockquote><p>使用Redis脚本(在Redis版本2.6+), 可以使用执行服务器端所需的大量工作的脚本更高效地处理一些pipelining用例;</p></blockquote><blockquote><p>脚本的一大优势是它能够以最小的延迟读取和写入数据，使得读取，计算，写入等操作非常快速(在这种情况下, 流水线操作无法提供帮助, 因为客户端先需要读命令的回应, 它才可以调用写命令);</p></blockquote><blockquote><p>有时，应用程序可能还想在管道中发送<code>EVAL</code>或<code>EVALSHA</code>命令。这是完全可能的，Redis通过SCRIPT LOAD命令明确地支持它(它保证可以调用EVALSHA而没有失败的风险);</p></blockquote><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>从分析<code>redis</code>采用<code>tcp</code>消息协议入手, 为进一步提升<code>redis</code>性能的角度，探讨了<code>管道</code>技术在<code>redis</code>中的应用;  </li><li>进一步分析了管道的原理, 因为<code>redis</code>需要在处理完管道命令集前把之前的结果先缓存下来，所以管道并不是打包的命令越多越好，因为打包的命令越多占用的缓存也会相应的增大, 同时在执行管道命令完成前, 同一个<code>redis</code>连接无法继续执行非管道命令;</li><li>通过消息交互示例， 进一步深入分析了管道的本质，管道打包多条命令为客服端–服务端节省了往返(RTT)等待耗时, 因而进一步提升了<code>redis</code>性能;</li><li>最后对比分析了<code>管道</code>与<code>redis事务</code>, <code>redis脚本</code>之间的区别， 进一步阐述了<code>管道</code>，<code>事务</code>, <code>脚本</code>各自适用的场景;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;用&lt;code&gt;redis管道技术&lt;/code&gt;对执行结果没有互相依赖，对结果响应也无需立即获得的命令集批量提交到&lt;code&gt;redis&lt;/code&gt;服务器的方式，能在一定程度上提升&lt;code&gt;redis&lt;/code&gt;性能，性能提升的原因主要是TCP连接中减少了&lt;code&gt;交互往返&lt;/code&gt;的时间。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis专题13 数据存储与持久化</title>
    <link href="http://researchlab.github.io/2018/10/10/redis-13-data-storage-solution/"/>
    <id>http://researchlab.github.io/2018/10/10/redis-13-data-storage-solution/</id>
    <published>2018-10-10T04:36:10.000Z</published>
    <updated>2021-02-20T17:55:59.525Z</updated>
    
    <content type="html"><![CDATA[<p><code>redis</code>提供两种方式进行持久化，一种是<code>RDB</code>快照持久化(原理是将Reids在内存中的数据库记录压缩后定时dump到磁盘上的<code>RDB</code>持久化,存储紧凑)，另外一种是<code>AOF</code>(append only file)持久化(原理是将Reids的操作日志以追加的方式写入文件), <code>AOF</code> 日志在长期的运行过程中会变的无比庞大，数据库重启时需要加载<code>AOF</code>日志进行指令重放，这个时间就会无比漫长。所以需要定期进行<code>AOF</code>重写，给<code>AOF</code>日志进行瘦身。<br><a id="more"></a></p><h4 id="RDB快照原理"><a href="#RDB快照原理" class="headerlink" title="RDB快照原理"></a><code>RDB</code>快照原理</h4><p><code>RDB</code>(快照/内存快照)，就是<code>redis</code>按照一定的时间周期将目前服务中的所有数据全部写入到磁盘中。<br>我们知道<code>redis</code>是单线程程序，这个线程要同时负责多个客户端套接字的并发读写操作和内存数据结构的逻辑读写。</p><p>在服务线上请求的同时，<code>redis</code> 还需要进行内存快照，内存快照要求<code>redis</code> 必须进行文件IO操作，可<strong>文件IO操作是不能使用多路复用API</strong>。那该怎么办呢？</p><p><code>redis</code>使用操作系统的多进程<code>COW(Copy On Write) 机制</code>来实现快照持久化。</p><p><code>redis</code> 在持久化时会<code>fork</code>一个子进程，<strong>快照持久化完全交给子进程来处理，父进程继续处理客户端请求</strong>。子进程做数据持久化，它不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是父进程不一样，它必须持续服务客户端请求，然后对内存数据结构进行不间断的修改。</p><p>这个时候就会使用操作系统的<code>COW机制</code>来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬间的数据。</p><p>随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长。但是也不会超过原有数据内存的<code>2</code>倍大小。另外一个<code>redis</code>实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都会被分离，被分离的往往只有其中一部分页面。每个页面的大小只有<code>4K</code>，一个 <code>redis</code>实例里面一般都会有成千上万的页面。<br>原理过程<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0.假设现在redis数据存储在内存的A区;</span><br><span class="line">1.此时因配置或某种原因触发了RDB快照事件;</span><br><span class="line">2.触发RDB快照事件后，父进程会先fork出一个子进程, 把处理RDB快照的事情完全交给这个子进程处理，而父进程则继续处理来自客服端的请求;</span><br><span class="line">3.子进程会先将当前内存A区数据压缩, 然后dump刷盘到一个临时RDB文件中, 当dump完成后，再将这个临时RDB文件替换之前的RDB文件, 然后子进程结束退出;</span><br><span class="line">4.同样，在子进程处理快照dump过程中, 如果父进程接收到新的客服端请求，则父进程需要先拷贝一份内存A区中相关数据页的信息到内存B区，然后在B区上完成客服端的请求; </span><br><span class="line">5.当父进程完成新的客服端请求后，发现子进程已经完成了RDB快照处理， 则将刚才更新的B区数据取替换A区数据, 如果子进程还没有完成则等待;</span><br></pre></td></tr></table></figure></p><h5 id="AOF-原理"><a href="#AOF-原理" class="headerlink" title="AOF 原理"></a><code>AOF</code> 原理</h5><p><code>AOF</code>日志存储的是<code>redis</code>服务器的顺序指令序列，<code>AOF</code>日志<strong>只记录对内存进行修改的指令记录(查询/删除指令是不记录的)</strong>。<br>假设 <code>AOF</code> 日志记录了自<code>redis</code>实例创建以来所有的修改性指令序列，那么就可以通过对一个空的<code>redis</code> 实例顺序执行所有的指令，也就是「重放」，来恢复<code>redis</code>当前实例的内存数据结构的状态。</p><p><code>redis</code>会在收到客户端修改指令后，进行参数校验进行逻辑处理后，如果没问题，就立即将该指令文本存储到<code>AOF</code>日志中，也就是先执行指令才将日志存盘。这点不同于<code>leveldb、hbase</code>等存储引擎，它们都是先存储日志再做逻辑处理。</p><p><code>redis</code> 在长期运行的过程中，<code>AOF</code> 的日志会越变越长。如果实例宕机重启，重放整个 <code>AOF</code> 日志会非常耗时，导致长时间<code>redis</code>无法对外提供服务。所以需要对 <code>AOF</code> 日志瘦身。</p><p><strong>AOF重写</strong><br><code>redis</code> 提供了<code>bgrewrite AOF</code>指令用于对 <code>AOF</code> 日志进行瘦身。其原理就是开辟一个子进程对内存进行遍历转换成一系列 <code>redis</code> 的操作指令，序列化到一个新的 <code>AOF</code> 日志文件中。序列化完毕后再将操作期间发生的增量 <code>AOF</code> 日志追加到这个新的 <code>AOF</code> 日志文件中，追加完毕后就立即替代旧的 <code>AOF</code> 日志文件了，瘦身工作就完成了。</p><p><strong>fsync</strong><br><code>AOF</code> 日志是以文件的形式存在的，当程序对 <code>AOF</code> 日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核会异步将脏数据刷回到磁盘的。</p><p>这就意味着如果机器突然宕机，<code>AOF</code> 日志内容可能还没有来得及完全刷到磁盘中，这个时候就会出现日志丢失。那该怎么办？ 可以通过开启<code>fsync</code>配置来强制同步刷盘, 过于频繁的<code>fsync</code>会严重拖慢<code>redis</code>性能，所以在生产环境的服务器中，<code>redis</code> 通常是每隔<code>1s</code>左右执行一次<code>fsync</code>操作, 在保持高性能的同时，尽可能使得数据少丢失。</p><h5 id="混合持久化"><a href="#混合持久化" class="headerlink" title="混合持久化"></a>混合持久化</h5><p>重启 <code>redis</code> 时，我们很少使用 <code>RDB</code> 来恢复内存状态，因为会丢失大量数据。我们通常使用 <code>AOF</code> 日志重放，但是重放 <code>AOF</code> 日志性能相对 <code>RDB</code> 来说要慢很多，这样在<code>redis</code> 实例很大的情况下，启动需要花费很长的时间。</p><p><code>redis4.0</code>为了解决这个问题，带来了一个新的持久化选项——混合持久化。将 <code>RDB</code> 文件的内容和增量的 <code>AOF</code> 日志文件存在一起。这里的 <code>AOF</code> 日志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的增量 <code>AOF</code> 日志，通常这部分 <code>AOF</code> 日志很小。</p><p>于是在<code>redis</code> 重启的时候，可以先加载 <code>RDB</code> 的内容，然后再重放增量 <code>AOF</code> 日志就可以完全替代之前的 <code>AOF</code> 全量文件重放，重启效率因此大幅得到提升。</p><h5 id="RDB优势"><a href="#RDB优势" class="headerlink" title="RDB优势"></a>RDB优势</h5><blockquote><p> 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。</p></blockquote><blockquote><p>对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。</p></blockquote><blockquote><p> 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。</p></blockquote><blockquote><p> 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。</p></blockquote><h5 id="RDB劣势"><a href="#RDB劣势" class="headerlink" title="RDB劣势"></a>RDB劣势</h5><blockquote><p>如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。</p></blockquote><blockquote><p>由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。</p></blockquote><h4 id="AOF优势"><a href="#AOF优势" class="headerlink" title="AOF优势"></a>AOF优势</h4><blockquote><p>该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。</p></blockquote><blockquote><p>由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。</p></blockquote><blockquote><p> 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。</p></blockquote><blockquote><p>AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。</p></blockquote><h5 id="AOF劣势"><a href="#AOF劣势" class="headerlink" title="AOF劣势"></a>AOF劣势</h5><blockquote><p> 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。</p></blockquote><blockquote><p>根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。</p></blockquote><h5 id="常用配置"><a href="#常用配置" class="headerlink" title="常用配置"></a>常用配置</h5><p><strong>RDB持久化配置</strong><br>Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">save 900 1              #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 300 10            #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。</span><br><span class="line"></span><br><span class="line">save 60 10000        #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。</span><br></pre></td></tr></table></figure></p><p><strong>AOF持久化配置</strong><br>在Redis的配置文件中存在三种同步方式，它们分别是:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">appendfsync always     #每次有数据修改发生时都会写入AOF文件。</span><br><span class="line"></span><br><span class="line">appendfsync everysec  #每秒钟同步一次，该策略为AOF的缺省策略。</span><br><span class="line"></span><br><span class="line">appendfsync no          #从不同步。高效但是数据不会被持久化。</span><br></pre></td></tr></table></figure></p><h5 id="过期策略"><a href="#过期策略" class="headerlink" title="过期策略"></a>过期策略</h5><p><strong>RDB过期key处理策略</strong></p><blockquote><p>已过期的键不会被保存到新创建的RDB文件中。举个例子，如果数据库中包含三个键k1、k2、k3，并且k2已经过期，那么当执行SAVE命令或者BGSAVE命令时，程序只会将k1和k3的数据保存到RDB文件中，而k2则会被忽略。因此，数据库中包含过期键不会对生成新的RDB文件造成影响。</p></blockquote><p>在启动Redis服务器时，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入:</p><blockquote><p>如果服务器以主服务器模式运行，那么在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而过期键则会被忽略，所以过期键对载入RDB文件的主服务器不会造成影响；</p></blockquote><blockquote><p>如果服务器以从服务器模式运行，那么在载入RDB文件时，文件中保存的所有键，不论是否过期，都会被载入到数据库中。不过，因为主从服务器在进行数据同步的时候，从服务器的数据库就会被清空，所以一般来讲，过期键对载入RDB文件的从服务器也不会造成影响；</p></blockquote><p><strong>AOF过期key处理策略</strong></p><blockquote><p>当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但它还没有被惰性删除或者定期删除，那么AOF文件不会因为这个过期键而产生任何影响。<strong>当过期键被惰性删除或者定期删除之后，程序会向AOF文件追加（append）一条DEL命令，来显式地记录该键已被删除。</strong></p></blockquote><blockquote><p>和生成RDB文件时类似，在执行AOF重写的过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中。举个例子，如果数据库中包含三个键k1、k2、k3，并且k2已经过期，那么在进行重写工作时，程序只会对k1和k3进行重写，而k2则会被忽略。</p></blockquote><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>在<code>redis</code>数据存储持久化机制上, 探讨了<code>RDB快照</code> 和 <code>AOF</code>两种持久化方案，对其原理,区别等进行了进一步的说明;</li><li>进一步探讨了<code>redis.4.0</code>提供的混合持久化方案;</li><li>归类总结了<code>RDB</code>和<code>AOF</code>两种方案的优缺点;</li><li>从经验出发， 进一步总结了实际中常用的配置方案;</li><li>进一步总结了<code>RDB</code>和<code>AOF</code>对过期key的处理策略;</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;redis&lt;/code&gt;提供两种方式进行持久化，一种是&lt;code&gt;RDB&lt;/code&gt;快照持久化(原理是将Reids在内存中的数据库记录压缩后定时dump到磁盘上的&lt;code&gt;RDB&lt;/code&gt;持久化,存储紧凑)，另外一种是&lt;code&gt;AOF&lt;/code&gt;(append only file)持久化(原理是将Reids的操作日志以追加的方式写入文件), &lt;code&gt;AOF&lt;/code&gt; 日志在长期的运行过程中会变的无比庞大，数据库重启时需要加载&lt;code&gt;AOF&lt;/code&gt;日志进行指令重放，这个时间就会无比漫长。所以需要定期进行&lt;code&gt;AOF&lt;/code&gt;重写，给&lt;code&gt;AOF&lt;/code&gt;日志进行瘦身。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>redis专题12 redis通信协议</title>
    <link href="http://researchlab.github.io/2018/10/09/redis-12-resp/"/>
    <id>http://researchlab.github.io/2018/10/09/redis-12-resp/</id>
    <published>2018-10-09T07:34:03.000Z</published>
    <updated>2021-02-20T17:55:59.525Z</updated>
    
    <content type="html"><![CDATA[<p>Redis的客户端与服务端采用一种叫做<code>RESP(REdis Serialization Protocol)</code>的网络通信协议交换数据。 这种协议采用明文传输，易读也易解析。<code>Redis</code>客户端采用此协议格式来对服务端发送不同的命令，服务端会根据具体的操作而返回具体的答复。客户端和服务端采用的是简单的<code>请求-响应</code>模型进行通信的。<br><a id="more"></a></p><h5 id="RESP"><a href="#RESP" class="headerlink" title="RESP"></a>RESP</h5><p><code>RESP(Redis Serialization Protocol)</code>是 Redis序列化协议的简写。它是一种直观的文本协议，优势在于实现异常简单，解析性能极好。</p><p>Redis协议将传输的结构数据分为<code>5</code>种最小单元类型，单元结束时统一加上回车换行符号<code>\r\n</code>。</p><blockquote><p>单行字符串 以<code>+</code>符号开头。<br>多行字符串 以<code>$</code>符号开头，后跟字符串长度。<br>整数值 以<code>:</code>符号开头，后跟整数的字符串形式。<br>错误消息 以<code>-</code>符号开头。<br>数组 以<code>*</code>号开头，后跟数组的长度。</p></blockquote><p><strong>单行字符串</strong> <code>hello world</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+hello world\r\n</span><br></pre></td></tr></table></figure></p><p><strong>多行字符串</strong> <code>hello world</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">11\r\nhello world\r\n</span></span><br></pre></td></tr></table></figure></p><p>多行字符串当然也可以表示单行字符串。</p><p><strong>整数</strong> <code>1024</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:1024\r\n</span><br></pre></td></tr></table></figure></p><p><strong>错误</strong> 参数类型错误<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-WRONGTYPE Operation against a key holding the wrong kind of value\r\n</span><br></pre></td></tr></table></figure></p><p><strong>数组</strong> <code>[1,2,3]</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*3\r\n:1\r\n:2\r\n:3\r\n</span><br></pre></td></tr></table></figure></p><p><code>NULL</code>用多行字符串表示，不过长度要写成<code>-1</code>。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">-1\r\n</span></span><br></pre></td></tr></table></figure></p><p><strong>空串</strong> 用多行字符串表示，长度填<code>0</code>。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">0\r\n\r\n</span></span><br></pre></td></tr></table></figure></p><p><strong>注:</strong> 这里有两个<code>\r\n</code>。为什么是两个?因为两个<code>\r\n</code>之间,隔的是空串。</p><h5 id="客户端-gt-服务器"><a href="#客户端-gt-服务器" class="headerlink" title="客户端 -&gt; 服务器"></a>客户端 -&gt; 服务器</h5><p><strong>客户端向服务器发送的指令只有一种格式，多行字符串数组</strong>。比如一个简单的<code>set</code>指令<code>set learn redis</code>会被序列化成下面的字符串。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*3\r\n$3\r\nset\r\n$6\r\nlearn\r\n$8\r\nredis\r\n</span><br></pre></td></tr></table></figure></p><h5 id="服务器-gt-客户端"><a href="#服务器-gt-客户端" class="headerlink" title="服务器 -&gt; 客户端"></a>服务器 -&gt; 客户端</h5><p>服务器向客户端回复的响应要支持多种数据结构，所以消息响应在结构上要复杂不少。不过再复杂的响应消息也是以上<code>5</code>中基本类型的组合。</p><p><strong>1.单行字符串响应</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; set learn redis</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></p><p>这里的<code>OK</code>就是单行响应，没有使用引号括起来。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+OK</span><br></pre></td></tr></table></figure></p><p><strong>2.错误响应</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; incr learn </span><br><span class="line">(error) ERR value is not an integer or out of range</span><br></pre></td></tr></table></figure></p><p>试图对一个字符串进行自增，服务器抛出一个通用的错误。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-ERR value is not an integer or out of range</span><br></pre></td></tr></table></figure></p><p><strong>3.整数响应</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; incr books</span><br><span class="line">(integer) 1</span><br></pre></td></tr></table></figure></p><p>这里的<code>1</code>就是整数响应<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:1</span><br></pre></td></tr></table></figure></p><p><strong>4.多行字符串响应</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; get learn </span><br><span class="line">"redis"</span><br></pre></td></tr></table></figure></p><p><u>这里使用双引号括起来的字符串就是多行字符串响应</u></p><p><strong>5.数组响应</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; hset shanghai num 021</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; hset shanghai date 10.1</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; hset shanghai abbr sh</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; hgetall shanghai</span><br><span class="line">1) "num"</span><br><span class="line">2) "021"</span><br><span class="line">3) "date"</span><br><span class="line">4) "10.1"</span><br><span class="line">5) "abbr"</span><br><span class="line">6) "sh"</span><br></pre></td></tr></table></figure></p><p>这里的<code>hgetall</code>命令返回的就是一个数组，第<code>0|2|4</code>位置的字符串是<code>hash</code>表的<code>key</code>，第<code>1|3|5</code>位置的字符串是<code>value</code>，客户端负责将数组组装成字典再返回。</p><p><strong>6.嵌套</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; scan 0</span><br><span class="line">1) "0"</span><br><span class="line">2) 1) "shanghai"</span><br><span class="line">   2) "learn"</span><br></pre></td></tr></table></figure></p><p><code>scan</code>命令用来扫描服务器包含的所有<code>key</code>列表，它是以游标的形式获取，一次只获取一部分。</p><p><code>scan</code>命令返回的是一个嵌套数组。数组的第一个值表示游标的值，如果这个值为零，说明已经遍历完毕。如果不为零，使用这个值作为<code>scan</code>命令的参数进行下一次遍历。数组的第二个值又是一个数组，这个数组就是<code>key</code>列表。</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li><code>Redis</code>协议里有大量冗余的回车换行符，但是这不影响它成为互联网技术领域非常受欢迎的一个文本协议。有很多开源项目使用<code>RESP</code>作为它的通讯协议。在技术领域性能并不总是一切，还有简单性、易理解性和易实现性，这些都需要进行适当权衡。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;Redis的客户端与服务端采用一种叫做&lt;code&gt;RESP(REdis Serialization Protocol)&lt;/code&gt;的网络通信协议交换数据。 这种协议采用明文传输，易读也易解析。&lt;code&gt;Redis&lt;/code&gt;客户端采用此协议格式来对服务端发送不同的命令，服务端会根据具体的操作而返回具体的答复。客户端和服务端采用的是简单的&lt;code&gt;请求-响应&lt;/code&gt;模型进行通信的。&lt;br&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="redis专题" scheme="http://researchlab.github.io/categories/redis专题/"/>
    
    
    <category term="redis" scheme="http://researchlab.github.io/tags/redis/"/>
    
  </entry>
  
</feed>
